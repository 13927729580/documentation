[[deployment]]
== Развертывание приложений

В данной главе рассматриваются различные аспекты развертывания и эксплуатации CUBA-приложений.

На диаграмме ниже приведена возможная структура развернутого приложения. 

image::DeploymentStructure.png[align="center"]

В приведенном варианте приложение обеспечивает отсутствие единой точки отказа, балансировку нагрузки и подключение различных типов клиентов. В простейшем случае, однако, серверная часть приложения может быть установлена на одном компьютере, содержащем, в том числе, и базу данных. Различные варианты развертывания в зависимости от нагрузки и требований к отказоустойчивости подробно рассмотрены в <<scaling,Масштабирование приложения>>.

[[app_dirs]]
=== Каталоги приложения

В данном разделе описываются каталоги файловой системы, используемые различными <<app_tiers,блоками приложения>> во время выполнения.

[[conf_dir]]
==== Конфигурационный каталог

Каталог конфигурации предназначен для размещения ресурсов, дополняющих и переопределяющих свойства приложения, пользовательский интерфейс и бизнес-логику после развертывания приложения. Переопределение обеспечивается механизмом загрузки интерфейса инфраструктуры `<<resources,Resources>>`, который сначала выполняет поиск в конфигурационном каталоге, а потом в CLASSPATH, так что одноименные ресурсы в конфигурационном каталоге имеют приоритет над расположенными в JAR-файлах и каталогах классов.

Конфигурационный каталог может содержать следующие типы ресурсов:

* Файл `<<app_properties_files,local.app.properties>>`, определяющий параметры развертывания блоков приложения, работающих под управлением веб-сервера.

* Конфигурационные файлы `<<metadata.xml,metadata.xml>>`, `<<persistence.xml,persistence.xml>>`, `<<views.xml,views.xml>>`, `<<remoting-spring.xml,remoting-spring.xml>>`.

* <<screen_xml,XML-дескрипторы>> экранов UI.

* <<screen_controller,Контроллеры>> экранов UI в виде исходных текстов Java или Groovy.

* Скрипты или классы Groovy, а также исходные тексты классов Java, используемые приложением через интерфейс `<<scripting,Scripting>>`.

Расположение конфигурационного каталога определяется свойством приложения `<<cuba.confDir,cuba.confDir>>`. Для блоков *Middleware*, *Web Client* и *Web Portal* в варианте <<fast_deployment,быстрого развертывания>> в *Tomcat* это подкаталог с именем веб-приложения в каталоге `tomcat/conf`, например `tomcat/conf/app-core` для *Middleware*.

[[work_dir]]
==== Рабочий каталог

Рабочий каталог используется приложением для хранения файлов данных и конфигурации.

Например, подкаталог `filestorage` рабочего каталога по умолчанию используется <<file_storage,хранилищем загруженных файлов>>. Кроме того, блок *Middleware* на старте сохраняет в рабочем каталоге сгенерированные файлы `<<persistence.xml,persistence.xml>>` и `orm.xml`.

Расположение рабочего каталога определяется свойством приложения `<<cuba.dataDir,cuba.dataDir>>`. Для блоков *Middleware*, *Web Client* и *Web Portal* в варианте <<fast_deployment,быстрого развертывания>> в *Tomcat* это подкаталог с именем веб-приложения в каталоге `tomcat/work`.

[[log_dir]]
==== Каталог журналов

В каталоге журналов создаются лог-файлы приложения.

Состав и настройка файлов журналов определяются конфигурацией фреймворка *Logback*. Расположение файла конфигурации определяется системным свойством <<logback.configurationFile,logback.configurationFile>>.

Данный каталог может быть также использован для сохранения произвольной информации о выполнении приложения. Путь к каталогу журналов определяется свойством приложения <<cuba.logDir,cuba.logDir>>. Для блоков Middleware, Web Client и Web Portal в варианте <<fast_deployment,быстрого развертывания>> в Tomcat это каталог `tomcat/logs`.

См. также <<logging, Логгирование>>.

[[temp_dir]]
==== Временный каталог

Данный каталог может быть использован для создания произвольных временных файлов во время выполнения приложения. Путь к временному каталогу определяется свойством приложения `<<cuba.tempDir,cuba.tempDir>>`. Для блоков *Middleware*, *Web Client* и *Web Portal* в варианте <<fast_deployment,быстрого развертывания>> в *Tomcat* это подкаталог с именем веб-приложения в каталоге `tomcat/temp`.

[[db_dir]]
==== Каталог скриптов базы данных

В данном каталоге развернутого блока *Middleware* хранится набор SQL скриптов создания и обновления БД.

Структура каталога скриптов повторяет описанную в <<db_scripts,>>, но имеет один дополнительный верхний уровень, разделяющий скрипты используемых <<app_components,компонентов>> и самого приложения. Нумерация каталогов верхнего уровня определяется во время сборки проекта.

Расположение каталога скриптов БД определяется свойством приложения <<cuba.dbDir,cuba.dbDir>>. В варианте <<fast_deployment,быстрого развертывания>> в Tomcat это подкаталог `WEB-INF/db` каталога веб-приложения среднего слоя: `tomcat/webapps/app-core/WEB-INF/db`.

[[deployment_variants]]
=== Варианты развертывания

В данном разделе рассматриваются различные варианты развертывания CUBA-приложений.

[[fast_deployment]]
==== Быстрое развертывание в Tomcat

Быстрое развертывание используется по умолчанию при разработке приложения, так как обеспечивает минимальное время сборки, установки и старта приложения. Данный вариант удобен также и для эксплуатации приложения.

Быстрое развертывание производится с помощью задачи <<build.gradle_deploy,deploy>>, объявленной для модулей core и web в файле `build.gradle`. Перед первым выполнением `deploy` необходимо установить и проинициализировать локальный сервер Tomcat с помощью задачи <<build.gradle_setupTomcat,setupTomcat>>. 

В результате быстрого развертывания в каталоге, задаваемом свойством `ext.tomcatDir` скрипта `build.gradle` создается следующая структура (перечислены только важные каталоги и файлы, описанные ниже):

[source, plain]
----
bin/
    setenv.bat, setenv.sh
    startup.bat, startup.sh
    debug.bat, debug.sh
    shutdown.bat, shutdown.sh

conf/
    catalina.properties
    server.xml
    logback.xml
    logging.properties
    Catalina/
        localhost/
    app/
    app-core/

lib/
    hsqldb-2.2.9.jar

logs/
    app.log

shared/
    lib/

temp/
    app/
    app-core/

webapps/
    app/
    app-core/

work/
    app/
    app-core/
----

* `bin` - каталог, содержащий средства запуска и остановки сервера Tomcat:

** `setenv.bat`, `setenv.sh` - скрипты установки переменных окружения. Эти скрипты следует использовать для установки параметров памяти JVM, указания файла конфигурации <<logging_setup_tomcat,логгирования>>, настройки <<jmx_remote_access,доступа по JMX>>, параметров <<debug_setup,подключения отладчика>>.

** `startup.bat`, `startup.sh` - скрипты запуска Tomcat. Сервер стартует в отдельном консольном окне в *Windows* и в фоне в **nix*.
+
Для запуска сервера в текущем консольном окне вместо `startup.*` используйте команды
+
`> catalina.bat run`
+
`$ ./catalina.sh run`

** `debug.bat`, `debug.sh` - скрипты, аналогичные `++startup.*++`, однако запускающие Tomcat с возможностью подключения отладчика. Именно эти скрипты запускаются при выполнении задачи <<build.gradle_start,start>> скрипта сборки.

** `shutdown.bat`, `shutdown.sh` - скрипты остановки Tomcat.

* `conf` - каталог, содержащий файлы конфигурации Tomcat и развернутых в нем приложений.

** `catalina.properties` - свойства Tomcat. Для загрузки общих библиотек из каталога `shared/lib` (см. ниже) данный файл должен содержать строку:
+
[source, properties]
----
shared.loader=${catalina.home}/shared/lib/*.jar
----

** `server.xml` - описатель конфигурации Tomcat. В этом файле можно изменить порты сервера.

** `logback.xml` - описатель конфигурации <<logging_setup_tomcat,логгирования>> приложений.

** `logging.properties` - описатель конфигурации логгирования самого сервера Tomcat.

** `Catalina/localhost` - в этом каталоге можно разместить дескрипторы развертывания приложений <<context.xml,context.xml>>. Дескрипторы, расположенные в данном каталоге имеют приоритет над дескрипторами в каталогах `META-INF` самих приложений, что часто бывает удобно при эксплуатации системы. Например, в таком дескрипторе на уровне сервера можно указать параметры подключения к базе данных, отличные от указанных в самом приложении.
+
Дескриптор развертывания на уровне сервера должен иметь имя приложения и расширение `.xml`. То есть для создания такого дескриптора, например, для приложения `app-core`, необходимо скопировать содержимое файла `webapps/app-core/META-INF/context.xml` в файл `conf/Catalina/localhost/app-core.xml`.

** `app` - <<conf_dir,конфигурационный каталог>> приложения веб-клиента `app`.

** `app-core` - <<conf_dir,конфигурационный каталог>> приложения среднего слоя `app-core`.

* `lib` - каталог библиотек, загружаемых в _common classloader_ сервера. Эти библиотеки доступны как самому серверу, так и всем развернутым в нем веб-приложениям. В частности, в данном каталоге должны располагаться JDBC-драйверы используемых баз данных (`hsqldb-XYZ.jar`, `postgresql-XYZ.jar` и т.д.)

* `logs` - каталог <<logging,логов>> приложений и сервера. Основной лог-файл приложений - `app.log`.

* `shared/lib` - каталог библиотек, доступных всем развернутым приложениям. Классы этих библиотек загружаются в специальный _shared classloader_ сервера. Использование shared classloader задается в файле `conf/catalina.properties` как описано выше.
+
Задачи <<build.gradle_deploy,deploy>> файла сборки копируют в этот каталог все библиотеки, не перечисленные в параметре `jarNames`, то есть не специфичные для данного приложения.

* `temp/app`, `temp/app-core` - <<temp_dir,временные каталоги>> приложений веб-клиента и среднего слоя.

* `webapps` - каталог веб-приложений. Каждое приложение располагается в собственном подкаталоге в формате _exploded WAR_.
+
Задачи <<build.gradle_deploy,deploy>> файла сборки создают подкаталоги приложений с именами, указанными в параметрах `appName`, и кроме прочего копируют в их подкаталоги `WEB-INF/lib` библиотеки, перечисленные в параметре `jarNames`.

* `work/app`, `work/app-core` - <<work_dir,рабочие каталоги>> приложений веб-клиента и среднего слоя.

[[tomcat_in_prod]]
===== Использование Tomcat при эксплуатации приложения

Процедура <<fast_deployment,быстрого развертывания>> по умолчанию создает веб приложения `app` и `app-core`, работающие на локальном инстансе Tomcat на порту 8080. Это означает, что веб клиент доступен по адресу `++http://localhost:8080/app++`.

Вы можете использовать этот экземпляр Tomcat для эксплуатации приложения, просто скопировав его на сервер. После этого необходимо установить имя хоста сервера в файлах `conf/app/local.app.properties` и `conf/app-core/local.app.properties` (создайте файлы если они не существуют):

[source, properties]
----
  cuba.webHostName = myserver
  cuba.webAppUrl = http://myserver:8080/app
---- 

Кроме того, необходимо настроить подключение к production базе данных. Это можно сделать в файле <<context.xml>> веб-приложения (`webapps/app-core/META-INF/context.xml`), или скопировать этот файл в `conf/Catalina/localhost/app-core.xml` как описано в предыдущем разделе, чтобы разделить настройки соединения с БД для разработки и эксплуатации.

Базу данных для production можно создать из бэкапа той базы, которая использовалась при разработке, либо настроить автоматическое создание и обновление БД. См. <<db_update_in_prod>>.

Если вы хотите изменить порт Tomcat или веб-контекст (последнюю часть URL после `/`), используйте *Studio*:

* Откройте проект в Studio.

* Перейдите в *Project Properties* > *Edit* > *Advanced*.

* Чтобы изменить веб-контекст, отредактируйте поле *Modules prefix*.

* Чтобы изменить порт Tomcat, отредактируйте поле *Tomcat ports* > *HTTP port*.

Если для веб клиента вы хотите использовать корневой контекст (`++http://myserver:8080++`), переименуйте каталоги `app` в `ROOT`

[source, plain]
----
tomcat/
  conf/
      ROOT/
          local.app.properties
      app-core/
          local.app.properties
  webapps/
      ROOT/
      app-core/
----

и используйте `/` в качестве веб контекста в файле `conf/ROOT/local.app.properties`:

[source, properties]
----
cuba.webContextName = /
---- 

[[war_deployment]]
==== Развертывание WAR в Jetty

CUBA-приложения можно разворачивать в файлы с помощью задачи сборки <<build.gradle_buildWar,buildWar>> и выполнять на любом контейнере сервлетов Java. Рассмотрим пример сборки WAR-файлов и их развертывания на сервере *Jetty*.

. Добавьте в конец <<build.gradle,build.gradle>> задачу сборки <<build.gradle_buildWar,buildWar>>:
+
[source, groovy]
----
include::{sourcesdir}/deployment/warDeployment_1.groovy[]
----
+
[TIP]
====
В данном случае собирается два WAR-файла, отдельно для блоков Middleware и Web Client. Если вы хотите объединить блоки в одном WAR-файле с помощью параметра `singleWar = true`, предоставьте также специальный файл `web.xml`, описанный в <<build.gradle_buildWar,этой секции>>.
====

. Запустите сборку:
+
[source, plain]
----
gradlew buildWar
----
+
В результате в подкаталоге `build\distributions\war` проекта будут созданы файлы `app-core.war` и `app.war`.

. Создайте домашний каталог приложения на сервере, например, `c:\work\app_home`.

. Загрузите и установите сервер Jetty, например в каталог `c:\work\jetty-home`. Данный пример тестировался на версии `jetty-distribution-9.3.6.v20151106.zip`.

. Создайте каталог `c:\work\jetty-base`, откройте в нем командную строку и выполните:
+
[source, plain]
----
java -jar c:\work\jetty-home\start.jar --add-to-start=http,jndi,deploy,plus,ext,resources
----

. Создайте файл `c:\work\jetty-base\app-jetty.xml` следующего содержания (для БД PostgreSQL с именем `test`):
+
[source, xml]
----
include::{sourcesdir}/deployment/warDeployment_2.xml[]
----

. Добавьте следующий текст в начало файла `c:\work\jetty-base\start.ini`:
+
[source, plain]
----
include::{sourcesdir}/deployment/warDeployment_3.ini[]
----

. Скопируйте JDBC-драйвер используемой базы данных в каталог `c:\work\jetty-base\lib\ext`. Файл драйвера можно взять из каталога `lib` CUBA Studio, либо из каталога `build\tomcat\lib` проекта. В случае PostgreSQL это файл `postgresql-9.1-901.jdbc4.jar`.

. Скопируйте файлы WAR в каталог `c:\work\jetty-base\webapps`.

. Откройте командную строку в каталоге `c:\work\jetty-base` и выполните:
+
[source, plain]
----
java -jar c:\work\jetty-home\start.jar
----

. Откройте `++http://localhost:8080/app++` в веб-браузере.

[[tomcat_war_deployment]]
==== Развертывание WAR в Tomcat Windows Service

. Добавьте в конец <<build.gradle,build.gradle>> задачу сборки <<build.gradle_buildWar,buildWar>>:
+
--
[source, groovy]
----
include::{sourcesdir}/deployment/warDeployment_2.groovy[]
----

Если параметры сервера отличаются от параметров локального Tomcat, используемого для <<fast_deployment,быстрого развертывания>>, укажите соответствующие свойства приложения:

[source, groovy]
----
include::{sourcesdir}/deployment/warDeployment_3.groovy[]
----

Можно также указать отдельный `context.xml` для настройки соединения с production БД, например:

[source, groovy]
----
include::{sourcesdir}/deployment/warDeployment_4.groovy[]
----
--

. Запустите задачу `buildWar`. В результате, в каталоге `build/distibutions` проекта будут сгенерированы файлы `app.war` и `app-core.war`.
+
[source, plain]
----
gradlew buildWar
----

. Скачайте и установите Tomcat 8 Windows Service Installer.

. После установки, перейдите в подкаталог `bin` установленного сервера и запустите `tomcat8w.exe` от имени администратора.
На вкладке *Java* установите параметр *Maximum memory pool* 1024MB. Перейдите на вкладку *General* и запустите сервис.
+
image::tomcatPropeties.jpg[align="center"]

. Пропишите `-Dfile.encoding=UTF-8` в поле _Java Options_.

. Скопируйте сгенерированные файлы `app.war` и `app-core.war` в подкаталог `webapps` сервера.

. Запустите сервис Tomcat.

. Откройте `++http://localhost:8080/app++` в браузере.


[[jelastic_deployment]]
==== Развертывание в облаке Jelastic

CUBA Studio позволяет легко развернуть приложение в облаке link:$$https://jelastic.com/$$[Jelastic].

[TIP]
====
В данный момент развертывание в облаке возможно только для проектов, использующих в качестве сервера базы данных PostgreSQL или HSQL.
====

. Нажмите на ссылку *Deployment settings* в секции *Project properties* и перейдите на вкладку *CLOUD*.

. Если для данного проекта еще нет настроек развертывания в облаке, вы можете использовать поле вверху открывшейся страницы для создания бесплатной тестовой учетной записи Jelastic.

. После завершения регистрации введите email, пароль и выбранного хостинг-провайдера.
+
image::jelastic_1.png[align="center"]

. В поле *Environment* вводится имя окружения, в которое будет развернут WAR. Нажмите на кнопку с троеточием и выберите существующее окружение, или создайте новое. Вы можете проверить окружение на совместимость с вашим проектом. Совместимое окружение должно иметь Java 8, Tomcat 8 и PostgreSQL 9.1+ (если в проекте используется база данных PostgreSQL). Если ваш проект использует PostgreSQL, вы получите email с информацией о подключении к БД. Используйте эту информацию при генерации context.xml, см. поле *Custom context.xml path* ниже. Кроме того, вы должны создать пустую базу данных PostgreSQL через веб-интерфейс провайдера, ссылка на который содержится в письме. Выбранное имя базы данных должно быть указано позже в context.xml.
+
image::jelastic_6.png[align="center"]

. Нажмите кнопку *Generate* рядом с полем *Custom web.xml path*. Studio сгенерирует специальный `web.xml` для <<build.gradle_buildWar,единого WAR>>, содержащего блоки Middleware и Web Client.
+
image::jelastic_2.png[align="center"]

. Если проект использует HSQLDB, то это все - вы можете нажать *OK* и запустить развертывание командой *Run > Deploy to cloud* главного меню.

. Если проект использует PostgreSQL, перейдите в административный веб-интерфейс по ссылке в письме, полученном после создания Environment, и создайте базу данных.

. Нажмите кнопку *Generate* рядом с полем *Custom context.xml path* и укажите пользователя, пароль, хомт и имя базы данных.
+
image::jelastic_3.png[align="center"]

. Оставьте флажки *Include JDBC driver* и *Include context.xml* включенными.
+
image::jelastic_4.png[align="center"]

. Нажмите *OK* и запустите развертывание командой *Run > Deploy to cloud* главного меню.

. После завершения развертывания используйте сссылку в левом нижнем углу чтобы открыть веб-интерфейс приложения.
+
image::jelastic_5.png[align="center"]

[[bluemix_deployment]]
==== Deployment to Bluemix Cloud

С помощью CUBA Studio можно легко развернуть приложение в облаке IBM® Bluemix®.

[TIP]
====
Развёртывание в облаке Bluemix в настоящее время рекомендуется только для проектов, использующих базу данных PostgreSQL. HSQLDB доступна только с опцией _in-process_, таким образом, база данных будет пересоздаваться каждый раз при перезапуске облачного приложения, соответственно, пользовательские данные будут потеряны.
====

. Создайте учётную запись в сервисе Bluemix. Также скачайте и установите следующее программное обеспечение:
.. Bluemix CLI: http://clis.ng.bluemix.net/ui/home.html
.. Cloud Foundry CLI: https://github.com/cloudfoundry/cli/releases
.. После установки убедитесь, что команды `bluemix` и `cf` работают в командной строке. При необходимости добавьте путь к исполняемым файлам `\IBM\Bluemix\bin` в переменную среды `PATH`.

. Создайте новое пространство (Space) в облаке Bluemix, задайте ему любое имя. В дальнейшем вы можете поместить несколько приложений в одно пространство.

. Добавьте к созданному пространству сервер приложений Tomcat: *Create App* -> *CloudFoundry Apps* -> *Tomcat*.

. Задайте имя приложения. Имя должно быть уникальным, так как на его основе строится URL, по которому WEB-приложение будет доступно впоследствии.

. Чтобы добавить к пространству подходящий сервис базы данных, нажмите *Create service* в панели управления пространством и выберите *ElephantSQL*.

. Откройте панель управления приложением (ранее созданный Tomcat) и подключите сервис базы данных к приложению. Нажмите *Connect Existing*. Чтобы изменения вступили в силу, система предлагает обновить (restage) приложение. На данном этапе в этом нет необходимости: сервер Tomcat будет обновлен позже при развертывании CUBA-приложения.

. После подключения сервиса базы данных к приложению параметры подключения к СУБД будут доступны по кнопке *View Credentials*. Также параметры подключения к СУБД сохраняются в переменной среды `VCAP_SERVICES` облачного приложения и доступны по команде `cf&#160;env`. Созданная БД доступна глобально, управлять базой данных можно по указанному URL.

. Настройте CUBA-проект на базу данных PostgreSQL (на СУБД, аналогичную той которую Вы используете в облаке Bluemix).

. Создайте скрипты базы данных и запустите локальный сервер Tomcat. Убедитесь, что приложение работоспособно.

. Создайте WAR-файл, при помощи которого приложение будет равзернуто в сервер Tomcat.
.. Нажмите *Deployment Settings* в разделе *Project Properties* панели навигатора Studio.
.. Перейдите на вкладку *WAR*.
.. При помощи чекбоксов выберите все доступные опции: для корректного развертывания в облаке необходим единый *Single WAR* файл с помещёнными в него драйвером базы данных и конфигурационным файлом `context.xml`.
+
image::bluemix_war_settings.png[align="center"]

.. Нажмите кнопку *Generate* рядом с полем *Custom context.XML*. В появившемся диалоге укажите параметры подключения к базе данных - сервису в облаке Bluemix.
+
Используйте параметры из строки `uri` сервиса, как в примере ниже:
+
[source, json]
----
include::{sourcesdir}/deployment/bluemix_credentials.json[]
----
+
*Database user*: `ldwpelpl`
+
*Database password*: `eFwXx6lNFLheO5maP9iRbS77Sk1VGO_T`
+
*Database URL*: `echo-01.db.elephantsql.com:5432`
+
*Database name*: `ldwpelpl`

.. Нажмите кнопку *Generate* для создания собственного файла `web.xml`, необходимого для единого WAR-файла.

.. Сохраните настройки. Создайте WAR-файл, выполнив команду Gradle `buildWar` в Studio или из командной строки.
+
image::bluemix_buildWar.png[align="center"]
+
В результате, в папке проекта `build/distributions/war/` появился файл `app.war`.

. В корневом каталоге прокекта вручную создайте файл `manifest.yml` со следующим содержимым:
+
[source, yml]
----
include::{sourcesdir}/deployment/bluemix_manifest.yml[]
----
+
где
+
* `path` - относительный путь к сгенерированному WAR-файлу.
* `memory`: по умолчанию  серверу Tomcat выделяется лимит памяти в 1G. При необходимости вы можете уменьшить или увеличить объём выделенной памяти, эта настройка также доступна через WEB-интерфейс Bluemix. Учтите, что количество памяти, выделенной приложению, влияет на стоимость облачного размещения.
* `name` - имя сервера приложения Tomcat, созданного в облаке.
* `host`: идентично имени приложения.
* `env`: этим параметром задаются переменные среды. В нашем случае переменными среды задаются версии Tomcat и Java, необходимые для правильного функционирования CUBA-приложения.

. В комадной строке перейдите в корневой каталог проекта CUBA.
+
[source, yml]
----
cd your_project_directory
----

. Создайте подключение к Bluemix.
+
[source, yml]
----
bluemix api https://api.ng.bluemix.net
----

. Зайдите в Вашу учетную запись Bluemix.
+
[source, yml]
----
cf login
----

.  Разверните созданный WAR в облачный Tomcat.
+
[source, yml]
----
cf push
----
+
Команда `push` использует параметры, указанные в конфигурационном файле `manifest.yml`.

. Посмотреть логи сервера Tomcat можно на вкладке *Logs* панели управления приложением в WEB-интерфейсе Bluemix, а также в командной строке при помощи команды
+
[source, yml]
----
cf logs cuba-app --recent
----

. По завершению процесса развёртывания CUBA-приложение будет доступно в облаке Bluemix. Чтобы его открыть, воспользуйтесь URL `host.domain` в браузере. Этот URL будет отображаться в поле *ROUTE* таблицы ваших приложений *Cloud Foundry Apps*.

[[scaling]]
=== Масштабирование приложения

В данном разделе рассмотрены способы масштабирования CUBA-приложения, состоящего из блоков Middleware и Web Client, при возрастании нагрузки и ужесточении требований к отказоустойчивости.

[cols="2", frame="all", width="100%"]
|===

a| *Этап 1. Оба блока развернуты на одном сервере приложения.*

Это простейший вариант, реализуемый стандартной процедурой <<fast_deployment,быстрого развертывания>>.

В данном случае обеспечивается максимальная производительность передачи данных между блоками *Web Client* и *Middleware*, так как при включенном свойстве приложения <<cuba.useLocalServiceInvocation,cuba.useLocalServiceInvocation>> сервисы Middleware вызываются в обход сетевого стека.
^| image:scaling_1.png[align="center"]

a| *Этап 2. Блоки Middleware и Web Client развернуты на отдельных серверах приложения.*

Данный вариант позволяет распределить нагрузку между двумя серверами приложения и более оптимально использовать ресурсы серверов. Кроме того, в этом случае нагрузка от веб-пользователей меньше сказывается на выполнении других процессов. Под другими процессами здесь понимается обслуживание средним слоем других типов клиентов (например Desktop), выполнение <<scheduled_tasks,задач по расписанию>> и, возможно, интеграционные задачи.

Требования к ресурсам серверов:

* Tomcat 1 (Web Client):
** Объем памяти - пропорционально количеству одновременно подключенных пользователей.
** Мощность CPU - зависит от интенсивности работы пользователей.
* Tomcat 2 (Middleware):
** Объем памяти - фиксированный и относительно небольшой.
** Мощность CPU - зависит от интенсивности работы пользователей и других процессов.

В этом и более сложных вариантах развертывания в блоке Web Client свойство приложения <<cuba.useLocalServiceInvocation,cuba.useLocalServiceInvocation>> должно быть установлено в false, а свойство <<cuba.connectionUrlList,cuba.connectionUrlList>> должно содержать URL блока Middleware.
^| image:scaling_2.png[align="center"]

| *Этап 3. Кластер серверов Web Client работает с одним сервером Middleware.*

Данный вариант применяется, когда вследствие большого количества одновременно подключенных пользователей требования к памяти для блока Web Client превышают возможности одной JVM. В этом случае запускается кластер (два или более) серверов Web Client, и подключение пользователей производится через Load Balancer. Все серверы Web Client работают с одним сервером Middleware.

Дублирование серверов Web Client автоматически обеспечивает отказоустойчивость на этом уровне. Однако, так как репликация HTTP-сессий не поддерживается, при незапланированном отключении одного из серверов Web Client все пользователи, подключенные к нему, вынуждены будут выполнить новый логин в приложение.

Настройка данного варианта развертывания описана в <<cluster_webclient,Настройка кластера Web Client>>.
^| image:scaling_3.png[align="center"]

| *Этап 4. Кластер серверов Web Client работает с кластером серверов Middleware.*

Это максимальный вариант развертывания, обеспечивающий отказоустойчивость и балансировку нагрузки для Middleware и Web Client.

Подключение пользователей к серверам Web Client производится через Load Balancer. Серверы WebClient работают с кластером серверов Middleware. Для этого им не требуется дополнительный Load Balancer - достаточно определить список URL серверов Middleware в свойстве <<cuba.connectionUrlList,cuba.connectionUrlList>>.

В кластере серверов Middleware организуется взаимодействие для обмена информацией о пользовательских сессиях, блокировках и пр. При этом обеспечивается полная отказоустойчивость блока Middleware - при отключении одного из серверов выполнение запросов от клиентских блоков продолжается на доступном сервере прозрачно для пользователей.

Настройка данного варианта развертывания описана в <<cluster_mw,Настройка кластера Middleware>>.
^| image:scaling_4.png[align="center"]

|===

[[cluster_webclient]]
==== Настройка кластера Web Client

В данном разделе рассматривается следующая конфигурация развертывания:

image::cluster_webclient.png[align="center"]

Здесь на серверах `host1` и `host2` блок установлены инстансы Tomcat с веб-приложением `app`, реализующим блок Web Client. Пользователи обращаются к балансировщику нагрузки по адресу `http://host0/app`, который перенаправляет запрос этим серверам. На сервере `host3` установлен Tomcat с веб-приложением `app-core`, реализующим блок Middleware.

[[cluster_webclient_lb]]
===== Установка и настройка Load Balancer

Рассмотрим процесс установки балансировщика нагрузки на базе *Apache HTTP Server* для операционной системы *Ubuntu 14.04*.

. Выполните установку *Apache HTTP Server* и его модуля *mod_jk*:
+
`$ sudo apt-get install apache2 libapache2-mod-jk`

. Замените содержимое файла `/etc/libapache2-mod-jk/workers.properties` на следующее:
+
[source, properties]
----
workers.tomcat_home=
workers.java_home=
ps=/

worker.list=tomcat1,tomcat2,loadbalancer,jkstatus

worker.tomcat1.port=8009
worker.tomcat1.host=host1
worker.tomcat1.type=ajp13
worker.tomcat1.connection_pool_timeout=600
worker.tomcat1.lbfactor=1

worker.tomcat2.port=8009
worker.tomcat2.host=host2
worker.tomcat2.type=ajp13
worker.tomcat2.connection_pool_timeout=600
worker.tomcat2.lbfactor=1

worker.loadbalancer.type=lb
worker.loadbalancer.balance_workers=tomcat1,tomcat2

worker.jkstatus.type=status
----

. Добавьте в файл `/etc/apache2/sites-available/000-default.conf` следующее:
+
[source, xml]
----
<VirtualHost *:80>
...
    <Location /jkmanager>
        JkMount jkstatus
        Order deny,allow
        Allow from all
    </Location>

    JkMount /jkmanager/* jkstatus
    JkMount /app loadbalancer
    JkMount /app/* loadbalancer

</VirtualHost>
----

. Перезапустите сервис Apache HTTP:
+
`$ sudo service apache2 restart`


[[cluster_webclient_tomcat]]
===== Настройка серверов Web Client

На серверах Tomcat 1 и Tomcat 2 необходимо произвести следующие настройки:

. В файлах `tomcat/conf/server.xml` добавить параметр `jvmRoute`, эквивалентный имени worker, заданному в настройках балансировщика нагрузки - `tomcat1` и `tomcat2`:
+
[source, xml]
----
<Server port="8005" shutdown="SHUTDOWN">
  ...
  <Service name="Catalina">
    ...
    <Engine name="Catalina" defaultHost="localhost" jvmRoute="tomcat1">
      ...
    </Engine>
  </Service>
</Server>
----

. Задать следующие свойства приложения в файлах `tomcat/conf/app/local.app.properties`:
+
[source, properties]
----
cuba.useLocalServiceInvocation = false
cuba.connectionUrlList = http://host3:8080/app-core

cuba.webHostName = host1
cuba.webPort = 8080
cuba.webContextName = app
----
+
Параметры <<cuba.webHostName,cuba.webHostName>>, <<cuba.webPort,cuba.webPort>>, <<cuba.webContextName,cuba.webContextName>> не обязательны для работы кластера WebClient, но позволяют проще идентифицировать сервера в других механизмах платформы, например в <<jmx_console,консоли JMX>>. Кроме того, в экране *User Sessions* в атрибуте *Client Info* отображается сформированный из этих параметров идентификатор блока Web Client, на котором работает данный пользователь.

[[cluster_mw]]
==== Настройка кластера Middleware

В данном разделе рассматривается следующая конфигурация развертывания:

image::cluster_mw.png[align="center"]

Здесь на серверах `host1` и `host2` блок установлены инстансы Tomcat с веб-приложением `app`, реализующим блок Web Client. Настройка кластера этих серверов рассмотрена в <<cluster_webclient,предыдущем разделе>>. На серверах `host3` и `host4` установлены инстансы Tomcat с веб-приложением `app-core`, реализующим блок Middleware. Между ними настроено взаимодействие для обмена информацией о пользовательских сессиях и блокировках, сброса кэшей и др.

[[cluster_mw_client]]
===== Настройка обращения к кластеру Middleware

Для того, чтобы клиентские блоки могли работать с несколькими серверами Middleware, достаточно указать список URL этих серверов в свойстве приложения <<cuba.connectionUrlList,cuba.connectionUrlList>>. Для Web Client это можно сделать в файле `tomcat/conf/app/local.app.properties`:

[source, properties]
----
cuba.useLocalServiceInvocation = false
cuba.connectionUrlList = http://host3:8080/app-core,http://host4:8080/app-core

cuba.webHostName = host1
cuba.webPort = 8080
cuba.webContextName = app
----

Порядок серверов в списке `cuba.connectionUrlList` определяет приоритет, в котором клиент будет пытаться направлять запросы. Например в данном случае клиент сначала попытается вызвать `host3`, если он недоступен - то `host4`. Если запрос к `host4` завершился успешно, данный клиент ставит `host4` первым в своем списке и продолжает работать с ним. После перезапуска клиента список восстанавливается в первоначальное значение. Для обеспечения равномерного распределения клиентов между серверами используется свойство <<cuba.randomServerPriority,cuba.randomServerPriority>>.

[[cluster_mw_server]]
===== Настройка взаимодействия серверов Middleware

Сервера Middleware могут поддерживать общие списки <<userSession,пользовательских сессий>> и других объектов, а также координировать сброс кэшей. Для этого достаточно на каждом их них включить свойство приложения <<cuba.cluster.enabled,cuba.cluster.enabled>>. Пример файла `tomcat/conf/app-core/local.app.properties`:

[source, properties]
----
cuba.cluster.enabled = true

cuba.webHostName = host3
cuba.webPort = 8080
cuba.webContextName = app-core
----

Для серверов Middleware обязательно нужно указать правильные значения свойств <<cuba.webHostName,cuba.webHostName>>, <<cuba.webPort,cuba.webPort>> и <<cuba.webContextName,cuba.webContextName>> для формирования уникального <<serverId,Server Id>>.

Механизм взаимодействия основан на библиотеке link:$$http://www.jgroups.org$$[JGroups]. Для тонкой настройки взаимодействия служит файл `jgroups.xml`, расположенный в корне архива `cuba-core-<version>.jar`. Его можно скопировать в каталог `tomcat/conf/app-core` и настроить нужным образом.

Программный интерфейс для взаимодействия в кластере Middleware обеспечивает бин `ClusterManagerAPI`. Его можно использовать в приложении - см. JavaDocs и примеры использования в коде платформы.

[[serverId]]
==== Server Id

_Server Id_ служит для надежной идентификации серверов в кластере *Middleware*. Идентификатор имеет вид `host:port/context`, например:

[source, plain]
----
tezis.haulmont.com:80/app-core
----

[source, plain]
----
192.168.44.55:8080/app-core
----

Идентификатор формируется на основе параметров конфигурации <<cuba.webHostName,cuba.webHostName>>, <<cuba.webPort,cuba.webPort>>, <<cuba.webContextName,cuba.webContextName>>, поэтому крайне важно корректно указать эти параметры для блока *Middleware*, работающего в кластере.

Server Id может быть получен c помощью бина `ServerInfoAPI` или через JMX-интерфейс `<<serverInfoMBean,ServerInfoMBean>>`.

[[jmx_tools]]
=== Использование инструментов JMX

В данном разделе рассмотрены различные аспекты использования инструментов *Java Management Extensions* в CUBA-приложениях.

[[jmx_console]]
==== Встроенная JMX консоль

Модуль *Web Client* базового проекта *cuba* платформы содержит средство просмотра и редактирования JMX объектов. Точкой входа в этот инструмент является экран `com/haulmont/cuba/web/app/ui/jmxcontrol/browse/display-mbeans.xml`, зарегистрированный под идентификатором `jmxConsole` и в стандартном меню доступный через пункт *Администрирование* → *Консоль JMX*.

Без дополнительной настройки консоль отображает все JMX объекты, зарегистрированные в JVM, на которой работает блок *Web Client*, к которому в данный момент подключен пользователь. Соответственно, в простейшем случае развертывания всех блоков приложения в одном экземпляре веб-контейнера консоль имеет доступ к JMX бинам всех уровней, а также к JMX объектам самой JVM и веб-контейнера. 

Имена бинов приложения имеют префикс, соответствующий имени веб-приложения, их содержащего. Например, бин `app-core.cuba:type=CachingFacade` загружен веб-приложением *app-core*, реализующим блок *Middleware*, а бин `app.cuba:type=CachingFacade` загружен веб-приложением *app*, реализующим блок *Web Client*.

Консоль JMX может также работать с JMX объектами произвольной удаленной JVM. Это актуально в случае развертывания блоков приложения на нескольких экземплярах веб-контейнера, например, отдельно *Web Client* и *Middleware*. 

Для подключения к удаленной JVM необходимо в поле *Соединение JMX* консоли выбрать созданное ранее соединение, либо вызвать экран создания нового соединения:

.Редактирование JMX соединения
image::jmx-connection-edit.png[align="center"]

Для соединения указывается JMX хост и порт, логин и пароль. Имеется также поле *Имя узла*, которое заполняется автоматически, если по указанному адресу обнаружен какой-либо блок CUBA-приложения. В этом случае значением этого поля становится комбинация свойств `<<cuba.webHostName,cuba.webHostName>>` и `<<cuba.webPort,cuba.webPort>>` данного блока, что позволяет идентифицировать содержащий его сервер. Если подключение произведено к постороннему JMX интерфейсу, то поле *Имя узла* будет иметь значение "Unknown JMX interface". Значение данного поля можно произвольно изменять.

Для подключения удаленной JVM она должна быть соответствующим образом настроена - см. ниже.

[[jmx_remote_access]]
==== Настройка удаленного доступа к JMX

В данном разделе рассматривается настройка запуска сервера *Tomcat*, необходимая для удаленного подключения к нему инструментов JMX.

[[jmx_remote_access_tomcat_windows]]
===== Tomcat JMX под Windows

* Отредактировать файл `bin/setenv.bat` следующим образом:
+
[source, plain]
----
set CATALINA_OPTS=%CATALINA_OPTS% ^
-Dcom.sun.management.jmxremote ^
-Djava.rmi.server.hostname=192.168.10.10 ^
-Dcom.sun.management.jmxremote.ssl=false ^
-Dcom.sun.management.jmxremote.port=7777 ^
-Dcom.sun.management.jmxremote.authenticate=true ^
-Dcom.sun.management.jmxremote.password.file=../conf/jmxremote.password ^
-Dcom.sun.management.jmxremote.access.file=../conf/jmxremote.access
----
+
Здесь в параметре `java.rmi.server.hostname` необходимо указать реальный IP адрес или DNS имя компьютера, на котором запущен сервер, в параметре `com.sun.management.jmxremote.port` - порт для подключения инструментов JMX.

* Отредактировать файл `conf/jmxremote.access`. Он должен содержать имена пользователей, которые будут подключаться к JMX, и их уровень доступа. Например:
+
[source, plain]
----
admin readwrite
----

* Отредактировать файл `conf/jmxremote.password`. Он должен содержать пароли пользователей JMX, например:
+
[source, plain]
----
admin admin
----

* Файл паролей должен иметь разрешение на чтение только для пользователя, от имени которого работает сервер *Tomcat*. Настроить права можно следующим образом:

** Открыть командную строку и перейти в каталог `conf`.

** Выполнить команду:
+
`++cacls jmxremote.password /P "domain_name\user_name":R++`
+
где `++domain_name\user_name++` - домен и имя пользователя.

** После выполнения данной команды файл в *Проводнике* будет отмечен изображением замка.

* Если *Tomcat* установлен как служба Windows, то для службы должен быть задан вход в систему с учетной записью, имеющей права на файл `jmxremote.password`. Кроме того, следует иметь в виду, что в этом случае файл `bin/setenv.bat` не используется, и соответствующие параметры запуска JVM должны быть заданы в приложении, настраивающем службу.

[[jmx_remote_access_tomcat_linux]]
===== Tomcat JMX под Linux

* Отредактировать файл `bin/setenv.sh` следующим образом:
+
[source, bash]
----
CATALINA_OPTS="$CATALINA_OPTS -Dcom.sun.management.jmxremote \
-Djava.rmi.server.hostname=192.168.10.10 \
-Dcom.sun.management.jmxremote.port=7777 \
-Dcom.sun.management.jmxremote.ssl=false \
-Dcom.sun.management.jmxremote.authenticate=true"

CATALINA_OPTS="$CATALINA_OPTS -Dcom.sun.management.jmxremote.password.file=../conf/jmxremote.password -Dcom.sun.management.jmxremote.access.file=../conf/jmxremote.access"
----
+
Здесь в параметре `java.rmi.server.hostname` необходимо указать реальный IP адрес или DNS имя компьютера, на котором запущен сервер, в параметре `com.sun.management.jmxremote.port` - порт для подключения инструментов JMX.

* Отредактировать файл `conf/jmxremote.access`. Он должен содержать имена пользователей, которые будут подключаться к JMX, и их уровень доступа. Например:
+
[source, plain]
----
admin readwrite
----

* Отредактировать файл `conf/jmxremote.password`. Он должен содержать пароли пользователей JMX, например:
+
[source, plain]
----
admin admin
----

* Файл паролей должен иметь разрешение на чтение только для пользователя, от имени которого работает сервер *Tomcat*. Настроить права для текущего пользователя можно следующим образом:

** Открыть командную строку и перейти в каталог `conf`.

** Выполнить команду:
+
`chmod go-rwx jmxremote.password`

[[server_push_settings]]
=== Настройка server push

Приложения CUBA используют технологию server push в механизме <<background_tasks,фоновых задач>>. Это может потребовать дополнительной настройки сервера приложения и прокси-сервера (если таковой используется).

По умолчанию server push использует протокол WebSocket. Следующие свойства приложения влияют на функциональность server push платформы:

<<cuba.web.pushLongPolling,cuba.web.pushLongPolling>>

<<cuba.web.pushEnabled,cuba.web.pushEnabled>>

Если пользователи подключаются к серверу приложения через прокси, не поддерживающий WebSocket, рекомендуется установить свойство `cuba.web.pushLongPolling` в `true` и увеличить таймаут запроса на прокси до 10 минут или больше.

Ниже приведен пример конфигурации веб-сервера *Nginx* для использования WebSocket:

[source, plain]
----
location / {
    proxy_set_header X-Forwarded-Host $host;
    proxy_set_header X-Forwarded-Server $host;
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_read_timeout     3600;
    proxy_connect_timeout  240;
    proxy_set_header Host $host;
    proxy_set_header X-RealIP $remote_addr;

    proxy_pass http://127.0.0.1:8080/;
    proxy_set_header X-Forwarded-Proto $scheme;

    proxy_set_header Upgrade $http_upgrade;
    proxy_set_header Connection "upgrade";
}
----
