[[deployment]]
== Развертывание приложений

В данной главе рассматриваются различные аспекты развертывания и эксплуатации CUBA-приложений.

На диаграмме ниже приведена возможная структура развернутого приложения. 

image::DeploymentStructure.png[align="center"]

В приведенном варианте приложение обеспечивает отсутствие единой точки отказа, балансировку нагрузки и подключение различных типов клиентов. В простейшем случае, однако, серверная часть приложения может быть установлена на одном компьютере, содержащем, в том числе, и базу данных. Различные варианты развертывания в зависимости от нагрузки и требований к отказоустойчивости подробно рассмотрены в <<scaling,Масштабирование приложения>>.

[[app_dirs]]
=== Каталоги приложения

В данном разделе описываются каталоги файловой системы, используемые различными <<app_tiers,блоками приложения>> во время выполнения.

[[conf_dir]]
==== Конфигурационный каталог

Каталог конфигурации предназначен для размещения ресурсов, дополняющих и переопределяющих свойства приложения, пользовательский интерфейс и бизнес-логику после развертывания приложения. Переопределение обеспечивается механизмом загрузки интерфейса инфраструктуры `<<resources,Resources>>`, который сначала выполняет поиск в конфигурационном каталоге, а потом в CLASSPATH, так что одноименные ресурсы в конфигурационном каталоге имеют приоритет над расположенными в JAR-файлах и каталогах классов.

Конфигурационный каталог может содержать следующие типы ресурсов:

* Файл `<<app_properties_files,local.app.properties>>`, определяющий параметры развертывания блоков приложения, работающих под управлением веб-сервера.

* Конфигурационные файлы `<<metadata.xml,metadata.xml>>`, `<<persistence.xml,persistence.xml>>`, `<<views.xml,views.xml>>`, `<<remoting-spring.xml,remoting-spring.xml>>`.

* <<screen_xml,XML-дескрипторы>> экранов UI.

* <<screen_controller,Контроллеры>> экранов UI в виде исходных текстов Java или Groovy.

* Скрипты или классы Groovy, а также исходные тексты классов Java, используемые приложением через интерфейс `<<scripting,Scripting>>`.

Расположение конфигурационного каталога определяется свойством приложения `<<cuba.confDir,cuba.confDir>>`. Для блоков *Middleware*, *Web Client* и *Web Portal* в варианте <<fast_deployment,быстрого развертывания>> в *Tomcat* это подкаталог с именем веб-приложения в каталоге `tomcat/conf`, например `tomcat/conf/app-core` для *Middleware*.

[[work_dir]]
==== Рабочий каталог

Рабочий каталог используется приложением для хранения файлов данных и конфигурации.

Например, подкаталог `filestorage` рабочего каталога по умолчанию используется <<file_storage,хранилищем загруженных файлов>>. Кроме того, блок *Middleware* на старте сохраняет в рабочем каталоге сгенерированные файлы `<<persistence.xml,persistence.xml>>` и `orm.xml`.

Расположение рабочего каталога определяется свойством приложения `<<cuba.dataDir,cuba.dataDir>>`. Для блоков *Middleware*, *Web Client* и *Web Portal* в варианте <<fast_deployment,быстрого развертывания>> в *Tomcat* это подкаталог с именем веб-приложения в каталоге `tomcat/work`.

[[log_dir]]
==== Каталог журналов

В каталоге журналов создаются лог-файлы приложения.

Состав и настройка файлов журналов определяются конфигурацией фреймворка *Logback*. Расположение файла конфигурации определяется системным свойством <<logback.configurationFile,logback.configurationFile>>.

Данный каталог может быть также использован для сохранения произвольной информации о выполнении приложения. Путь к каталогу журналов определяется свойством приложения <<cuba.logDir,cuba.logDir>>. Для блоков Middleware, Web Client и Web Portal в варианте <<fast_deployment,быстрого развертывания>> в Tomcat это каталог `tomcat/logs`.

См. также <<logging, Логгирование>>.

[[temp_dir]]
==== Временный каталог

Данный каталог может быть использован для создания произвольных временных файлов во время выполнения приложения. Путь к временному каталогу определяется свойством приложения `<<cuba.tempDir,cuba.tempDir>>`. Для блоков *Middleware*, *Web Client* и *Web Portal* в варианте <<fast_deployment,быстрого развертывания>> в *Tomcat* это подкаталог с именем веб-приложения в каталоге `tomcat/temp`.

[[db_dir]]
==== Каталог скриптов базы данных

В данном каталоге развернутого блока *Middleware* хранится набор SQL скриптов создания и обновления БД.

Структура каталога скриптов повторяет описанную в <<db_scripts,>>, но имеет один дополнительный верхний уровень, разделяющий скрипты используемых <<app_components,компонентов>> и самого приложения. Нумерация каталогов верхнего уровня определяется во время сборки проекта.

Расположение каталога скриптов БД определяется свойством приложения <<cuba.dbDir,cuba.dbDir>>. В варианте <<fast_deployment,быстрого развертывания>> в Tomcat это подкаталог `WEB-INF/db` каталога веб-приложения среднего слоя: `tomcat/webapps/app-core/WEB-INF/db`.

[[deployment_variants]]
=== Варианты развертывания

В данном разделе рассматриваются различные варианты развертывания CUBA-приложений.

[[fast_deployment]]
==== Быстрое развертывание в Tomcat

Быстрое развертывание используется по умолчанию при разработке приложения, так как обеспечивает минимальное время сборки, установки и старта приложения. Данный вариант удобен также и для эксплуатации приложения.

Быстрое развертывание производится с помощью задачи <<build.gradle_deploy,deploy>>, объявленной для модулей core и web в файле `build.gradle`. Перед первым выполнением `deploy` необходимо установить и проинициализировать локальный сервер Tomcat с помощью задачи <<build.gradle_setupTomcat,setupTomcat>>. 

В результате быстрого развертывания в каталоге, задаваемом свойством `ext.tomcatDir` скрипта `build.gradle` создается следующая структура (перечислены только важные каталоги и файлы, описанные ниже):

[source, plain]
----
bin/
    setenv.bat, setenv.sh
    startup.bat, startup.sh
    debug.bat, debug.sh
    shutdown.bat, shutdown.sh

conf/
    catalina.properties
    server.xml
    logback.xml
    logging.properties
    Catalina/
        localhost/
    app/
    app-core/

lib/
    hsqldb-2.2.9.jar

logs/
    app.log

shared/
    lib/

temp/
    app/
    app-core/

webapps/
    app/
    app-core/

work/
    app/
    app-core/
----

* `bin` - каталог, содержащий средства запуска и остановки сервера Tomcat:

** `setenv.bat`, `setenv.sh` - скрипты установки переменных окружения. Эти скрипты следует использовать для установки параметров памяти JVM, указания файла конфигурации <<logging_setup_tomcat,логгирования>>, настройки <<jmx_remote_access,доступа по JMX>>, параметров <<debug_setup,подключения отладчика>>.

** `startup.bat`, `startup.sh` - скрипты запуска Tomcat. Сервер стартует в отдельном консольном окне в *Windows* и в фоне в **nix*.
+
Для запуска сервера в текущем консольном окне вместо `startup.*` используйте команды
+
`> catalina.bat run`
+
`$ ./catalina.sh run`

** `debug.bat`, `debug.sh` - скрипты, аналогичные `++startup.*++`, однако запускающие Tomcat с возможностью подключения отладчика. Именно эти скрипты запускаются при выполнении задачи <<build.gradle_start,start>> скрипта сборки.

** `shutdown.bat`, `shutdown.sh` - скрипты остановки Tomcat.

* `conf` - каталог, содержащий файлы конфигурации Tomcat и развернутых в нем приложений.

** `catalina.properties` - свойства Tomcat. Для загрузки общих библиотек из каталога `shared/lib` (см. ниже) данный файл должен содержать строку:
+
[source, properties]
----
shared.loader=${catalina.home}/shared/lib/*.jar
----

** `server.xml` - описатель конфигурации Tomcat. В этом файле можно изменить порты сервера.

** `logback.xml` - описатель конфигурации <<logging_setup_tomcat,логгирования>> приложений.

** `logging.properties` - описатель конфигурации логгирования самого сервера Tomcat.

** `Catalina/localhost` - в этом каталоге можно разместить дескрипторы развертывания приложений <<context.xml,context.xml>>. Дескрипторы, расположенные в данном каталоге имеют приоритет над дескрипторами в каталогах `META-INF` самих приложений, что часто бывает удобно при эксплуатации системы. Например, в таком дескрипторе на уровне сервера можно указать параметры подключения к базе данных, отличные от указанных в самом приложении.
+
Дескриптор развертывания на уровне сервера должен иметь имя приложения и расширение `.xml`. То есть для создания такого дескриптора, например, для приложения `app-core`, необходимо скопировать содержимое файла `webapps/app-core/META-INF/context.xml` в файл `conf/Catalina/localhost/app-core.xml`.

** `app` - <<conf_dir,конфигурационный каталог>> приложения веб-клиента `app`.

** `app-core` - <<conf_dir,конфигурационный каталог>> приложения среднего слоя `app-core`.

* `lib` - каталог библиотек, загружаемых в _common classloader_ сервера. Эти библиотеки доступны как самому серверу, так и всем развернутым в нем веб-приложениям. В частности, в данном каталоге должны располагаться JDBC-драйверы используемых баз данных (`hsqldb-XYZ.jar`, `postgresql-XYZ.jar` и т.д.)

* `logs` - каталог <<logging,логов>> приложений и сервера. Основной лог-файл приложений - `app.log`.

* `shared/lib` - каталог библиотек, доступных всем развернутым приложениям. Классы этих библиотек загружаются в специальный _shared classloader_ сервера. Использование shared classloader задается в файле `conf/catalina.properties` как описано выше.
+
Задачи <<build.gradle_deploy,deploy>> файла сборки копируют в этот каталог все библиотеки, не перечисленные в параметре `jarNames`, то есть не специфичные для данного приложения.

* `temp/app`, `temp/app-core` - <<temp_dir,временные каталоги>> приложений веб-клиента и среднего слоя.

* `webapps` - каталог веб-приложений. Каждое приложение располагается в собственном подкаталоге в формате _exploded WAR_.
+
Задачи <<build.gradle_deploy,deploy>> файла сборки создают подкаталоги приложений с именами, указанными в параметрах `appName`, и кроме прочего копируют в их подкаталоги `WEB-INF/lib` библиотеки, перечисленные в параметре `jarNames`.

* `work/app`, `work/app-core` - <<work_dir,рабочие каталоги>> приложений веб-клиента и среднего слоя.

[[tomcat_in_prod]]
===== Использование Tomcat при эксплуатации приложения

Процедура <<fast_deployment,быстрого развертывания>> по умолчанию создает веб приложения `app` и `app-core`, работающие на локальном инстансе Tomcat на порту 8080. Это означает, что веб клиент доступен по адресу `++http://localhost:8080/app++`.

Вы можете использовать этот экземпляр Tomcat для эксплуатации приложения, просто скопировав его на сервер. После этого необходимо установить имя хоста сервера в файлах `conf/app/local.app.properties` и `conf/app-core/local.app.properties` (создайте файлы если они не существуют):

[source, properties]
----
  cuba.webHostName = myserver
  cuba.webAppUrl = http://myserver:8080/app
---- 

Кроме того, необходимо настроить подключение к production базе данных. Это можно сделать в файле <<context.xml>> веб-приложения (`webapps/app-core/META-INF/context.xml`), или скопировать этот файл в `conf/Catalina/localhost/app-core.xml` как описано в предыдущем разделе, чтобы разделить настройки соединения с БД для разработки и эксплуатации.

Базу данных для production можно создать из бэкапа той базы, которая использовалась при разработке, либо настроить автоматическое создание и обновление БД. См. <<db_update_in_prod>>.

Если вы хотите изменить порт Tomcat или веб-контекст (последнюю часть URL после `/`), используйте *Studio*:

* Откройте проект в Studio.

* Перейдите в *Project Properties* > *Edit* > *Advanced*.

* Чтобы изменить веб-контекст, отредактируйте поле *Modules prefix*.

* Чтобы изменить порт Tomcat, отредактируйте поле *Tomcat ports* > *HTTP port*.

Если для веб клиента вы хотите использовать корневой контекст (`++http://myserver:8080++`), переименуйте каталоги `app` в `ROOT`

[source, plain]
----
tomcat/
  conf/
      ROOT/
          local.app.properties
      app-core/
          local.app.properties
  webapps/
      ROOT/
      app-core/
----

и используйте `/` в качестве веб контекста в файле `conf/ROOT/local.app.properties`:

[source, properties]
----
cuba.webContextName = /
---- 

[[war_deployment]]
==== Развертывание WAR в Jetty

CUBA-приложения можно разворачивать в файлы с помощью задачи сборки <<build.gradle_buildWar,buildWar>> и выполнять на любом контейнере сервлетов Java. Рассмотрим пример сборки WAR-файлов и их развертывания на сервере *Jetty*.

. Добавьте в конец <<build.gradle,build.gradle>> задачу сборки <<build.gradle_buildWar,buildWar>>:
+
[source, groovy]
----
include::{sourcesdir}/deployment/warDeployment_1.groovy[]
----
+
[TIP]
====
В данном случае собирается два WAR-файла, отдельно для блоков Middleware и Web Client. Если вы хотите объединить блоки в одном WAR-файле с помощью параметра `singleWar = true`, предоставьте также специальный файл `web.xml`, описанный в <<build.gradle_buildWar,этой секции>>.
====

. Запустите сборку:
+
[source, plain]
----
gradlew buildWar
----
+
В результате в подкаталоге `build\distributions\war` проекта будут созданы файлы `app-core.war` и `app.war`.

. Создайте домашний каталог приложения на сервере, например, `c:\work\app_home`.

. Загрузите и установите сервер Jetty, например в каталог `c:\work\jetty-home`. Данный пример тестировался на версии `jetty-distribution-9.3.6.v20151106.zip`.

. Создайте каталог `c:\work\jetty-base`, откройте в нем командную строку и выполните:
+
[source, plain]
----
java -jar c:\work\jetty-home\start.jar --add-to-start=http,jndi,deploy,plus,ext,resources
----

. Создайте файл `c:\work\jetty-base\app-jetty.xml` следующего содержания (для БД PostgreSQL с именем `test`):
+
[source, xml]
----
include::{sourcesdir}/deployment/warDeployment_2.xml[]
----

. Добавьте следующий текст в начало файла `c:\work\jetty-base\start.ini`:
+
[source, plain]
----
include::{sourcesdir}/deployment/warDeployment_3.ini[]
----

. Скопируйте JDBC-драйвер используемой базы данных в каталог `c:\work\jetty-base\lib\ext`. Файл драйвера можно взять из каталога `lib` CUBA Studio, либо из каталога `build\tomcat\lib` проекта. В случае PostgreSQL это файл `postgresql-9.1-901.jdbc4.jar`.

. Скопируйте файлы WAR в каталог `c:\work\jetty-base\webapps`.

. Откройте командную строку в каталоге `c:\work\jetty-base` и выполните:
+
[source, plain]
----
java -jar c:\work\jetty-home\start.jar
----

. Откройте `++http://localhost:8080/app++` в веб-браузере.

[[tomcat_war_deployment]]
==== Развертывание WAR в Tomcat Windows Service

. Добавьте в конец <<build.gradle,build.gradle>> задачу сборки <<build.gradle_buildWar,buildWar>>:
+
--
[source, groovy]
----
include::{sourcesdir}/deployment/warDeployment_2.groovy[]
----

Если параметры сервера отличаются от параметров локального Tomcat, используемого для <<fast_deployment,быстрого развертывания>>, укажите соответствующие свойства приложения:

[source, groovy]
----
include::{sourcesdir}/deployment/warDeployment_3.groovy[]
----

Можно также указать отдельный `context.xml` для настройки соединения с production БД, например:

[source, groovy]
----
include::{sourcesdir}/deployment/warDeployment_4.groovy[]
----
--

. Запустите задачу `buildWar`. В результате, в каталоге `build/distibutions` проекта будут сгенерированы файлы `app.war` и `app-core.war`.
+
[source, plain]
----
gradlew buildWar
----

. Скачайте и установите Tomcat 8 Windows Service Installer.

. После установки, перейдите в подкаталог `bin` установленного сервера и запустите `tomcat8w.exe` от имени администратора.
На вкладке *Java* установите параметр *Maximum memory pool* 1024MB. Перейдите на вкладку *General* и запустите сервис.
+
image::tomcatPropeties.jpg[align="center"]

. Пропишите `-Dfile.encoding=UTF-8` в поле _Java Options_.

. Скопируйте сгенерированные файлы `app.war` и `app-core.war` в подкаталог `webapps` сервера.

. Запустите сервис Tomcat.

. Откройте `++http://localhost:8080/app++` в браузере.


[[jelastic_deployment]]
==== Развертывание в облаке Jelastic

CUBA Studio позволяет легко развернуть приложение в облаке link:$$https://jelastic.com/$$[Jelastic].

[TIP]
====
В данный момент развертывание в облаке возможно только для проектов, использующих в качестве сервера базы данных PostgreSQL или HSQL.
====

. Нажмите на ссылку *Deployment settings* в секции *Project properties* и перейдите на вкладку *CLOUD*.

. Если для данного проекта еще нет настроек развертывания в облаке, вы можете использовать поле вверху открывшейся страницы для создания бесплатной тестовой учетной записи Jelastic.

. После завершения регистрации введите email, пароль и выбранного хостинг-провайдера.
+
image::jelastic_1.png[align="center"]

. В поле *Environment* вводится имя окружения, в которое будет развернут WAR. Нажмите на кнопку с троеточием и выберите существующее окружение, или создайте новое. Вы можете проверить окружение на совместимость с вашим проектом. Совместимое окружение должно иметь Java 8, Tomcat 8 и PostgreSQL 9.1+ (если в проекте используется база данных PostgreSQL). Если ваш проект использует PostgreSQL, вы получите email с информацией о подключении к БД. Используйте эту информацию при генерации context.xml, см. поле *Custom context.xml path* ниже. Кроме того, вы должны создать пустую базу данных PostgreSQL через веб-интерфейс провайдера, ссылка на который содержится в письме. Выбранное имя базы данных должно быть указано позже в context.xml.
+
image::jelastic_6.png[align="center"]

. Нажмите кнопку *Generate* рядом с полем *Custom web.xml path*. Studio сгенерирует специальный `web.xml` для <<build.gradle_buildWar,единого WAR>>, содержащего блоки Middleware и Web Client.
+
image::jelastic_2.png[align="center"]

. Если проект использует HSQLDB, то это все - вы можете нажать *OK* и запустить развертывание командой *Run > Deploy to cloud* главного меню.

. Если проект использует PostgreSQL, перейдите в административный веб-интерфейс по ссылке в письме, полученном после создания Environment, и создайте базу данных.

. Нажмите кнопку *Generate* рядом с полем *Custom context.xml path* и укажите пользователя, пароль, хомт и имя базы данных.
+
image::jelastic_3.png[align="center"]

. Оставьте флажки *Include JDBC driver* и *Include context.xml* включенными.
+
image::jelastic_4.png[align="center"]

. Нажмите *OK* и запустите развертывание командой *Run > Deploy to cloud* главного меню.

. После завершения развертывания используйте сссылку в левом нижнем углу чтобы открыть веб-интерфейс приложения.
+
image::jelastic_5.png[align="center"]

[[bluemix_deployment]]
==== Deployment to Bluemix Cloud

С помощью CUBA Studio можно легко развернуть приложение в облаке IBM® Bluemix®.

[TIP]
====
Развёртывание в облаке Bluemix в настоящее время рекомендуется только для проектов, использующих базу данных PostgreSQL. HSQLDB доступна только с опцией _in-process_, таким образом, база данных будет пересоздаваться каждый раз при перезапуске облачного приложения, соответственно, пользовательские данные будут потеряны.
====

. Создайте учётную запись в сервисе Bluemix. Также скачайте и установите следующее программное обеспечение:
.. Bluemix CLI: http://clis.ng.bluemix.net/ui/home.html
.. Cloud Foundry CLI: https://github.com/cloudfoundry/cli/releases
.. После установки убедитесь, что команды `bluemix` и `cf` работают в командной строке. При необходимости добавьте путь к исполняемым файлам `\IBM\Bluemix\bin` в переменную среды `PATH`.

. Создайте новое пространство (Space) в облаке Bluemix, задайте ему любое имя. В дальнейшем вы можете поместить несколько приложений в одно пространство.

. Добавьте к созданному пространству сервер приложений Tomcat: *Create App* -> *CloudFoundry Apps* -> *Tomcat*.

. Задайте имя приложения. Имя должно быть уникальным, так как на его основе строится URL, по которому WEB-приложение будет доступно впоследствии.

. Чтобы добавить к пространству подходящий сервис базы данных, нажмите *Create service* в панели управления пространством и выберите *ElephantSQL*.

. Откройте панель управления приложением (ранее созданный Tomcat) и подключите сервис базы данных к приложению. Нажмите *Connect Existing*. Чтобы изменения вступили в силу, система предлагает обновить (restage) приложение. На данном этапе в этом нет необходимости: сервер Tomcat будет обновлен позже при развертывании CUBA-приложения.

. После подключения сервиса базы данных к приложению параметры подключения к СУБД будут доступны по кнопке *View Credentials*. Также параметры подключения к СУБД сохраняются в переменной среды `VCAP_SERVICES` облачного приложения и доступны по команде `cf&#160;env`. Созданная БД доступна глобально, управлять базой данных можно по указанному URL.

. Настройте CUBA-проект на базу данных PostgreSQL (на СУБД, аналогичную той которую Вы используете в облаке Bluemix).

. Создайте скрипты базы данных и запустите локальный сервер Tomcat. Убедитесь, что приложение работоспособно.

. Создайте WAR-файл, при помощи которого приложение будет равзернуто в сервер Tomcat.
.. Нажмите *Deployment Settings* в разделе *Project Properties* панели навигатора Studio.
.. Перейдите на вкладку *WAR*.
.. При помощи чекбоксов выберите все доступные опции: для корректного развертывания в облаке необходим единый *Single WAR* файл с помещёнными в него драйвером базы данных и конфигурационным файлом `context.xml`.
+
image::bluemix_war_settings.png[align="center"]

.. Нажмите кнопку *Generate* рядом с полем *Custom context.XML*. В появившемся диалоге укажите параметры подключения к базе данных - сервису в облаке Bluemix.
+
Используйте параметры из строки `uri` сервиса, как в примере ниже:
+
[source, json]
----
include::{sourcesdir}/deployment/bluemix_credentials.json[]
----
+
*Database user*: `ldwpelpl`
+
*Database password*: `eFwXx6lNFLheO5maP9iRbS77Sk1VGO_T`
+
*Database URL*: `echo-01.db.elephantsql.com:5432`
+
*Database name*: `ldwpelpl`

.. Нажмите кнопку *Generate* для создания собственного файла `web.xml`, необходимого для единого WAR-файла.

.. Сохраните настройки. Создайте WAR-файл, выполнив команду Gradle `buildWar` в Studio или из командной строки.
+
image::bluemix_buildWar.png[align="center"]
+
В результате, в папке проекта `build/distributions/war/` появился файл `app.war`.

. В корневом каталоге прокекта вручную создайте файл `manifest.yml` со следующим содержимым:
+
[source, yml]
----
include::{sourcesdir}/deployment/bluemix_manifest.yml[]
----
+
где
+
* `path` - относительный путь к сгенерированному WAR-файлу.
* `memory`: по умолчанию  серверу Tomcat выделяется лимит памяти в 1G. При необходимости вы можете уменьшить или увеличить объём выделенной памяти, эта настройка также доступна через WEB-интерфейс Bluemix. Учтите, что количество памяти, выделенной приложению, влияет на стоимость облачного размещения.
* `name` - имя сервера приложения Tomcat, созданного в облаке.
* `host`: идентично имени приложения.
* `env`: этим параметром задаются переменные среды. В нашем случае переменными среды задаются версии Tomcat и Java, необходимые для правильного функционирования CUBA-приложения.

. В комадной строке перейдите в корневой каталог проекта CUBA.
+
[source, yml]
----
cd your_project_directory
----

. Создайте подключение к Bluemix.
+
[source, yml]
----
bluemix api https://api.ng.bluemix.net
----

. Зайдите в Вашу учетную запись Bluemix.
+
[source, yml]
----
cf login
----

.  Разверните созданный WAR в облачный Tomcat.
+
[source, yml]
----
cf push
----
+
Команда `push` использует параметры, указанные в конфигурационном файле `manifest.yml`.

. Посмотреть логи сервера Tomcat можно на вкладке *Logs* панели управления приложением в WEB-интерфейсе Bluemix, а также в командной строке при помощи команды
+
[source, yml]
----
cf logs cuba-app --recent
----

. По завершению процесса развёртывания CUBA-приложение будет доступно в облаке Bluemix. Чтобы его открыть, воспользуйтесь URL `host.domain` в браузере. Этот URL будет отображаться в поле *ROUTE* таблицы ваших приложений *Cloud Foundry Apps*.


[[heroku_deployment]]
==== Развертывание в облаке Heroku

Данный раздел описывает порядок развертывания приложения CUBA в облаке https://www.heroku.com/[Heroku®].

[TIP]
=====
Это руководство охватывает процесс развертывания проекта с использованием базы данных PostgreSQL.
=====

[[heroku_war_deployment]]
===== Развертывание WAR-файла в Heroku

Учетная запись Heroku::
+
--
Создайте учетную запись в Heroku с помощью веб-браузера, будет достаточно бесплатного аккаунта `hobby-dev`. Затем войдите в аккаунт и создайте новое приложение с помощью кнопки *New* в верхней части страницы.

Задайте уникальное имя приложения (либо оставьте поле пустым, чтобы имя назначилось автоматически) и выберите подходящее геоположение сервера. Вы зарегистрировали приложение, например `morning-beach-4895`, это будет название приложения Heroku.

Сначала вас переадресует на вкладку *Deploy*. Выберите там метод развертывания *Heroku Git*.
--

Командная строка Heroku (CLI)::
+
--
. Установите на компьютер программное обеспечение https://devcenter.heroku.com/articles/heroku-command-line[Heroku CLI].

. Создайте пустую папку для проекта CUBA. В дальнейшем для этой папки будет использоваться переменная `$PROJECT_FOLDER`.

. Откройте командную строку в любой папке вашего компьютера и наберите команду:
+
[source]
----
heroku login
----

. По запросу введите логин и пароль для Heroku. Начиная с текущего момента от вас больше не потребуется вводить логин и пароль для команд heroku.

. Установите плагин *Heroku CLI deployment plugin*:
+
[source]
----
heroku plugins:install heroku-cli-deploy
----
--

База данных PostgreSQL::
+
--
С помощью браузера пройдите на страницу https://data.heroku.com/[Heroku data]

Вы можете использовать существующую базу Postgres или создать новую. Далее описываются шаги по созданию новой БД.
. Найдите на странице блок *Heroku Postgres* и нажмите кнопку *Create one*
. На следующем экране нажмите кнопку *Install Heroku Postgr...*
. Далее подключите базу к приложению Heroku, выбрав его из выпадающего списка
. Далее выберите тарифный план (например, бесплатный `hobby-dev`)

Как вариант, вы можете установить PostgreSQL с помощью Heroku CLI:

[source]
----
heroku addons:create heroku-postgresql:hobby-dev --app morning-beach-4895
----

Здесь `morning-beach-4895` это название вашего приложения Heroku.

Теперь вы можете увидеть новую БД на вкладке *Resources*. База соединена с приложением Heroku. Чтобы получить детали для подключения к сервису БД, перейдите на страницу *Datasource* вашей БД в Heroku, опуститесь вниз до секции *Administration* и нажмите кнопку *View credentials*.

----
Host compute.amazonaws.com
Database d2tk
User nmmd
Port 5432
Password 9c05
URI postgres://nmmd:9c05@compute.amazonaws.com:5432/d2tk
----
--

Настройки проекта перед развертыванием::
+
--
. Создайте новый проект CUBA в папке `$PROJECT_FOLDER` с помощью CUBA Studio.
. В Студии перейдите к секции *Project Properties*, нажмите *Edit*, установите в поле *Database type* в качестве БД *PostgreSQL*, а затем сохраните настройки.
. Перейдите к секции *Data Model*, выполните действие, нажав сначала *Create DB scripts* и затем *Save and close*.

Выполните дейтвие *Deployment settings*, перейдите на вкладку *WAR* и отредактируйте настройки, как описано ниже.

* Включите *Build WAR*
* Задайте точку '.' в качестве домашнего каталога приложения в поле *Application home directory*
* Включите *Include JDBC driver*
* Включите *Include Tomcat's context.xml*
* Нажмите кнопку *Generate*, находящуюся справа от поля *Custom context.xml path*. Во всплывающем окне заполните параметры подключения к БД
* Откройте сгенерированный файл `modules/core/web/META-INF/war-context.xml` и проверьте детали подключения:
+
[source, xml]
----
<Context>
    <!-- Database connection -->
    <Resource
      name="jdbc/CubaDS"
      type="javax.sql.DataSource"
      maxTotal="20"
      maxIdle="2"
      maxWaitMillis="5000"
      driverClassName="org.postgresql.Driver"
      url="jdbc:postgresql://compute.amazonaws.com/d2tk"
      username="nmmd"
      password="9c05"/>

      <!-- ... -->
</Context>
----
* Отметьте галочкой *Single WAR for Middleware and Web Client*
* Нажмите кнопку *Generate* справа от поля *Custom web.xml path*
* Скопируйте код, приведенный ниже, в поле *App properties*:
+
[source]
----
[
  'cuba.automaticDatabaseUpdate' : true
]
----
Сохраните настройки.
--

Сборка WAR-файла::
+
--
Соберите WAR-файл, выполнив команду `buildWar` в Gradle. Вы можете сделать это прямо в Студии, открыв диалог *Search*, или из командной строки:

[source]
----
gradlew buildWar
----

Проект CUBA использует Gradle wrapper (gradlew). Чтобы иметь возможность работать с командой `gradlew`, заранее создайте Gradle wrapper, использовав команду меню *Build > Create or update Gradle wrapper*.
--

Настройка приложения::
+
--

* Загрузите JAR-файл Tomcat Webapp Runner из репозитория https://mvnrepository.com/artifact/com.github.jsimone/webapp-runner. Версия Webapp Runner должна соответствовать используемой версии Tomcat. К примеру, Webapp Runner версии 8.5.11.3 подходит для Tomcat версии 8.5.11. Переименуйте JAR-файл в `webapp-runner.jar` и поместите его в корень проекта `$PROJECT_FOLDER`.

* Загрузите JAR-файл Tomcat DBCP из репозитория https://mvnrepository.com/artifact/org.apache.tomcat/tomcat-dbcp.
  Используйте версию, соответствующую вашему Tomcat, например 8.5.11. Создайте папку `$PROJECT_FOLDER/libs`, переименуйте JAR-файл в `tomcat-dbcp.jar` и поместите его в папку `$PROJECT_FOLDER/libs`.

* Создайте файл с названием `Procfile` в `$PROJECT_FOLDER`. Файл должен содержать следующий текст:
+
[source, bash]
----
web: java $JAVA_OPTS -cp webapp-runner.jar:libs/* webapp.runner.launch.Main --enable-naming --port $PORT build/distributions/war/app.war
----
--

Настройка Git::
+
--
Откройте командную строку в папке `$PROJECT_FOLDER` и запустите команды, указанные ниже:

[source]
----
git init
heroku git:remote -a morning-beach-4895
git add .
git commit -am "Initial commit"
----
--

Развертывание приложения::
+
--
Откройте командную строку в папке `$PROJECT_FOLDER` и запустите команды, указанные ниже:

Для *nix:

[source]
----
heroku jar:deploy webapp-runner.jar --includes libs/tomcat-dbcp.jar:build/distributions/war/app.war --app morning-beach-4895
----

Для Windows:

[source]
----
heroku jar:deploy webapp-runner.jar --includes libs\tomcat-dbcp.jar;build\distributions\war\app.war --app morning-beach-4895
----

Откройте вкладку *Resources* в панели управления Heroku. Должна появиться новая сущность Dyno с командой из вашего `Procfile`:

image::heroku_dyno.png[align="center"]

Приложение в данный момент разворачивается. Вы можете отслеживать процесс по логам.
--

Мониторинг логов::
+
--
Дождитесь сообщения в командной строке `++https://morning-beach-4895.herokuapp.com/  deployed to Heroku++`.

Чтобы отслеживать данные в логах, запускайте в командной строке из любой папки следующую команду:

[source]
----
heroku logs --tail --morning-beach-4895
----
--

После завершения процесса развертывания ваше приложение будет доступно в браузере по ссылке `++https://morning-beach-4895.herokuapp.com++`.

Вы также можете открыть приложение с помощью кнопки *Open app*, расположенной на панели Heroku.


[[heroku_tomcat_deployment]]
===== Tomcat Deployment to Heroku

Данное руководство предназначено для тех пользователей, у кого проект CUBA размещен на GitHub.
Чтобы инициировать процесс развертывания в Heroku, вам нужно выполнить Push в удаленный Git репозиторий. По этой причине на вашем компьютере должен быть установлен Git. Как вариант, вы можете установить Git, встроенный в Heroku CLI, выбрав соответствующий пункт при установке Heroku CLI. Также вам необходим GitHub репозиторий, содержащий проект CUBA.

Учетная запись Heroku::
+
--
Создайте учетную запись в Heroku с помощью веб-браузера, будет достаточно бесплатного аккаунта `hobby-dev`. Затем войдите в аккаунт и создайте новое приложение с помощью кнопки *New* в верхней части страницы.

Задайте уникальное имя приложения (либо оставьте поле пустым, чтобы имя назначилось автоматически) и выберите подходящее геоположение сервера. Вы зарегистрировали приложение, например `space-sheep-02453`, это будет название приложения Heroku.

Сначала вас переадресует на вкладку *Deploy*. Выберите там метод развертывания *GitHub*. Следуйте инструкциям на экране, чтобы авторизоваться в учетную запись GitHub.
Нажмите кнопку *Search*, чтобы вывести список доступных репозиториев GitHub вашей учетной записи, затем подключите желаемый репозиторий с проектом CUBA. Когда приложение Heroku подсоединено к GitHub, то вам доступна функция автоматического развертывания приложения *Automatic Deploys*. Это позволяет развертывать приложение в Heroku автоматически при каждом событии `git push`. В этом руководстве данная опция включена.
--

Командная строка Heroku (CLI)::
+
--
. Установите на компьютер программное обеспечение https://devcenter.heroku.com/articles/heroku-command-line[Heroku CLI]
. Откройте командную строку в любой папке вашего компьютера и наберите команду:
+
[source]
----
heroku login
----
+
. По запросу введите логин и пароль для Heroku. Начиная с текущего момента от вас больше не потребуется вводить логин и пароль для команд heroku.
--

База данных PostgreSQL::
+
--
. Откройте https://dashboard.heroku.com[панель] Heroku в веб-браузере
. Перейдите на вкладку *Resources*
. Нажмите кнопку *Find more add-ons*, чтобы найти дополнения для подключения СУБД
. Найдите блок *Heroku Postgres* и нажмите его. Проследуйте инструкциям на экране, нажмите кнопки *Login to install* / *Install Heroku Postgres* для установки дополнения.

Как вариант, вы можете установить PostgreSQL с помощью Heroku CLI:

[source]
----
heroku addons:create heroku-postgresql:hobby-dev --app space-sheep-02453
----
где `space-sheep-02453` это имя вашего Heroku приложения.

Теперь вы можете увидеть новую БД на вкладке *Resources*. База соединена с приложением Heroku. Чтобы получить детали для подключения к сервису БД, перейдите на страницу *Datasource* вашей БД в Heroku, опуститесь вниз до секции *Administration* и нажмите кнопку *View credentials*.

----
Host compute.amazonaws.com
Database zodt
User artd
Port 5432
Password 367f
URI postgres://artd:367f@compute.amazonaws.com:5432/zodt
----
--

Настройки проекта перед развертыванием::
+
--
. Перейдите в папку проекта CUBA (`$PROJECT_FOLDER`) на вашем компьютере
. Скопируйте содержимое файла `modules/core/web/META-INF/context.xml` в `modules/core/web/META-INF/heroku-context.xml`
. Впишите в файл `heroku-context.xml` актуальные данные для подключения в БД (см. пример ниже):
+
[source, xml]
----
<Context>
    <Resource driverClassName="org.postgresql.Driver"
              maxIdle="2"
              maxTotal="20"
              maxWaitMillis="5000"
              name="jdbc/CubaDS"
              password="367f"
              type="javax.sql.DataSource"
              url="jdbc:postgresql://compute.amazonaws.com/zodt"
              username="artd"/>

    <Manager pathname=""/>
</Context>
----
--

Настройка сборки::
+
--
Добавьте следующую задачу Gradle в ваш файл `$PROJECT_FOLDER/build.gradle`

[source, groovy]
----
include::{sourcesdir}/deployment/heroku_buildGradle.groovy[]
----
--

Procfile::
+
--
Команда, которая запускает приложение в Heroku, передается через специальный файл `Procfile`. Создайте файл с названием `Procfile` в папке `$PROJECT_FOLDER`, содержащий следующий текст:

[source]
----
web: cd ./deploy/tomcat/bin && export 'JAVA_OPTS=-Dport.http=$PORT' && ./catalina.sh run
----

Это передает значение переменной среды JAVA_OPTS в Tomcat, который в свою очередь запускает скрипт Catalina.
--

Премиум дополнения::
+
--
Если ваш проект использует премиальные дополнения CUBA, то укажите дополнительные переменные в приложении Heroku.

. Откройте панель Heroku в браузере
. Перейдите на вкладку *Settings*
. Разверните секцию *Config Variables*, нажав кнопку *Reveal Config Vars*
. Добавьте новые переменные *Config Vars*, используя части вашего лицензионного ключа (разделенные дефисом) как *username* и *password*:
[source]
----
CUBA_PREMIUIM_USER    | username
CUBA_PREMIUM_PASSWORD | password
----
--

Gradle wrapper::
+
--
Проект CUBA использует Gradle wrapper (gradlew). Чтобы иметь возможность работать с командой `gradlew`, заранее создайте Gradle wrapper, использовав команду меню *Build > Create or update Gradle wrapper*.

. Создайте файл `system.properties` в папке `$PROJECT_FOLDER` следующего содержания (пример соответствует локально установленной версии JDK 1.8.0_121):
+
[source]
----
java.runtime.version=1.8.0_121
----
+
. Убедитесь, что файлы `Procfile`, `system.properties`, `gradlew`, `gradlew.bat` и `gradle` не включены в `.gitignore`
. Добавьте эти файлы в репозиторий и выполните коммит:

[source]
----
git add gradlew gradlew.bat gradle/* system.properties Procfile
git commit -am "Gradle Wrapper, Procfile added"
----
--

Развертывание приложения::
+
--
Как только вы выполните Push изменений в GitHub, то Heroku начинает разворачивать приложение.

[source]
----
git push
----

Контроль процесса развертывания осуществляется в панели Heroku на вкладке *Activity*. Перейдите по ссылке *View build log*, чтобы отслеживать лог.


После завершения процесса развертывания ваше приложение будет доступно в браузере по ссылке `++https://space-sheep-02453.herokuapp.com/++`.

Вы также можете открыть приложение с помощью кнопки *Open app*, расположенной на панели Heroku.
--

Мониторинг логов::
+
--
Чтобы отслеживать данные в логах, запускайте в командной строке следующую команду:

[source]
----
heroku logs --tail --app space-sheep-02453
----

Логи Tomcat также доступны в веб-приложении: *Menu > Administration > Server Log*
--



[[scaling]]
=== Масштабирование приложения

В данном разделе рассмотрены способы масштабирования CUBA-приложения, состоящего из блоков Middleware и Web Client, при возрастании нагрузки и ужесточении требований к отказоустойчивости.

[cols="2", frame="all", width="100%"]
|===

a| *Этап 1. Оба блока развернуты на одном сервере приложения.*

Это простейший вариант, реализуемый стандартной процедурой <<fast_deployment,быстрого развертывания>>.

В данном случае обеспечивается максимальная производительность передачи данных между блоками *Web Client* и *Middleware*, так как при включенном свойстве приложения <<cuba.useLocalServiceInvocation,cuba.useLocalServiceInvocation>> сервисы Middleware вызываются в обход сетевого стека.
^| image:scaling_1.png[align="center"]

a| *Этап 2. Блоки Middleware и Web Client развернуты на отдельных серверах приложения.*

Данный вариант позволяет распределить нагрузку между двумя серверами приложения и более оптимально использовать ресурсы серверов. Кроме того, в этом случае нагрузка от веб-пользователей меньше сказывается на выполнении других процессов. Под другими процессами здесь понимается обслуживание средним слоем других типов клиентов (например Desktop), выполнение <<scheduled_tasks,задач по расписанию>> и, возможно, интеграционные задачи.

Требования к ресурсам серверов:

* Tomcat 1 (Web Client):
** Объем памяти - пропорционально количеству одновременно подключенных пользователей.
** Мощность CPU - зависит от интенсивности работы пользователей.
* Tomcat 2 (Middleware):
** Объем памяти - фиксированный и относительно небольшой.
** Мощность CPU - зависит от интенсивности работы пользователей и других процессов.

В этом и более сложных вариантах развертывания в блоке Web Client свойство приложения <<cuba.useLocalServiceInvocation,cuba.useLocalServiceInvocation>> должно быть установлено в false, а свойство <<cuba.connectionUrlList,cuba.connectionUrlList>> должно содержать URL блока Middleware.
^| image:scaling_2.png[align="center"]

| *Этап 3. Кластер серверов Web Client работает с одним сервером Middleware.*

Данный вариант применяется, когда вследствие большого количества одновременно подключенных пользователей требования к памяти для блока Web Client превышают возможности одной JVM. В этом случае запускается кластер (два или более) серверов Web Client, и подключение пользователей производится через Load Balancer. Все серверы Web Client работают с одним сервером Middleware.

Дублирование серверов Web Client автоматически обеспечивает отказоустойчивость на этом уровне. Однако, так как репликация HTTP-сессий не поддерживается, при незапланированном отключении одного из серверов Web Client все пользователи, подключенные к нему, вынуждены будут выполнить новый логин в приложение.

Настройка данного варианта развертывания описана в <<cluster_webclient,Настройка кластера Web Client>>.
^| image:scaling_3.png[align="center"]

| *Этап 4. Кластер серверов Web Client работает с кластером серверов Middleware.*

Это максимальный вариант развертывания, обеспечивающий отказоустойчивость и балансировку нагрузки для Middleware и Web Client.

Подключение пользователей к серверам Web Client производится через Load Balancer. Серверы WebClient работают с кластером серверов Middleware. Для этого им не требуется дополнительный Load Balancer - достаточно определить список URL серверов Middleware в свойстве <<cuba.connectionUrlList,cuba.connectionUrlList>>.

В кластере серверов Middleware организуется взаимодействие для обмена информацией о пользовательских сессиях, блокировках и пр. При этом обеспечивается полная отказоустойчивость блока Middleware - при отключении одного из серверов выполнение запросов от клиентских блоков продолжается на доступном сервере прозрачно для пользователей.

Настройка данного варианта развертывания описана в <<cluster_mw,Настройка кластера Middleware>>.
^| image:scaling_4.png[align="center"]

|===

[[cluster_webclient]]
==== Настройка кластера Web Client

В данном разделе рассматривается следующая конфигурация развертывания:

image::cluster_webclient.png[align="center"]

Здесь на серверах `host1` и `host2` блок установлены инстансы Tomcat с веб-приложением `app`, реализующим блок Web Client. Пользователи обращаются к балансировщику нагрузки по адресу `http://host0/app`, который перенаправляет запрос этим серверам. На сервере `host3` установлен Tomcat с веб-приложением `app-core`, реализующим блок Middleware.

[[cluster_webclient_lb]]
===== Установка и настройка Load Balancer

Рассмотрим процесс установки балансировщика нагрузки на базе *Apache HTTP Server* для операционной системы *Ubuntu 14.04*.

. Выполните установку *Apache HTTP Server* и его модуля *mod_jk*:
+
`$ sudo apt-get install apache2 libapache2-mod-jk`

. Замените содержимое файла `/etc/libapache2-mod-jk/workers.properties` на следующее:
+
[source, properties]
----
workers.tomcat_home=
workers.java_home=
ps=/

worker.list=tomcat1,tomcat2,loadbalancer,jkstatus

worker.tomcat1.port=8009
worker.tomcat1.host=host1
worker.tomcat1.type=ajp13
worker.tomcat1.connection_pool_timeout=600
worker.tomcat1.lbfactor=1

worker.tomcat2.port=8009
worker.tomcat2.host=host2
worker.tomcat2.type=ajp13
worker.tomcat2.connection_pool_timeout=600
worker.tomcat2.lbfactor=1

worker.loadbalancer.type=lb
worker.loadbalancer.balance_workers=tomcat1,tomcat2

worker.jkstatus.type=status
----

. Добавьте в файл `/etc/apache2/sites-available/000-default.conf` следующее:
+
[source, xml]
----
<VirtualHost *:80>
...
    <Location /jkmanager>
        JkMount jkstatus
        Order deny,allow
        Allow from all
    </Location>

    JkMount /jkmanager/* jkstatus
    JkMount /app loadbalancer
    JkMount /app/* loadbalancer

</VirtualHost>
----

. Перезапустите сервис Apache HTTP:
+
`$ sudo service apache2 restart`


[[cluster_webclient_tomcat]]
===== Настройка серверов Web Client

На серверах Tomcat 1 и Tomcat 2 необходимо произвести следующие настройки:

. В файлах `tomcat/conf/server.xml` добавить параметр `jvmRoute`, эквивалентный имени worker, заданному в настройках балансировщика нагрузки - `tomcat1` и `tomcat2`:
+
[source, xml]
----
<Server port="8005" shutdown="SHUTDOWN">
  ...
  <Service name="Catalina">
    ...
    <Engine name="Catalina" defaultHost="localhost" jvmRoute="tomcat1">
      ...
    </Engine>
  </Service>
</Server>
----

. Задать следующие свойства приложения в файлах `tomcat/conf/app/local.app.properties`:
+
[source, properties]
----
cuba.useLocalServiceInvocation = false
cuba.connectionUrlList = http://host3:8080/app-core

cuba.webHostName = host1
cuba.webPort = 8080
cuba.webContextName = app
----
+
Параметры <<cuba.webHostName,cuba.webHostName>>, <<cuba.webPort,cuba.webPort>>, <<cuba.webContextName,cuba.webContextName>> не обязательны для работы кластера WebClient, но позволяют проще идентифицировать сервера в других механизмах платформы, например в <<jmx_console,консоли JMX>>. Кроме того, в экране *User Sessions* в атрибуте *Client Info* отображается сформированный из этих параметров идентификатор блока Web Client, на котором работает данный пользователь.

[[cluster_mw]]
==== Настройка кластера Middleware

В данном разделе рассматривается следующая конфигурация развертывания:

image::cluster_mw.png[align="center"]

Здесь на серверах `host1` и `host2` блок установлены инстансы Tomcat с веб-приложением `app`, реализующим блок Web Client. Настройка кластера этих серверов рассмотрена в <<cluster_webclient,предыдущем разделе>>. На серверах `host3` и `host4` установлены инстансы Tomcat с веб-приложением `app-core`, реализующим блок Middleware. Между ними настроено взаимодействие для обмена информацией о пользовательских сессиях и блокировках, сброса кэшей и др.

[[cluster_mw_client]]
===== Настройка обращения к кластеру Middleware

Для того, чтобы клиентские блоки могли работать с несколькими серверами Middleware, достаточно указать список URL этих серверов в свойстве приложения <<cuba.connectionUrlList,cuba.connectionUrlList>>. Для Web Client это можно сделать в файле `tomcat/conf/app/local.app.properties`:

[source, properties]
----
cuba.useLocalServiceInvocation = false
cuba.connectionUrlList = http://host3:8080/app-core,http://host4:8080/app-core

cuba.webHostName = host1
cuba.webPort = 8080
cuba.webContextName = app
----

Порядок серверов в списке `cuba.connectionUrlList` определяет приоритет, в котором клиент будет пытаться направлять запросы. Например в данном случае клиент сначала попытается вызвать `host3`, если он недоступен - то `host4`. Если запрос к `host4` завершился успешно, данный клиент ставит `host4` первым в своем списке и продолжает работать с ним. После перезапуска клиента список восстанавливается в первоначальное значение. Для обеспечения равномерного распределения клиентов между серверами используется свойство <<cuba.randomServerPriority,cuba.randomServerPriority>>.

[[cluster_mw_server]]
===== Настройка взаимодействия серверов Middleware

Сервера Middleware могут поддерживать общие списки <<userSession,пользовательских сессий>> и других объектов, а также координировать сброс кэшей. Для этого достаточно на каждом их них включить свойство приложения <<cuba.cluster.enabled,cuba.cluster.enabled>>. Пример файла `tomcat/conf/app-core/local.app.properties`:

[source, properties]
----
cuba.cluster.enabled = true

cuba.webHostName = host3
cuba.webPort = 8080
cuba.webContextName = app-core
----

Для серверов Middleware обязательно нужно указать правильные значения свойств <<cuba.webHostName,cuba.webHostName>>, <<cuba.webPort,cuba.webPort>> и <<cuba.webContextName,cuba.webContextName>> для формирования уникального <<serverId,Server Id>>.

Механизм взаимодействия основан на библиотеке link:$$http://www.jgroups.org$$[JGroups]. Для тонкой настройки взаимодействия служит файл `jgroups.xml`, расположенный в корне архива `cuba-core-<version>.jar`. Его можно скопировать в каталог `tomcat/conf/app-core` и настроить нужным образом.

Программный интерфейс для взаимодействия в кластере Middleware обеспечивает бин `ClusterManagerAPI`. Его можно использовать в приложении - см. JavaDocs и примеры использования в коде платформы.

[[serverId]]
==== Server Id

_Server Id_ служит для надежной идентификации серверов в кластере *Middleware*. Идентификатор имеет вид `host:port/context`, например:

[source, plain]
----
tezis.haulmont.com:80/app-core
----

[source, plain]
----
192.168.44.55:8080/app-core
----

Идентификатор формируется на основе параметров конфигурации <<cuba.webHostName,cuba.webHostName>>, <<cuba.webPort,cuba.webPort>>, <<cuba.webContextName,cuba.webContextName>>, поэтому крайне важно корректно указать эти параметры для блока *Middleware*, работающего в кластере.

Server Id может быть получен c помощью бина `ServerInfoAPI` или через JMX-интерфейс `<<serverInfoMBean,ServerInfoMBean>>`.

[[jmx_tools]]
=== Использование инструментов JMX

В данном разделе рассмотрены различные аспекты использования инструментов *Java Management Extensions* в CUBA-приложениях.

[[jmx_console]]
==== Встроенная JMX консоль

Модуль *Web Client* базового проекта *cuba* платформы содержит средство просмотра и редактирования JMX объектов. Точкой входа в этот инструмент является экран `com/haulmont/cuba/web/app/ui/jmxcontrol/browse/display-mbeans.xml`, зарегистрированный под идентификатором `jmxConsole` и в стандартном меню доступный через пункт *Администрирование* → *Консоль JMX*.

Без дополнительной настройки консоль отображает все JMX объекты, зарегистрированные в JVM, на которой работает блок *Web Client*, к которому в данный момент подключен пользователь. Соответственно, в простейшем случае развертывания всех блоков приложения в одном экземпляре веб-контейнера консоль имеет доступ к JMX бинам всех уровней, а также к JMX объектам самой JVM и веб-контейнера. 

Имена бинов приложения имеют префикс, соответствующий имени веб-приложения, их содержащего. Например, бин `app-core.cuba:type=CachingFacade` загружен веб-приложением *app-core*, реализующим блок *Middleware*, а бин `app.cuba:type=CachingFacade` загружен веб-приложением *app*, реализующим блок *Web Client*.

Консоль JMX может также работать с JMX объектами произвольной удаленной JVM. Это актуально в случае развертывания блоков приложения на нескольких экземплярах веб-контейнера, например, отдельно *Web Client* и *Middleware*. 

Для подключения к удаленной JVM необходимо в поле *Соединение JMX* консоли выбрать созданное ранее соединение, либо вызвать экран создания нового соединения:

.Редактирование JMX соединения
image::jmx-connection-edit.png[align="center"]

Для соединения указывается JMX хост и порт, логин и пароль. Имеется также поле *Имя узла*, которое заполняется автоматически, если по указанному адресу обнаружен какой-либо блок CUBA-приложения. В этом случае значением этого поля становится комбинация свойств `<<cuba.webHostName,cuba.webHostName>>` и `<<cuba.webPort,cuba.webPort>>` данного блока, что позволяет идентифицировать содержащий его сервер. Если подключение произведено к постороннему JMX интерфейсу, то поле *Имя узла* будет иметь значение "Unknown JMX interface". Значение данного поля можно произвольно изменять.

Для подключения удаленной JVM она должна быть соответствующим образом настроена - см. ниже.

[[jmx_remote_access]]
==== Настройка удаленного доступа к JMX

В данном разделе рассматривается настройка запуска сервера *Tomcat*, необходимая для удаленного подключения к нему инструментов JMX.

[[jmx_remote_access_tomcat_windows]]
===== Tomcat JMX под Windows

* Отредактировать файл `bin/setenv.bat` следующим образом:
+
[source, plain]
----
set CATALINA_OPTS=%CATALINA_OPTS% ^
-Dcom.sun.management.jmxremote ^
-Djava.rmi.server.hostname=192.168.10.10 ^
-Dcom.sun.management.jmxremote.ssl=false ^
-Dcom.sun.management.jmxremote.port=7777 ^
-Dcom.sun.management.jmxremote.authenticate=true ^
-Dcom.sun.management.jmxremote.password.file=../conf/jmxremote.password ^
-Dcom.sun.management.jmxremote.access.file=../conf/jmxremote.access
----
+
Здесь в параметре `java.rmi.server.hostname` необходимо указать реальный IP адрес или DNS имя компьютера, на котором запущен сервер, в параметре `com.sun.management.jmxremote.port` - порт для подключения инструментов JMX.

* Отредактировать файл `conf/jmxremote.access`. Он должен содержать имена пользователей, которые будут подключаться к JMX, и их уровень доступа. Например:
+
[source, plain]
----
admin readwrite
----

* Отредактировать файл `conf/jmxremote.password`. Он должен содержать пароли пользователей JMX, например:
+
[source, plain]
----
admin admin
----

* Файл паролей должен иметь разрешение на чтение только для пользователя, от имени которого работает сервер *Tomcat*. Настроить права можно следующим образом:

** Открыть командную строку и перейти в каталог `conf`.

** Выполнить команду:
+
`++cacls jmxremote.password /P "domain_name\user_name":R++`
+
где `++domain_name\user_name++` - домен и имя пользователя.

** После выполнения данной команды файл в *Проводнике* будет отмечен изображением замка.

* Если *Tomcat* установлен как служба Windows, то для службы должен быть задан вход в систему с учетной записью, имеющей права на файл `jmxremote.password`. Кроме того, следует иметь в виду, что в этом случае файл `bin/setenv.bat` не используется, и соответствующие параметры запуска JVM должны быть заданы в приложении, настраивающем службу.

[[jmx_remote_access_tomcat_linux]]
===== Tomcat JMX под Linux

* Отредактировать файл `bin/setenv.sh` следующим образом:
+
[source, bash]
----
CATALINA_OPTS="$CATALINA_OPTS -Dcom.sun.management.jmxremote \
-Djava.rmi.server.hostname=192.168.10.10 \
-Dcom.sun.management.jmxremote.port=7777 \
-Dcom.sun.management.jmxremote.ssl=false \
-Dcom.sun.management.jmxremote.authenticate=true"

CATALINA_OPTS="$CATALINA_OPTS -Dcom.sun.management.jmxremote.password.file=../conf/jmxremote.password -Dcom.sun.management.jmxremote.access.file=../conf/jmxremote.access"
----
+
Здесь в параметре `java.rmi.server.hostname` необходимо указать реальный IP адрес или DNS имя компьютера, на котором запущен сервер, в параметре `com.sun.management.jmxremote.port` - порт для подключения инструментов JMX.

* Отредактировать файл `conf/jmxremote.access`. Он должен содержать имена пользователей, которые будут подключаться к JMX, и их уровень доступа. Например:
+
[source, plain]
----
admin readwrite
----

* Отредактировать файл `conf/jmxremote.password`. Он должен содержать пароли пользователей JMX, например:
+
[source, plain]
----
admin admin
----

* Файл паролей должен иметь разрешение на чтение только для пользователя, от имени которого работает сервер *Tomcat*. Настроить права для текущего пользователя можно следующим образом:

** Открыть командную строку и перейти в каталог `conf`.

** Выполнить команду:
+
`chmod go-rwx jmxremote.password`

[[server_push_settings]]
=== Настройка server push

Приложения CUBA используют технологию server push в механизме <<background_tasks,фоновых задач>>. Это может потребовать дополнительной настройки сервера приложения и прокси-сервера (если таковой используется).

По умолчанию server push использует протокол WebSocket. Следующие свойства приложения влияют на функциональность server push платформы:

<<cuba.web.pushLongPolling,cuba.web.pushLongPolling>>

<<cuba.web.pushEnabled,cuba.web.pushEnabled>>

Если пользователи подключаются к серверу приложения через прокси, не поддерживающий WebSocket, рекомендуется установить свойство `cuba.web.pushLongPolling` в `true` и увеличить таймаут запроса на прокси до 10 минут или больше.

Ниже приведен пример конфигурации веб-сервера *Nginx* для использования WebSocket:

[source, plain]
----
location / {
    proxy_set_header X-Forwarded-Host $host;
    proxy_set_header X-Forwarded-Server $host;
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_read_timeout     3600;
    proxy_connect_timeout  240;
    proxy_set_header Host $host;
    proxy_set_header X-RealIP $remote_addr;

    proxy_pass http://127.0.0.1:8080/;
    proxy_set_header X-Forwarded-Proto $scheme;

    proxy_set_header Upgrade $http_upgrade;
    proxy_set_header Connection "upgrade";
}
----
