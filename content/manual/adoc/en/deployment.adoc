[[chapter_deployment]]
== Application Deployment

This chapter describes different aspects of CUBA applications deployment and operation.

image::DeploymentStructure.png[align="center"]

In the example above, the application prevents a single point of failure existence, provides load balancing and various client types connection. In the simplest case, however, the server part of an application can be installed on one computer that includes the database. Various deployment options depending on load and fault tolerance requirements are described in detail in <<scaling,>>. 

[[app_dirs]]
=== Application Directories

This section describes file system directories used by various <<app_tiers,application blocks>> at runtime.

[[conf_dir]]
==== Configuration Directory

The configuration directory is intended for resources that complement and override application properties, user interface and business logic after the application is deployed. Overriding is provided by the loading mechanism of the <<resources,Resources>> infrastructure interface. Firstly it performs search in the configuration directory and then in CLASSPATH, so that resources from the configuration directory take precedence over identically named resources located in JAR files and class directories.

The configuration directory may contain the following resource types:

*  <<app_properties_files,local.app.properties>> – a file that defines deployment parameters of the server-based application blocks.

*  <<metadata.xml,metadata.xml>>, <<persistence.xml,persistence.xml>>, <<views.xml,views.xml>>, <<remoting-spring.xml,remoting-spring.xml>> configuration files.

* <<screen_xml,XML-descriptors>> of UI screens.

* <<screen_controller,Controllers>> of UI screens in the form of Java or Groovy source code.

* Groovy scripts or classes, and Java source code that is used by the application via the <<scripting,Scripting>> interface.

The location of the configuration directory is determined by the <<cuba.confDir,cuba.confDir>> application property. For the Middleware, Web Client and Web Portal blocks in <<fast_deployment,fast deployment>> configuration in Tomcat, it is a subdirectory with the web application name in the `tomcat/conf` directory, for example, `tomcat/conf/app-core` for the Middleware.

[[work_dir]]
==== Work Directory

The application uses the work directory to store data and configuration files.

For example, the `filestorage` subdirectory of the work directory is used by the <<file_storage,file storage>>. In addition, the Middleware block saves generated <<persistence.xml,persistence.xml>> and orm.xml files in the work directory on start.

Work directory location is determined by the <<cuba.dataDir,cuba.dataDir>> application property. For the Middleware, Web Client and Web Portal blocks in <<fast_deployment,fast deployment>> configuration in Tomcat, it is a subdirectory with the name of the web application in the `tomcat/work` directory.

[[log_dir]]
==== Log Directory

The content and settings of the log files are determined by the configuration of the *Logback* framework. The configuration file location is determined by <<logback.configurationFile,logback.configurationFile>> system property.

This directory can also be used to store arbitrary information about the running application. The log directory location is determined by <<cuba.logDir,cuba.logDir>> application property. For the Middleware, Web Client and Web Portal blocks in <<fast_deployment,fast deployment>> configuration in Tomcat, it is the `tomcat/logs` directory.

See also <<logging,>>.

[[temp_dir]]
==== Temporary Directory

This directory can be used for creating arbitrary temporary files at application run time. The path to the temporary directory is determined by the <<cuba.tempDir,cuba.tempDir>> application property. For the Middleware, Web Client and Web Portal blocks in <<fast_deployment,fast deployment>> configuration in Tomcat, it is a subdirectory with the web application name in the `tomcat/temp` directory.

[[db_dir]]
==== Database Scripts Directory

This directory of the deployed Middleware block stores the set of SQL scripts to create and update the DB.

The script directory structure reproduces the one described in <<db_scripts,>>, but it also has an additional top level that separates <<base_projects,base projects>> and the application scripts. The numbering of top level directories is performed by project build <<build.gradle,tasks>>.

The DB scripts directory location is determined by <<cuba.dbDir,cuba.dbDir>> application property. For <<fast_deployment,fast deployment>> configuration in *Tomcat*, it is the `WEB-INF/db` subdirectory of the middleware web application directory: `tomcat/webapps/app-core/WEB-INF/db`.

[[deployment_variants]]
=== Deployment Options

This section describes different ways to deploy CUBA applications.

[[fast_deployment]]
==== Fast Deployment in Tomcat

Fast deployment is used by default when developing an application, as it provides minimum time for building, installation and starting the application. This option is also convenient when using the application in production.

Fast deployment is performed using the <<build.gradle_deploy,deploy>> task that is declared for *core* and *web* modules in the `build.gradle` file. Before the first execution of `deploy`, a local Tomcat server should be set up and initialized using the <<build.gradle_setupTomcat,setupTomcat>> task.

As result of fast deployment, the following structure is created in the directory that is specified by the `cuba.tomcat.dir` property of the `build.gradle` script (only important directories and files are listed below):

[source, plain]
----
bin/
    setenv.bat, setenv.sh
    startup.bat, startup.sh
    debug.bat, debug.sh
    shutdown.bat, shutdown.sh

conf/
    catalina.properties
    server.xml
    logback.xml
    logging.properties
    Catalina/
        localhost/
    app/
    app-core/

lib/
    hsqldb-2.2.9.jar

logs/
    app.log

shared/
    lib/

temp/
    app/
    app-core/

webapps/
    app/
    app-core/

work/
    app/
    app-core/
----

* `bin` – the directory that contains tools to start and stop the Tomcat server:

** `setenv.bat`, `setenv.sh` – the scripts that set environment variables. These scripts should be used for setting JVM memory parameters, specifying a configuration file for <<logging_setup_tomcat,logging>>, configuring <<jmx_remote_access,access to JMX>>, parameters to <<debug_setup,connect the debugger>>.

** `startup.bat`, `startup.sh` – the scripts that start Tomcat. The server starts in a separate console window on *Windows* and in background on **nix*.
+
To start the server in the current console window, use the following commands instead of `++startup.*++`:
+
`> catalina.bat run`
+
`$ ./catalina.sh run`

** `debug.bat`, `debug.sh` – the scripts that are similar to `++startup.*++`, but start Tomcat with an ability to connect the debugger. These scripts are launched when running the <<build.gradle_start,start>> task of the build script.

** `shutdown.bat`, `shutdown.sh` – the scripts that stop Tomcat.

* `conf` – the directory that contains configuration files of Tomcat and its deployed applications.

** `catalina.properties` – the Tomcat properties. To load shared libraries from the `shared/lib` directory (see below), this file should contain the following line:
+
[source, properties]
----
shared.loader=${catalina.home}/shared/lib/*.jar
----

** `server.xml` – Tomcat configuration descriptor.

** `logback.xml` – application <<logging_setup_tomcat,logging>> configuration descriptor.

** `logging.properties` – Tomcat server logging configuration descriptor.

** `Catalina/localhost` – in this directory, <<context.xml,context.xml>> application deployment descriptors can be placed. Descriptors located in this directory take precedence over the descriptors in the `META-INF` directories of the application. This approach is often convenient for production environment. For example, with this descriptor, it is possible to specify the database connection parameters that are different from those specified in the application itself.
+
Server-specific deployment descriptor should have the application name and the `.xml` extension. So, to create this descriptor, for example, for the `app-core` application, copy the contents of the `webapps/app-core/META-INF/context.xml` file to the `conf/Catalina/localhost/app-core.xml` file.

** `app` – web client application <<conf_dir,configuration directory>>.

** `app-core` – middleware application <<conf_dir,configuration directory>>.

* `lib` – directory of the libraries that are loaded by the server's _common classloader_. These libraries are available for both the server and all web applications deployed in it. In particular, this directory should have JDBC drivers of the utilized databases (`hsqldb-XYZ.jar`, `postgresql-XYZ.jar`, etc.)

* `logs` – application and server <<logging,logs>> directory. The main log file of the application is `app.log` (see <<logging_setup_tomcat>>).

* `shared/lib` – directory of libraries that are available to all deployed applications. These libraries classes are loaded by the server's special _shared classloader_. Its usage is configured in the `conf/catalina.properties` file as described above.
+
The <<build.gradle_deploy,deploy>> task of the build script copies all libraries not listed in the `jarNames` parameter, i.e. not specific for the given application, into this directory.

* `temp/app`, `temp/app-core` – web client and the middleware applications <<temp_dir,temporary directories>>.

* `webapps` – web application directories. Each application is located in its own subdirectory in the _exploded WAR_ format.
+
The <<build.gradle_deploy,deploy>> task of the build script create application subdirectories with the names specified in the `appName` parameters and, among other things, copy the libraries listed in the `jarNames` parameter to the `WEB-INF/lib` subdirectory for each application.

* `work/app`, `work/app-core` – web client and the middleware applications <<work_dir,work directories>>.

[[tomcat_in_prod]]
===== Using Tomcat in Production

By default, the <<fast_deployment,fast deployment>> procedure creates the `app` and `app-core` web applications running on port 8080 of the local Tomcat instance. It means that the web client is available at `++http://localhost:8080/app++`.

You can use this Tomcat instance in production just by copying the `tomcat` directory to the server. All you have to do is to set up the server host name in both `conf/app/local.app.properties` and `conf/app-core/local.app.properties` files (create the files if they do not exist):

[source, properties]
----
cuba.webHostName = myserver
cuba.webAppUrl = http://myserver:8080/app
---- 

Besides, set up the connection to you production database. You can do it in the <<context.xml>> file of your web application (`webapps/app-core/META-INF/context.xml`), or copy this file to `conf/Catalina/localhost/app-core.xml` as described in the previous section to separate development and production settings.

You can create the production database from a development database backup, or set up the automatic creation and further updating of the database. See <<db_update_in_prod>>.

If you want to change the Tomcat port or web context (the last part of the URL after `/`), use *Studio*:

* Open the project in Studio.

* Go to *Project Properties* > *Edit* > *Advanced*.

* To change the web context, edit the *Modules prefix* field.

* To change the Tomcat port, edit the *Tomcat ports* > *HTTP port* field.

If you want to use the root context for the web client (`++http://myserver:8080++`), rename `app` directories to `ROOT`

[source, plain]
----
tomcat/
    conf/
        ROOT/
            local.app.properties
        app-core/
            local.app.properties
    webapps/
        ROOT/
        app-core/
----

and use `/` as the web context name in `conf/ROOT/local.app.properties`:

[source, properties]
----
cuba.webContextName = /
---- 

[[war_deployment]]
==== Deployment in WAR

JavaEE standard application deployment into WAR files is performed using the <<build.gradle_buildWar,buildWar>> build task. An example of building WAR files and their deployment on the *Glassfish 4* server is provided below.

. Add the <<build.gradle_buildWar, buildWar>> task to the end of <<build.gradle,build.gradle>>:
+
[source, groovy]
----
task buildWar(type: CubaWarBuilding) {
    coreProject = project(':app-core')
    webProject = project(':app-web')
    appName = 'app'
    appHome = '${app.home}'
    singleWar = false
}
----

. Start build process:
+
`gradlew buildWar`
+
As a result, the `app-core.war` and `app.war` files are created in the `build/distributions/war` project subdirectory.

. Create an application home directory on the server, for example, `/home/user/app_home`.

. Install the *Glassfish 4* server, for example, into the `/home/user/glassfish4` directory.

. Copy the JDBC driver of the database to the `/home/user/glassfish4/glassfish/domains/domain1/lib` directory. You can take the driver file from the `lib` directory in Studio, or from the `build/tomcat/lib` project directory (if <<fast_deployment,fast deployment>> in Tomcat has been performed before).

. Create a <<logging, Logback>> configuration file on the basis of `build/tomcat/conf/logback.xml` replacing the ${catalina.home} variable to ${app.home}. Place the file into the application home directory.

. Start the server:
+
`$ cd /home/user/glassfish4/bin`
+
`$ ./asadmin start-domain`

. Go to `++http://localhost:4848++` and do the following steps in the server management console:

.. Create a *JDBC Connection Pool* to connect to our database, for example:

* Pool Name: AppDB 

* Resource Type: javax.sql.DataSource

* Database Driver Vendor: Postgresql

* Datasource Classname: org.postgresql.ds.PGSimpleDataSource 

* User: cuba

* DatabaseName: app_db

* Password: cuba

.. Create a *JDBC Resource*:

* JNDI Name: jdbc/CubaDS

* Pool Name: AppDB

.. In the *server (Admin Server)* -> *Properties* -> *System Properties* screen, set the following Java system variables:

* `++app.home = /home/user/app_home++` – application home directory.

* `++logback.configurationFile = file:///home/user/app_home/logback.xml++` – the Logback configuration file created in the step above.

. Restart the server:
+
`$ ./asadmin stop-domain`
+
`$ ./asadmin start-domain`

. Open the server console at `++http://localhost:4848++` and, in the *Applications* screen, perform deployment of the app-core.war and app.war files located in the distribution folder created in Step 3.

. The application has now been started:

* Web interface is available at `++http://localhost:8080/app++`

* Log files are created in the `/home/user/app_home/logs`

[[scaling]]
=== Application Scaling

This section describes ways to scale a CUBA application that consists of the *Middleware* and the *Web Client* for increased load and stronger fault tolerance requirements.

[cols="2", frame="all", width="70%"]
|===

a| *Stage 1. Both blocks are deployed on the same application server.*

This is the simplest option implemented by the standard <<fast_deployment,fast deployment>> procedure.

In this case, maximum data transfer performance between the *Web Client* and the *Middleware* is provided, because when the <<cuba.useLocalServiceInvocation,cuba.useLocalServiceInvocation>> application property is enabled, the Middleware services are invoked bypassing the network stack.
| image:scaling_1.png[align="center"]

a| *Stage 2. The Middleware and the Web Client blocks are deployed on separate application servers.*

This option allows you to distribute load between two application servers and use server resources better. Furthermore, in this case the load coming from web users has smaller effect on the other processes execution. Here, the other processes mean handling other client types (for example, Desktop), running <<scheduled_tasks,scheduled tasks>> and, potentially, integration tasks which are performed by the middle layer.

Requirements for server resources:

* Tomcat 1 (Web Client):
** Memory size – proportional to the number of simultaneous users
** CPU power – depends on the usage intensity
* Tomcat 2 (Middleware):
** Memory size – fixed and relatively small
** CPU power – depends on the intensity of web client usage and of other processes

In this case and when more complex deployment options are used, the Web Client's <<cuba.useLocalServiceInvocation,cuba.useLocalServiceInvocation>> application property should be set to `false`, and <<cuba.connectionUrlList,cuba.connectionUrlList>> property should contain the URL of the Middleware block.
| image:scaling_2.png[align="center"]

| *Stage 3. A cluster of Web Client servers works with one Middleware server.*

This option is used when memory requirements for the Web Client exceed the capabilities of a single JVM due to a large number of concurrent users. In this case, a cluster of Web Client servers (two or more) is started and user connection is performed through a Load Balancer. All Web Client servers work with one Middleware server.

Duplication of Web Client servers automatically provides fault tolerance at this level. However, the replication of HTTP sessions is not supported, in case of unscheduled outage of one of the Web Client servers, all users connected to it will have to login into the application again.

Configuration of this option is described in <<cluster_webclient,>>.
| image:scaling_3.png[align="center"]

| *Stage 4. A cluster of Web Client servers working with a cluster of Middleware servers.*

This is the maximum deployment option, which provides fault tolerance and load balancing for the Middleware and the Web Client.

Connection of users to the Web Client servers is performed through a load balancer. The Web Client servers work with a cluster of Middleware servers. They do not need an additional load balancer – it is sufficient to determine the list of URLs for the Middleware servers in the <<cuba.connectionUrlList,cuba.connectionUrlList>> application property.

Middleware servers exchange the information about user sessions, locks, etc. In this case, full fault tolerance of the Middleware is provided – in case of an outage of one of the servers, execution of requests from client blocks will continue on an available server without affecting users.

Configuration of this option is described in <<cluster_mw,>>.
| image:scaling_4.png[align="center"]

|===

[[cluster_webclient]]
==== Setting up a Web Client Cluster

This section describes the following deployment configuration:

image::cluster_webclient.png[align="center"]

Servers `host1` and `host2` host Tomcat instances with the `app` web-app implementing the Web Client block. Users access the load balancer at `++http://host0/app++`, which redirects their requests to the servers. Server `host3` hosts a Tomcat instance with the `app-core` web-app that implements the Middleware block.

[[cluster_webclient_lb]]
===== Installing and Setting up a Load Balancer

Let us consider the installation of a load balancer based on *Apache HTTP Server* for *Ubuntu 14.04*.

. Install *Apache HTTP Server* and its *mod_jk* module:
+
`$ sudo apt-get install apache2 libapache2-mod-jk`

. Replace the contents of the `/etc/libapache2-mod-jk/workers.properties` file with the following:
+
[source, properties]
----
workers.tomcat_home=
workers.java_home=
ps=/

worker.list=tomcat1,tomcat2,loadbalancer,jkstatus

worker.tomcat1.port=8009
worker.tomcat1.host=host1
worker.tomcat1.type=ajp13
worker.tomcat1.connection_pool_timeout=600
worker.tomcat1.lbfactor=1

worker.tomcat2.port=8009
worker.tomcat2.host=host2
worker.tomcat2.type=ajp13
worker.tomcat2.connection_pool_timeout=600
worker.tomcat2.lbfactor=1

worker.loadbalancer.type=lb
worker.loadbalancer.balance_workers=tomcat1,tomcat2

worker.jkstatus.type=status
----

. Add the lines listed below to `/etc/apache2/sites-available/000-default.conf`:
+
[source, xml]
----
<VirtualHost *:80>
...
    <Location /jkmanager>
        JkMount jkstatus
        Order deny,allow
        Allow from all
    </Location>

    JkMount /jkmanager/* jkstatus
    JkMount /app loadbalancer
    JkMount /app/* loadbalancer

</VirtualHost>
---- 

. Restart the Apache HTTP service:
+
`$ sudo service apache2 restart`

[[cluster_webclient_tomcat]]
===== Setting up Web Client Servers

On the Tomcat 1 and Tomcat 2 servers, the following settings should be applied:

. In `tomcat/conf/server.xml`, add the `jvmRoute` parameter equivalent to the name of the worker specified in the load balancer settings for `tomcat1` and `tomcat2`:
+
[source, xml]
----
<Server port="8005" shutdown="SHUTDOWN">
  ...
  <Service name="Catalina">
    ...
    <Engine name="Catalina" defaultHost="localhost" jvmRoute="tomcat1">
      ...
    </Engine>
  </Service>
</Server>
----

. Set the following application properties in `tomcat/conf/app/local.app.properties`:
+
[source, properties]
----
cuba.useLocalServiceInvocation = false
cuba.connectionUrlList = http://host3:8080/app-core

cuba.webHostName = host1
cuba.webPort = 8080
cuba.webContextName = app
----
+
<<cuba.webHostName,cuba.webHostName>>, <<cuba.webPort,cuba.webPort>> and <<cuba.webContextName,cuba.webContextName>> parameters are not mandatory for WebClient cluster, but they allow easier identification of a server in other platform mechanisms, such as the <<jmx_console, JMX console>>. Additionally, *Client Info* attribute of the *User Sessions* screen shows an identifier of the Web Client that the current user is working with.

[[cluster_mw]]
==== Setting up a Middleware Cluster

This section describes the following deployment configuration:

image::cluster_mw.png[align="center"]

Servers `host1` and `host2` host Tomcat instances with the `app` web-app implementing the Web Client block. Cluster configuration for these servers is described in the <<cluster_webclient,previous section>>. Servers `host3` and `host4` host Tomcat instances with the `app-core` web-app implementing the Middleware block. They are configured to interact and share information about user sessions, locks, cash flushes, etc.

[[cluster_mw_client]]
===== Setting up Connection to the Middleware Cluster

In order for the client blocks to be able to work with multiple Middleware servers, the list of URLs should be specified to these servers in the <<cuba.connectionUrlList,cuba.connectionUrl>> application property. For the Web Client, this can be done in `tomcat/conf/app/local.app.properties`:

[source, properties]
----
cuba.useLocalServiceInvocation = false
cuba.connectionUrlList = http://host3:8080/app-core,http://host4:8080/app-core

cuba.webHostName = host1
cuba.webPort = 8080
cuba.webContextName = app
----

The order of servers in `cuba.connectionUrl` defines priority and order for the client to send the requests. In the example above, the client will first attempt to access `host1`, and then, if it is not available, `host2`. If a request to `host2` completes successfully, the client will save `host2` as the first server in the list and will continue working with this server. Restarting a client will reset the initial values. Uniform distribution of clients among all servers can be achieved using the <<cuba.randomServerPriority,cuba.randomServerPriority>> property.

[[cluster_mw_server]]
===== Configuring Interaction between Middleware Servers

Middleware servers can maintain shared lists of <<userSession,user sessions>> and other objects and coordinate invalidation of caches. <<cuba.cluster.enabled,cuba.cluster.enabled>> property should be enabled on each server to achieve this. Example of the `tomcat/conf/app-core/local.app.properties` file is shown below:

[source, properties]
----
cuba.cluster.enabled = true

cuba.webHostName = host3
cuba.webPort = 8080
cuba.webContextName = app-core
----

For the Middleware servers, correct values of the <<cuba.webHostName,cuba.webHostName>>, <<cuba.webPort,cuba.webPort>> and <<cuba.webContextName,cuba.webContextName>> properties should be specified to form a unique <<serverId,Server ID>>.

Interaction mechanism is based on link:http://www.jgroups.org[JGroups]. It is possible to fine-tune the interaction using the `jgroups.xml` file located in the root of cuba-core-<version>.jar. It can be copied to `tomcat/conf/app-core` and configured as needed.

`ClusterManagerAPI` bean provides the program interface for servers interaction in the Middleware cluster. It can be used in the application – see JavaDocs and examples in the platform code.

[[serverId]]
==== Server ID

_Server ID_ is used for reliable identification of servers in a *Middleware* cluster. The identifier is formatted as `host:port/context`:

[source, plain]
----
tezis.haulmont.com:80/app-core
----

[source, plain]
----
192.168.44.55:8080/app-core
----

The identifier is formed based on the configuration parameters <<cuba.webHostName,cuba.webHostName>>, <<cuba.webPort,cuba.webPort>>, <<cuba.webContextName,cuba.webContextName>>, therefore it is very important to specify these parameters for the Middleware blocks working within the cluster. 

Server ID can be obtained using the `ServerInfoAPI` bean or via the <<serverInfoMBean,ServerInfoMBean>> JMX interface.

[[jmx_tools]]
=== Using JMX Tools

This section describes various aspects of using *Java Management Extensions* in CUBA-based applications.

[[jmx_console]]
==== Built-In JMX Console

The Web Client module of the *cuba* base project contains JMX objects viewing and editing tool. The entry point for this tool is `com/haulmont/cuba/web/app/ui/jmxcontrol/browse/display-mbeans.xml` screen registered under the `jmxConsole` identifier and accessible via *Administration* > *JMX Console* in the standard application menu.

Without extra configuration, the console shows all JMX objects registered in the JVM where the Web Client block of the current user is running. Therefore, in the simplest case, when all application blocks are deployed to one web container instance, the console has access to the JMX beans of all tiers as well as the JMX objects of the JVM itself and the web container. 

Names of the application beans have a prefix corresponding to the name of the web-app that contains them. For example, the `app-core.cuba:type=CachingFacade` bean has been loaded by the *app-core* web-app implementing the Middleware block, while the `app.cuba:type=CachingFacade` bean has been loaded by the *app* web-app implementing the Web Client block.

JMX console can also work with the JMX objects of a remote JVM. This is useful when application blocks are deployed over several instances of a web container, for example separate Web Client and Middleware. 

To connect to a remote JVM, a previously created connection should be selected in the *JMX Connection* field of the console, or a new connection can be created:

.Editing a JMX Connection
image::jmx-connection-edit.png[align="center"]

To get a connection, JMX host, port, login and password should be specified. There is also the *Host name* field, which is populated automatically, if any CUBA-application block is detected at the specified address. In this case, the value of this field is defined as the combination of <<cuba.webHostName,cuba.webHostName>> and <<cuba.webPort,cuba.webPort>> properties of this block, which enables identifying the server that contains it. If the connection is done to a 3rd party JMX interface, then the *Host name* field will have the "Unknown JMX interface" value. However it can be changed arbitrarily.

In order to allow a remote JVM connection, the JVM should be configured properly (see below).

[[jmx_remote_access]]
==== Setting up a Remote JMX Connection

This section describes *Tomcat* startup configuration required for a remote connection of JMX tools.

===== Tomcat JMX for Windows

* Edit `bin/setenv.bat` in the following way:
+
[source, properties]
----
set CATALINA_OPTS=%CATALINA_OPTS% ^
-Dcom.sun.management.jmxremote ^
-Djava.rmi.server.hostname=192.168.10.10 ^
-Dcom.sun.management.jmxremote.ssl=false ^
-Dcom.sun.management.jmxremote.port=7777 ^
-Dcom.sun.management.jmxremote.authenticate=true ^
-Dcom.sun.management.jmxremote.password.file=../conf/jmxremote.password ^
-Dcom.sun.management.jmxremote.access.file=../conf/jmxremote.access
----
+
Here, the `java.rmi.server.hostname` parameter should contain the actual IP address or the DNS name of the computer where the server is running; `com.sun.management.jmxremote.port` sets the port for JMX tools connection.

* Edit the `conf/jmxremote.access` file. It should contain user names that will be connecting to the JMX and their access level. For example:
+
[source, plain]
----
admin readwrite
----

* Edit the `conf/jmxremote.password` file. It should contain passwords for the JMX users, for example:
+
[source, plain]
----
admin admin
----

* The password file should have reading permissions only for the user running the *Tomcat*. server. You can configure permissions the following way:

** Open the command line and go to the conf folder

** Run the command:`++cacls jmxremote.password /P "domain_name\user_name":R++`
+
where `++domain_name\user_name++` is the user's domain and name

** After this command is executed, the file will be displayed as locked (with a lock icon) in *Explorer*.

* If *Tomcat* is installed as a Windows service, than the service should be started on behalf of the user who has access permissions for jmxremote.password. It should be kept in mind that in this case the `bin/setenv.bat` file is ignored and the corresponding JVM startup properties should be specified in the application that configures the service.

===== Tomcat JMX for Linux

* Edit `bin/setenv.sh` the following way:
+
[source, properties]
----
CATALINA_OPTS="$CATALINA_OPTS -Dcom.sun.management.jmxremote \
-Djava.rmi.server.hostname=192.168.10.10 \
-Dcom.sun.management.jmxremote.port=7777 \
-Dcom.sun.management.jmxremote.ssl=false \
-Dcom.sun.management.jmxremote.authenticate=true"

CATALINA_OPTS="$CATALINA_OPTS -Dcom.sun.management.jmxremote.password.file=../conf/jmxremote.password -Dcom.sun.management.jmxremote.access.file=../conf/jmxremote.access"
----
+
Here, the `java.rmi.server.hostname` parameter should contain the real IP address or the DNS name of the computer where the server is running; `com.sun.management.jmxremote.port` sets the port for JMX tools connection

* Edit `conf/jmxremote.access` file. It should contain user names that will be connecting to the JMX and their access level. For example:
+
[source, plain]
----
admin readwrite
----

* Edit the `conf/jmxremote.password` file. It should contain passwords for the JMX users, for example:
+
[source, plain]
----
admin admin
----

* The password file should have reading permissions only for the user running the *Tomcat* server. Permissions for the current user can be configured the following way:

** Open the command line and go to the conf folder.

** Run the command:
+
`chmod go-rwx jmxremote.password`

[[license_file]]
=== License File Usage

The platform is shipped with the `cuba.license` free license file, available in the root of the classpath. The <<cuba.licensePath,cuba.licensePath>> application property points to it by default.

If you have purchased a commercial license, you can use it in the application by one of the following ways. 

. If you are going to use the application within one organization, or you have an embedded license, include the license file into your application distribution. Add the license file into the *core* module source code folder. The file name or path should be different from `/cuba.license`:
+
[source, plain]
----
modules/core/src/
  myapp-cuba.license
  app.properties
---- 
+
Configure the `cuba.licensePath` application property in the `app.properties` file of the *core* module:
+
[source, properties]
----
cuba.licensePath = /myapp-cuba.license
---- 

. If you are going to use the application in different organizations, you have to obtain a separate file for each of them. In this case you can place the license files into <<conf_dir,configuration directories>> of the installed applications:
+
[source, plain]
----
tomcat/conf/app-core/
  myapp-cuba.license
  local.app.properties
---- 
+
Set the `cuba.licensePath` application property in the `local.app.properties` file:
+
[source, properties]
----
cuba.licensePath = /myapp-cuba.license
----