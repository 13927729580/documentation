[[deployment]]
== Application Deployment

This chapter describes different aspects of CUBA applications deployment and operation.

image::DeploymentStructure.png[align="center"]

In the example above, the application prevents a single point of failure existence, provides load balancing and various client types connection. In the simplest case, however, the server part of an application can be installed on one computer that includes the database. Various deployment options depending on load and fault tolerance requirements are described in detail in <<scaling,>>.

[[app_dirs]]
=== Application Directories

This section describes file system directories used by various <<app_tiers,application blocks>> at runtime.

[[conf_dir]]
==== Configuration Directory

The configuration directory is intended for resources that complement and override application properties, user interface and business logic after the application is deployed. Overriding is provided by the loading mechanism of the <<resources,Resources>> infrastructure interface. Firstly it performs search in the configuration directory and then in CLASSPATH, so that resources from the configuration directory take precedence over identically named resources located in JAR files and class directories.

The configuration directory may contain the following resource types:

*  <<app_properties_files,local.app.properties>> – a file that defines deployment parameters of the server-based application blocks.

*  <<metadata.xml,metadata.xml>>, <<persistence.xml,persistence.xml>>, <<views.xml,views.xml>>, <<remoting-spring.xml,remoting-spring.xml>> configuration files.

* <<screen_xml,XML-descriptors>> of UI screens.

* <<screen_controller,Controllers>> of UI screens in the form of Java or Groovy source code.

* Groovy scripts or classes, and Java source code that is used by the application via the <<scripting,Scripting>> interface.

The location of the configuration directory is determined by the <<cuba.confDir,cuba.confDir>> application property. For the Middleware, Web Client and Web Portal blocks in <<fast_deployment,fast deployment>> configuration in Tomcat, it is a subdirectory with the web application name in the `tomcat/conf` directory, for example, `tomcat/conf/app-core` for the Middleware.

[[work_dir]]
==== Work Directory

The application uses the work directory to store data and configuration files.

For example, the `filestorage` subdirectory of the work directory is used by the <<file_storage,file storage>>. In addition, the Middleware block saves generated <<persistence.xml,persistence.xml>> and orm.xml files in the work directory on start.

Work directory location is determined by the <<cuba.dataDir,cuba.dataDir>> application property. For the Middleware, Web Client and Web Portal blocks in <<fast_deployment,fast deployment>> configuration in Tomcat, it is a subdirectory with the name of the web application in the `tomcat/work` directory.

[[log_dir]]
==== Log Directory

The content and settings of the log files are determined by the configuration of the *Logback* framework. The configuration file location is determined by <<logback.configurationFile,logback.configurationFile>> system property.

This directory can also be used to store arbitrary information about the running application. The log directory location is determined by <<cuba.logDir,cuba.logDir>> application property. For the Middleware, Web Client and Web Portal blocks in <<fast_deployment,fast deployment>> configuration in Tomcat, it is the `tomcat/logs` directory.

See also <<logging,>>.

[[temp_dir]]
==== Temporary Directory

This directory can be used for creating arbitrary temporary files at application run time. The path to the temporary directory is determined by the <<cuba.tempDir,cuba.tempDir>> application property. For the Middleware, Web Client and Web Portal blocks in <<fast_deployment,fast deployment>> configuration in Tomcat, it is a subdirectory with the web application name in the `tomcat/temp` directory.

[[db_dir]]
==== Database Scripts Directory

This directory of the deployed Middleware block stores the set of SQL scripts to create and update the DB.

The script directory structure reproduces the one described in <<db_scripts,>>, but it also has an additional top level that separates <<app_components,application components>> and the application scripts. The numbering of top level directories is performed by project build <<build.gradle,tasks>>.

The DB scripts directory location is determined by <<cuba.dbDir,cuba.dbDir>> application property. For <<fast_deployment,fast deployment>> configuration in *Tomcat*, it is the `WEB-INF/db` subdirectory of the middleware web application directory: `tomcat/webapps/app-core/WEB-INF/db`.

[[deployment_variants]]
=== Deployment Options

This section describes different ways to deploy CUBA applications.

[[fast_deployment]]
==== Fast Deployment in Tomcat

Fast deployment is used by default when developing an application, as it provides minimum time for building, installation and starting the application. This option is also convenient when using the application in production.

Fast deployment is performed using the <<build.gradle_deploy,deploy>> task that is declared for *core* and *web* modules in the `build.gradle` file. Before the first execution of `deploy`, a local Tomcat server should be set up and initialized using the <<build.gradle_setupTomcat,setupTomcat>> task.

As result of fast deployment, the following structure is created in the directory that is specified by the `cuba.tomcat.dir` property of the `build.gradle` script (only important directories and files are listed below):

[source, plain]
----
bin/
    setenv.bat, setenv.sh
    startup.bat, startup.sh
    debug.bat, debug.sh
    shutdown.bat, shutdown.sh

conf/
    catalina.properties
    server.xml
    logback.xml
    logging.properties
    Catalina/
        localhost/
    app/
    app-core/

lib/
    hsqldb-2.2.9.jar

logs/
    app.log

shared/
    lib/

temp/
    app/
    app-core/

webapps/
    app/
    app-core/

work/
    app/
    app-core/
----

* `bin` – the directory that contains tools to start and stop the Tomcat server:

** `setenv.bat`, `setenv.sh` – the scripts that set environment variables. These scripts should be used for setting JVM memory parameters, specifying a configuration file for <<logging_setup_tomcat,logging>>, configuring <<jmx_remote_access,access to JMX>>, parameters to <<debug_setup,connect the debugger>>.

** `startup.bat`, `startup.sh` – the scripts that start Tomcat. The server starts in a separate console window on *Windows* and in background on **nix*.
+
To start the server in the current console window, use the following commands instead of `++startup.*++`:
+
`> catalina.bat run`
+
`$ ./catalina.sh run`

** `debug.bat`, `debug.sh` – the scripts that are similar to `++startup.*++`, but start Tomcat with an ability to connect the debugger. These scripts are launched when running the <<build.gradle_start,start>> task of the build script.

** `shutdown.bat`, `shutdown.sh` – the scripts that stop Tomcat.

* `conf` – the directory that contains configuration files of Tomcat and its deployed applications.

** `catalina.properties` – the Tomcat properties. To load shared libraries from the `shared/lib` directory (see below), this file should contain the following line:
+
[source, properties]
----
shared.loader=${catalina.home}/shared/lib/*.jar
----

** `server.xml` – Tomcat configuration descriptor.

** `logback.xml` – application <<logging_setup_tomcat,logging>> configuration descriptor.

** `logging.properties` – Tomcat server logging configuration descriptor.

** `Catalina/localhost` – in this directory, <<context.xml,context.xml>> application deployment descriptors can be placed. Descriptors located in this directory take precedence over the descriptors in the `META-INF` directories of the application. This approach is often convenient for production environment. For example, with this descriptor, it is possible to specify the database connection parameters that are different from those specified in the application itself.
+
Server-specific deployment descriptor should have the application name and the `.xml` extension. So, to create this descriptor, for example, for the `app-core` application, copy the contents of the `webapps/app-core/META-INF/context.xml` file to the `conf/Catalina/localhost/app-core.xml` file.

** `app` – web client application <<conf_dir,configuration directory>>.

** `app-core` – middleware application <<conf_dir,configuration directory>>.

* `lib` – directory of the libraries that are loaded by the server's _common classloader_. These libraries are available for both the server and all web applications deployed in it. In particular, this directory should have JDBC drivers of the utilized databases (`hsqldb-XYZ.jar`, `postgresql-XYZ.jar`, etc.)

* `logs` – application and server <<logging,logs>> directory. The main log file of the application is `app.log` (see <<logging_setup_tomcat>>).

* `shared/lib` – directory of libraries that are available to all deployed applications. These libraries classes are loaded by the server's special _shared classloader_. Its usage is configured in the `conf/catalina.properties` file as described above.
+
The <<build.gradle_deploy,deploy>> task of the build script copies all libraries not listed in the `jarNames` parameter, i.e. not specific for the given application, into this directory.

* `temp/app`, `temp/app-core` – web client and the middleware applications <<temp_dir,temporary directories>>.

* `webapps` – web application directories. Each application is located in its own subdirectory in the _exploded WAR_ format.
+
The <<build.gradle_deploy,deploy>> task of the build script create application subdirectories with the names specified in the `appName` parameters and, among other things, copy the libraries listed in the `jarNames` parameter to the `WEB-INF/lib` subdirectory for each application.

* `work/app`, `work/app-core` – web client and the middleware applications <<work_dir,work directories>>.

[[tomcat_in_prod]]
===== Using Tomcat in Production

By default, the <<fast_deployment,fast deployment>> procedure creates the `app` and `app-core` web applications running on port 8080 of the local Tomcat instance. It means that the web client is available at `++http://localhost:8080/app++`.

You can use this Tomcat instance in production just by copying the `tomcat` directory to the server. All you have to do is to set up the server host name in both `conf/app/local.app.properties` and `conf/app-core/local.app.properties` files (create the files if they do not exist):

[source, properties]
----
cuba.webHostName = myserver
cuba.webAppUrl = http://myserver:8080/app
----

Besides, set up the connection to you production database. You can do it in the <<context.xml>> file of your web application (`webapps/app-core/META-INF/context.xml`), or copy this file to `conf/Catalina/localhost/app-core.xml` as described in the previous section to separate development and production settings.

You can create the production database from a development database backup, or set up the automatic creation and further updating of the database. See <<db_update_in_prod>>.

If you want to change the Tomcat port or web context (the last part of the URL after `/`), use *Studio*:

* Open the project in Studio.

* Go to *Project Properties* > *Edit* > *Advanced*.

* To change the web context, edit the *Modules prefix* field.

* To change the Tomcat port, edit the *Tomcat ports* > *HTTP port* field.

If you want to use the root context for the web client (`++http://myserver:8080++`), rename `app` directories to `ROOT`

[source, plain]
----
tomcat/
    conf/
        ROOT/
            local.app.properties
        app-core/
            local.app.properties
    webapps/
        ROOT/
        app-core/
----

and use `/` as the web context name in `conf/ROOT/local.app.properties`:

[source, properties]
----
cuba.webContextName = /
----

[[war_deployment]]
==== WAR deployment to Jetty

You can deploy CUBA applications to WAR files using the <<build.gradle_buildWar,buildWar>> build task and run them on any Java servlet container. An example of deployment of the WAR files to the *Jetty* web server is provided below.

. Add the <<build.gradle_buildWar, buildWar>> task to the end of <<build.gradle,build.gradle>>:
+
[source, groovy]
----
include::{sourcesdir}/deployment/warDeployment_1.groovy[]
----
+
[TIP]
====
Please note that we build two separate WAR files for Middleware and Web Client blocks here. If you want to combine them into one WAR file with the `singleWar = true` parameter, provide the special `web.xml` file as described in <<build.gradle_buildWar,this section>>.
====

. Start build process:
+
[source, plain]
----
gradlew buildWar
----
+
As a result, the `app-core.war` and `app.war` files will be created in the `build\distributions\war` project subdirectory.

. Create an application home directory, for example, `c:\work\app_home`.

. Download and install Jetty to a local directory, for example `c:\work\jetty-home`. This example has been tested on `jetty-distribution-9.3.6.v20151106.zip`.

. Create the `c:\work\jetty-base` directory, open the command prompt in it and execute:
+
[source, plain]
----
java -jar c:\work\jetty-home\start.jar --add-to-start=http,jndi,deploy,plus,ext,resources
----

. Create the `c:\work\jetty-base\app-jetty.xml` file with the following contents (for a PostgreSQL database named `test`):
+
[source, xml]
----
include::{sourcesdir}/deployment/warDeployment_2.xml[]
----

. Add the following text to the beginning of `c:\work\jetty-base\start.ini` file:
+
[source, plain]
----
include::{sourcesdir}/deployment/warDeployment_3.ini[]
----

. Copy the JDBC driver for your database to the `c:\work\jetty-base\lib\ext` directory. You can take the driver file from the CUBA Studio `lib` directory or from the `build\tomcat\lib` project directory. In case of PostgreSQL database, it is `postgresql-9.1-901.jdbc4.jar`.

. Copy WAR files to the `c:\work\jetty-base\webapps` directory.

. Open the command prompt in the `c:\work\jetty-base` directory and run:
+
[source, plain]
----
java -jar c:\work\jetty-home\start.jar
----

. Open `++http://localhost:8080/app++` in your web browser.


[[tomcat_war_deployment]]
==== WAR deployment to Tomcat Windows Service

. Add the <<build.gradle_buildWar, buildWar>> task to the end of <<build.gradle,build.gradle>>:
+
--
[source, groovy]
----
include::{sourcesdir}/deployment/warDeployment_2.groovy[]
----

If the target server parameters differ from what you have on the local Tomcat used for <<fast_deployment,fast deployment>>, provide appropriate application properties. For example, if the target server runs on port 9999, the task definition should be as follows:

[source, groovy]
----
include::{sourcesdir}/deployment/warDeployment_3.groovy[]
----

You can also specify a different `context.xml` file to setup the connection to the production database, for example:

[source, groovy]
----
include::{sourcesdir}/deployment/warDeployment_4.groovy[]
----
--

. Run the `buildWar` gradle task. As a result, `app.war` and `app-core.war` files will be generated in the `build/distibutions` directory of your project.
+
[source, plain]
----
gradlew buildWar
----

. Download and run Tomcat 8 Windows Service Installer.

. Go to the `bin` directory of the installed server and run `tomcat8w.exe` with the administrative rights.
Set *Maximum memory pool* to 1024MB on the *Java* tab. Then go to the *General* tab and restart the service.
+
image::tomcatPropeties.jpg[align="center"]

. Add `-Dfile.encoding=UTF-8` to the _Java Options_ field.

. Copy the generated `app.war` and `app-core.war` files to the `webapps` directory of the server.

. Start the Tomcat service.

. Open `++http://localhost:8080/app++` in your web browser.


[[jelastic_deployment]]
==== Deployment to Jelastic Cloud

CUBA Studio allows you to deploy your application to the link:$$https://jelastic.com/$$[Jelastic] cloud in a few easy steps.

[TIP]
====
Please note that only projects using PostgreSQL or HSQL databases are currently supported.
====

. Click the *Deployment settings* link on the *Project properties* section and switch to the *CLOUD* tab.

. If the project is not yet set up for cloud deployment, you can use the field on top to create a free trial Jelastic account.

. After completing your registration, enter the email, password and selected provider.
+
image::jelastic_1.png[align="center"]

. *Environment* field defines the environment in which the application WAR will be deployed. Click on the ellipsis button and select an existing environment or create a new one. You can check the selected environment for compatibility with your project. A compatible environment should have Java 8, Tomcat 8 and PostgreSQL 9.1+ (if the project uses PostgreSQL database). If your project uses PostgreSQL, you will receive an email with the database connection details. Please use them when generating custom `context.xml` file, see *Custom context.xml path* field below. Besides, you should create an empty PostgreSQL database using the provider's web interface link containing in the email. The database name should be specified later in custom context.xml (see below).
+
image::jelastic_6.png[align="center"]

. Press *Generate* button next to the *Custom web.xml path* field. Studio will generate a special `web.xml` of the <<build.gradle_buildWar,single WAR>> comprising the Middleware and Web Client application blocks.
+
image::jelastic_2.png[align="center"]

. If your project uses HSQLDB, that is all - you can press *OK* and start deployment by clicking *Run > Deploy to cloud* main menu item.

. If your project uses PostgreSQL, go to the database administration web interface by the link in the email recieved after creation of the environment and create a database.

. Press *Generate* button next to the *Custom context.xml path* field and specify the database user, password, host and name.
+
image::jelastic_3.png[align="center"]

. Leave the *Include JDBC driver* and *Include context.xml* checkboxes selected.
+
image::jelastic_4.png[align="center"]

. Now you can press *OK* and start deployment by clicking *Run > Deploy to cloud* main menu item.

. After completing the deployment, use the link at the bottom left corner to open the application web interface.
+
image::jelastic_5.png[align="center"]

[[bluemix_deployment]]
==== Deployment to Bluemix Cloud

CUBA Studio provides support of IBM® Bluemix® cloud deployment in a few easy steps.

[TIP]
====
Bluemix cloud deployment is currently applicable only for projects using PostgreSQL database. HSQLDB is available with _in-process_ option only, that means the database will be recreated on every application restart, and the user data will be lost.
====

. Create an account in the Bluemix. Download and install:
.. Bluemix CLI: http://clis.ng.bluemix.net/ui/home.html
.. Cloud Foundry CLI: https://github.com/cloudfoundry/cli/releases
.. Make sure the commands `bluemix` and `cf` work in the command line. If not, add your `\IBM\Bluemix\bin` path to the `PATH` environment variable.

. Create a Space in the Bluemix with any space name. You can group several applications within one space, if needed.

. In the Space create an application server: *Create App* -> *CloudFoundry Apps* -> *Tomcat*.

. Specify the name of the application. The name should be unique as it will be used as part of the URL of your application.

. To create a Database service, click *Create service* in the Space dashboard and choose *ElephantSQL*.

. Open the application manager and connect the created DB Service to the application. Click *Connect Existing*. For the changes to take effect, the system requires to restage (update) the application. In our case it is not necessary, as the application will be redeployed.

. After the DB Service is connected, DB credentials become available by the *View Credentials* button. The DB properties are also stored in the `VCAP_SERVICES` environment variable of the application runtime and could be viewed by calling the `cf&#160;env` command. The created database is also accessible from outside of the Space, so you can work with it from your development environment.

. Setup your CUBA project to run with the PostgreSQL (the DBMS similar to one you have in the Bluemix).

. Generate DB scripts and start the local Tomcat server. Make sure the application works.

. Generate WAR-file to deploy the application to Tomcat.
.. Click *Deployment Settings* in the *Project Properties* section of Studio navigation panel.
.. Switch to the *WAR* tab.
.. Enable all the options using checkboxes, as for correct deployment it should be the *Single WAR* with JDBC driver and `context.xml` inside.
+
image::bluemix_war_settings.png[align="center"]

.. Click *Generate* button near the *Custom context.XML field*. In the opened dialog fill the credentials of the Database you have created in Bluemix.
+
Use the credentials from `uri` of your DB service following the example below:
+
[source, json]
----
include::{sourcesdir}/deployment/bluemix_credentials.json[]
----
+
*Database user*: `ldwpelpl`
+
*Database password*: `eFwXx6lNFLheO5maP9iRbS77Sk1VGO_T`
+
*Database URL*: `echo-01.db.elephantsql.com:5432`
+
*Database name*: `ldwpelpl`

.. Click *Generate* button to generate the custom `web.xml` file required for the single WAR.

.. Save the settings. Generate the WAR-file using the `buildWar` Gradle task in Studio or command line.
+
image::bluemix_buildWar.png[align="center"]
+
As a result, the  `app.war` appears in the `build/distributions/war/` sub-directory of the project.

. In the root directory of the project create manually the `manifest.yml` file. The contents of the file should be like follows:
+
[source, yml]
----
include::{sourcesdir}/deployment/bluemix_manifest.yml[]
----
+
where
+
* `path` is the relative path to WAR-file.
* `memory`: the default memory limit is 1G. You may want to allocate less or more memory to your application, this can also be done via Bluemix WEB interface. Note that the allocated memory affects the Runtime Cost.
* `name` is the name of the Tomcat application you have created in the Cloud above.
* `host`: the same as name.
* `env`: the environment variables used to set the Tomcat and Java versions.

. In the command line switch to the root directory of your CUBA project.
+
[source, yml]
----
cd your_project_directory
----

. Connect to Bluemix.
+
[source, yml]
----
bluemix api https://api.ng.bluemix.net
----

. Log in to your Bluemix account.
+
[source, yml]
----
cf login
----

. Deploy your WAR to your Tomcat.
+
[source, yml]
----
cf push
----
+
The `push` command gets all the required parameters from the `manifest.yml` file.

. You can find Tomcat server logs via Bluemix WEB-interface in the *Logs* tab on the application dashboard, as well as in command line using the command
+
[source, yml]
----
cf logs cuba-app --recent
----

. After the deployment process is completed, your application will become accessible in browser using the URL `host.domain`. This URL will be displayed in the *ROUTE* field in the table of your *Cloud Foundry Apps*.


[[heroku_deployment]]
==== WAR Deployment to Heroku

The section describes how to build WAR file and run it on https://www.heroku.com/[Heroku®] cloud platform.

[TIP]
====
This tutorial covers deployment of a CUBA project using PostgreSQL database.
====

Heroku account::
+
--
First create an account on Heroku using the web browser, free account `hobby-dev` is enough. Then login to the account and create new application using *New* button at the top of the page.

image::heroku_new_app.png[align="center"]

Select unique name (or left the field blank to assign automatically) and choose a server location. Now you have an application, for example `morning-beach-4895`, this is the Heroku application name.

First time you will be redirected to the *Deploy* tab. Use Heroku Git deployment method.

image::heroku_create_app.png[align="center"]
--

Heroku CLI::
+
--
. Install https://devcenter.heroku.com/articles/heroku-command-line[Heroku CLI].

. Open command prompt on your computer.

. Create an empty folder for CUBA project. Further on we will use `$PROJECT_FOLDER` for it.

. Navigate to `$PROJECT_FOLDER` and type:
+
[source]
----
heroku login
----

. Enter your credentials when prompted. From now on you don't need to enter credentials for this project anymore.

. Install Heroku CLI deployment plugin:
+
[source]
----
heroku plugins:install heroku-cli-deploy
----
--

PostgreSQL database::
+
--
Open https://dashboard.heroku.com[Heroku dashboard] in your web browser, go to *Resources* tab and click *Find more add-ons* button to find the database add-on.
Find *Heroku Postgres* block and click on it. Follow the instruction on the screen, click *Login to install / Install Heroku Postgres*.

Alternatively, you can install PostgreSQL using Heroku CLI:

[source]
----
heroku addons:create heroku-postgresql:hobby-dev --app morning-beach-4895
----

Here `morning-beach-4895` is your Heroku application name.

Now you can find the new database on the *Resources* tab. The database is connected to the Heroku application.

To obtain database credentials go to the *Administration* section of your PostgreSQL database in the dashboard and click *View credentials* button:

----
Host compute.amazonaws.com
Database d2tk
User nmmd
Port 5432
Password 9c05
URI postgres://nmmd:9c05@compute.amazonaws.com:5432/d2tk
----
--

Project deployment settings::
+
--
Create new CUBA project in `$PROJECT_FOLDER` using CUBA Studio.

Open *Deployment settings* and configure options as described below.

image::heroku_war_settings.png[align="center"]

* Select *Build WAR*
* Set application home directory to '.' (dot)
* Select *Include Tomcat's context.xml*
* Click *Generate* button next to the *Custom context.xml path* field. Then open the generated `modules/core/web/META-INF/war-context.xml` file and enter connection details using your database connection params and credentials:
+
[source, xml]
----
<Context>
    <!-- Database connection -->
    <Resource
      name="jdbc/CubaDS"
      type="javax.sql.DataSource"
      maxTotal="20"
      maxIdle="2"
      maxWaitMillis="5000"
      driverClassName="org.postgresql.Driver"
      url="jdbc:postgresql://compute.amazonaws.com/d2tk"
      username="nmmd"
      password="9c05"/>

      <!-- ... -->
</Context>
----

* Select *Single WAR for Middleware and Web Client*.

* Click *Generate* button next to the *Custom context.xml path* field.

* Copy the code shown below and paste it into the *App properties* field:
+
[source, groovy]
----
[
  'cuba.automaticDatabaseUpdate' : true,
  'cuba.logDir' : './logs',
  'cuba.confDir' : './${cuba.webContextName}/conf',
  'cuba.tempDir' : './${cuba.webContextName}/temp',
  'cuba.dataDir' : './${cuba.webContextName}/work',
  'cuba.download.directories' : '${cuba.tempDir};${cuba.logDir}',
  'cuba.dbDir' : 'web-inf:db'
]
----

Save deployment settings.
--

Build WAR file::
+
--
Build WAR file by executing the `buildWar` Gradle task. You can do it right from the Studio *Search* dialog or from the command line:

[source, groovy]
----
gradlew buildWar
----

In order to use `gradlew` command in the command line, create Gradle wrapper using Studio *Build* menu command beforehand.
--

Application setup::
+
--

* Download Tomcat Webapp Runner from https://mvnrepository.com/artifact/com.github.jsimone/webapp-runner. The version of Webapp Runner must conform to the Tomcat version in use. For example, version 8.5.11.3 of Webapp Runner is suitable for Tomcat version 8.5.11. Rename JAR to `webapp-runner.jar` and place it into `$PROJECT_FOLDER`.

* Download Tomcat DBCP from https://mvnrepository.com/artifact/org.apache.tomcat/tomcat-dbcp.
  Use the version corresponding to your Tomcat version, for example 8.5.11. Rename JAR to `tomcat-dbcp.jar` and place it into the `$PROJECT_FOLDER/libs` folder.

* Download PostgreSQL JDBC driver from https://mvnrepository.com/artifact/org.postgresql/postgresql/9.4.1212, rename JAR to `postgresql.jar` and place it into the `$PROJECT_FOLDER/libs` folder.

* Create a file named `Procfile` in `$PROJECT_FOLDER`. The file should contain the following text:
+
[source, bash]
----
web: java $JAVA_OPTS -cp webapp-runner.jar:libs/* webapp.runner.launch.Main --enable-naming --port $PORT --context-xml modules/core/web/META-INF/war-context.xml build/distributions/war/app.war
----
--

Git setup::
+
--
Open the command prompt in `$PROJECT_FOLDER` and run the commands listed below:

[source]
----
git init
heroku git:remote -a morning-beach-4895
git add .
git commit -am "Initial commit"
----
--

Application deployment::
+
--
Open the command prompt in `$PROJECT_FOLDER` and run the following command:

On *nix:

[source]
----
heroku jar:deploy webapp-runner.jar --includes libs/postgresql.jar:libs/tomcat-dbcp.jar:modules/core/web/META-INF/war-context.xml:build/distributions/war/app.war
----

On Windows:

[source]
----
heroku jar:deploy webapp-runner.jar --includes libs\postgresql.jar;libs\tomcat-dbcp.jar;modules\core\web\META-INF\war-context.xml;build\distributions\war\app.war
----

Open the *Resources* tab in Heroku dashboard. A new Dyno should appear with a command from your `Procfile`:

image::heroku_dyno.png[align="center"]

The application is deploying now. You can monitor logs to track the process.
--

Logs monitoring::
+
--
Wait for a message `++https://morning-beach-4895.herokuapp.com/  deployed to Heroku++`.

In order to track application logs, run the following command in the command line:

[source]
----
heroku logs --tail
----
--

After the deployment process is completed your application will be accessible in web browser by an URL like `++https://morning-beach-4895.herokuapp.com++`.

You can also open the application from the Heroku dashboard using the *Open app* button.

==== Tomcat deployment to Heroku

The document describes how to deploy Tomcat to run CUBA on https://www.heroku.com/[Heroku®].

[TIP]
====
This tutorial covers Heroku cloud deployment for CUBA project using PostgreSQL database.
====

*Prerequisities*

. *CUBA* project
. *Github* repo
. *Git* installed on your computer
. *Heroku account*
. *Heroku CLI* https://devcenter.heroku.com/articles/heroku-command-line

*Git*

To initiate Heroku application deployment you need the ability to perform push to your remote git repository. For that reason a git utility installed on your computer is mandatory. As an option, you can install built-in git during Heroku CLI installation.

*Heroku account*

Create an account on https://www.heroku.com/[Heroku®] using the web browser, free account 'hobby-dev' is enough. Then login to Heroku account and create new application using 'New' button at the top of the page.

image::heroku_new_app.png[align="center"]

Select unique name (or left the field blank to assign automatically) and choose server location. +
Now you have an application, for example `space-sheep-02453`, this is a Heroku application name. +

image::heroku_create_newapp.png[align="center"]

First time you will be redirected to 'Deploy' tab. Use GitHub deployment method.

image::heroku_github_selected.png[align="center"]

Follow the screen instructions how to authorize your GitHub account. +

image::heroku_github_access.png[align="center"]

Click Search button to list all available repositories then connect to desired repo. When your Heroku application is connected to GitHub you are able to activate Automatic Deploys. This allows to redeploy Heroku application automatically on each git push event. In this tutorial the option is enabled.

image::heroku_github_option.png[align="center"]

*Heroku CLI*

. Open command line utility on your computer (Cmd, PowerShell, Bash etc)
. Navigate to project directory ($PROJECT_FOLDER) and type command 'heroku login'
. Enter your credentials when prompted. From this moment you won't needed to enter credentials for this project anymore.

[source, bash]
----
heroku login
----

*Postgres Database*

. Return to web browser with Heroku https://dashboard.heroku.com[dashboard]
. Go to Resources tab
. Click 'Find more add-ons' button to find the database add-on
. Find 'Heroku Postgres' block and click it
. Follow the instruction on the screen, click 'Login to install' / 'Install Heroku Postgres' +
*OR* +
. Type command in Heroku CLI:

[source, groovy]
----
heroku addons:create heroku-postgresql:hobby-dev --app space-sheep-02453
----
where `space-sheep-02453` is your Heroku application name.

Now you can find a new database mention appears on Resources tab. New database is connected to Heroku application.


*Database administration*

To obtain database credentials go to Administartion section of your Postgres database in Heroku dashboard and click 'View credentials' button +

[source, bash]
----
Host compute.amazonaws.com
Database zodt
User artd
Port 5432
Password 367f
URI postgres://artd:367f@compute.amazonaws.com:5432/zodt
----


*Project deployment settings*

. On your computer navigate to $PROJECT_FOLDER
. Open file `modules/core/web/META-INF/context.xml`
. Copy the content of `modules/core/web/META-INF/context.xml` to `modules/core/web/META-INF/heroku-context.xml`
. Fill `heroku-context.xml` with your actual database connection details (see example below):

[source, xml]
----
<Context>

    <!-- Database connection -->

        <Resource driverClassName="org.postgresql.Driver"
                  maxIdle="2"
                  maxTotal="20"
                  maxWaitMillis="5000"
                  name="jdbc/CubaDS"
                  password="367f"
                  type="javax.sql.DataSource"
                  url="jdbc:postgresql://compute.amazonaws.com/zodt"
                  username="artd"/>
        <!-- Switch off session serialization -->
        <Manager pathname=""/>

</Context>
----

*build.gradle*

Add following gradle task to your `$PROJECT_FOLDER/build.gradle`

[source, groovy]
----
include::{sourcesdir}/deployment/heroku_buildGradle.groovy[]
----

*Gradle Wrapper*

Your project requires gradle wrapper. To add it you can use Cuba Studio, a command from menu 'Build/Create' or update gradle wrapper

. Create file system.properties with following content (for local JDK 1.8.0_121): `java.runtime.version=1.8.0_121`
. Check that system.properties, gradlew, gradlew.bat and gradle are not in .gitignore
. Add the files to repository and commit it

[source, bash]
----
git add gradlew gradlew.bat gradle/* system.properties
git commit -am "Gradle Wrapper added"
----

image::heroku_gradlew_create.png[align="center"]

This action adds gradlew/gradlew.bat file into the project. Because Heroku uses this wrapper, you must add gradlew, gradlew.bat gradle/wrapper to GitHub. Ensure that these items are not in .gitignore.

*Procfile*

A command that launches the application on Heroku side is passed by special file `Procfile`. Create `Procfile` in $PROJECT_FOLDER with following text:

[source, groovy]
----
web: cd ./deploy/tomcat/bin && export 'JAVA_OPTS=-Dport.http=$PORT' && ./catalina.sh run
----

This provides JAVA_OPTS environment setting to Tomcat which starts by catalina script.

*Deploy application*

Once you commit and push all changes to GitHub, Heroku starts redeploying the application.

[source, bash]
----
git push
----

Building process is available in dashboard on Activity tab:

image::heroku_activity_tomcat.png[align="center"]

Click `View build log` link to track build log.

After building process is completed, your application will become accessible in browser using the https://space-sheep-02453.herokuapp.com/.

You can open the application from Heroku dashboard using the button https://space-sheep-02453.herokuapp.com/['Open app']

image::heroku_open_app.png[align="center"]

*Logs*

Heroku application log is shown by console command:

[source, groovy]
----
heroku logs --tail --app space-sheep-02453
----

Tomcat logs are also available in web application:

image::heroku_tomcat_logs_menu.png[align="center"]

image::heroku_tomcat_logs_list.png[align="center"]

image::heroku_tomcat_logs_tail.png[align="center"]

Done.


[[scaling]]
=== Application Scaling

This section describes ways to scale a CUBA application that consists of the *Middleware* and the *Web Client* for increased load and stronger fault tolerance requirements.

[cols="2", frame="all", width="70%"]
|===

a| *Stage 1. Both blocks are deployed on the same application server.*

This is the simplest option implemented by the standard <<fast_deployment,fast deployment>> procedure.

In this case, maximum data transfer performance between the *Web Client* and the *Middleware* is provided, because when the <<cuba.useLocalServiceInvocation,cuba.useLocalServiceInvocation>> application property is enabled, the Middleware services are invoked bypassing the network stack.
| image:scaling_1.png[align="center"]

a| *Stage 2. The Middleware and the Web Client blocks are deployed on separate application servers.*

This option allows you to distribute load between two application servers and use server resources better. Furthermore, in this case the load coming from web users has smaller effect on the other processes execution. Here, the other processes mean handling other client types (for example, Desktop), running <<scheduled_tasks,scheduled tasks>> and, potentially, integration tasks which are performed by the middle layer.

Requirements for server resources:

* Tomcat 1 (Web Client):
** Memory size – proportional to the number of simultaneous users
** CPU power – depends on the usage intensity
* Tomcat 2 (Middleware):
** Memory size – fixed and relatively small
** CPU power – depends on the intensity of web client usage and of other processes

In this case and when more complex deployment options are used, the Web Client's <<cuba.useLocalServiceInvocation,cuba.useLocalServiceInvocation>> application property should be set to `false`, and <<cuba.connectionUrlList,cuba.connectionUrlList>> property should contain the URL of the Middleware block.
| image:scaling_2.png[align="center"]

| *Stage 3. A cluster of Web Client servers works with one Middleware server.*

This option is used when memory requirements for the Web Client exceed the capabilities of a single JVM due to a large number of concurrent users. In this case, a cluster of Web Client servers (two or more) is started and user connection is performed through a Load Balancer. All Web Client servers work with one Middleware server.

Duplication of Web Client servers automatically provides fault tolerance at this level. However, the replication of HTTP sessions is not supported, in case of unscheduled outage of one of the Web Client servers, all users connected to it will have to login into the application again.

Configuration of this option is described in <<cluster_webclient,>>.
| image:scaling_3.png[align="center"]

| *Stage 4. A cluster of Web Client servers working with a cluster of Middleware servers.*

This is the maximum deployment option, which provides fault tolerance and load balancing for the Middleware and the Web Client.

Connection of users to the Web Client servers is performed through a load balancer. The Web Client servers work with a cluster of Middleware servers. They do not need an additional load balancer – it is sufficient to determine the list of URLs for the Middleware servers in the <<cuba.connectionUrlList,cuba.connectionUrlList>> application property.

Middleware servers exchange the information about user sessions, locks, etc. In this case, full fault tolerance of the Middleware is provided – in case of an outage of one of the servers, execution of requests from client blocks will continue on an available server without affecting users.

Configuration of this option is described in <<cluster_mw,>>.
| image:scaling_4.png[align="center"]

|===

[[cluster_webclient]]
==== Setting up a Web Client Cluster

This section describes the following deployment configuration:

image::cluster_webclient.png[align="center"]

Servers `host1` and `host2` host Tomcat instances with the `app` web-app implementing the Web Client block. Users access the load balancer at `++http://host0/app++`, which redirects their requests to the servers. Server `host3` hosts a Tomcat instance with the `app-core` web-app that implements the Middleware block.

[[cluster_webclient_lb]]
===== Installing and Setting up a Load Balancer

Let us consider the installation of a load balancer based on *Apache HTTP Server* for *Ubuntu 14.04*.

. Install *Apache HTTP Server* and its *mod_jk* module:
+
`$ sudo apt-get install apache2 libapache2-mod-jk`

. Replace the contents of the `/etc/libapache2-mod-jk/workers.properties` file with the following:
+
[source, properties]
----
workers.tomcat_home=
workers.java_home=
ps=/

worker.list=tomcat1,tomcat2,loadbalancer,jkstatus

worker.tomcat1.port=8009
worker.tomcat1.host=host1
worker.tomcat1.type=ajp13
worker.tomcat1.connection_pool_timeout=600
worker.tomcat1.lbfactor=1

worker.tomcat2.port=8009
worker.tomcat2.host=host2
worker.tomcat2.type=ajp13
worker.tomcat2.connection_pool_timeout=600
worker.tomcat2.lbfactor=1

worker.loadbalancer.type=lb
worker.loadbalancer.balance_workers=tomcat1,tomcat2

worker.jkstatus.type=status
----

. Add the lines listed below to `/etc/apache2/sites-available/000-default.conf`:
+
[source, xml]
----
<VirtualHost *:80>
...
    <Location /jkmanager>
        JkMount jkstatus
        Order deny,allow
        Allow from all
    </Location>

    JkMount /jkmanager/* jkstatus
    JkMount /app loadbalancer
    JkMount /app/* loadbalancer

</VirtualHost>
----

. Restart the Apache HTTP service:
+
`$ sudo service apache2 restart`

[[cluster_webclient_tomcat]]
===== Setting up Web Client Servers

On the Tomcat 1 and Tomcat 2 servers, the following settings should be applied:

. In `tomcat/conf/server.xml`, add the `jvmRoute` parameter equivalent to the name of the worker specified in the load balancer settings for `tomcat1` and `tomcat2`:
+
[source, xml]
----
<Server port="8005" shutdown="SHUTDOWN">
  ...
  <Service name="Catalina">
    ...
    <Engine name="Catalina" defaultHost="localhost" jvmRoute="tomcat1">
      ...
    </Engine>
  </Service>
</Server>
----

. Set the following application properties in `tomcat/conf/app/local.app.properties`:
+
[source, properties]
----
cuba.useLocalServiceInvocation = false
cuba.connectionUrlList = http://host3:8080/app-core

cuba.webHostName = host1
cuba.webPort = 8080
cuba.webContextName = app
----
+
<<cuba.webHostName,cuba.webHostName>>, <<cuba.webPort,cuba.webPort>> and <<cuba.webContextName,cuba.webContextName>> parameters are not mandatory for WebClient cluster, but they allow easier identification of a server in other platform mechanisms, such as the <<jmx_console, JMX console>>. Additionally, *Client Info* attribute of the *User Sessions* screen shows an identifier of the Web Client that the current user is working with.

[[cluster_mw]]
==== Setting up a Middleware Cluster

This section describes the following deployment configuration:

image::cluster_mw.png[align="center"]

Servers `host1` and `host2` host Tomcat instances with the `app` web-app implementing the Web Client block. Cluster configuration for these servers is described in the <<cluster_webclient,previous section>>. Servers `host3` and `host4` host Tomcat instances with the `app-core` web-app implementing the Middleware block. They are configured to interact and share information about user sessions, locks, cash flushes, etc.

[[cluster_mw_client]]
===== Setting up Connection to the Middleware Cluster

In order for the client blocks to be able to work with multiple Middleware servers, the list of URLs should be specified to these servers in the <<cuba.connectionUrlList,cuba.connectionUrl>> application property. For the Web Client, this can be done in `tomcat/conf/app/local.app.properties`:

[source, properties]
----
cuba.useLocalServiceInvocation = false
cuba.connectionUrlList = http://host3:8080/app-core,http://host4:8080/app-core

cuba.webHostName = host1
cuba.webPort = 8080
cuba.webContextName = app
----

The order of servers in `cuba.connectionUrl` defines priority and order for the client to send the requests. In the example above, the client will first attempt to access `host3`, and then, if it is not available, `host4`. If a request to `host4` completes successfully, the client will save `host4` as the first server in the list and will continue working with this server. Restarting a client will reset the initial values. Uniform distribution of clients among all servers can be achieved using the <<cuba.randomServerPriority,cuba.randomServerPriority>> property.

[[cluster_mw_server]]
===== Configuring Interaction between Middleware Servers

Middleware servers can maintain shared lists of <<userSession,user sessions>> and other objects and coordinate invalidation of caches. <<cuba.cluster.enabled,cuba.cluster.enabled>> property should be enabled on each server to achieve this. Example of the `tomcat/conf/app-core/local.app.properties` file is shown below:

[source, properties]
----
cuba.cluster.enabled = true

cuba.webHostName = host3
cuba.webPort = 8080
cuba.webContextName = app-core
----

For the Middleware servers, correct values of the <<cuba.webHostName,cuba.webHostName>>, <<cuba.webPort,cuba.webPort>> and <<cuba.webContextName,cuba.webContextName>> properties should be specified to form a unique <<serverId,Server ID>>.

Interaction mechanism is based on link:http://www.jgroups.org[JGroups]. It is possible to fine-tune the interaction using the `jgroups.xml` file located in the root of cuba-core-<version>.jar. It can be copied to `tomcat/conf/app-core` and configured as needed.

`ClusterManagerAPI` bean provides the program interface for servers interaction in the Middleware cluster. It can be used in the application – see JavaDocs and examples in the platform code.

[[serverId]]
==== Server ID

_Server ID_ is used for reliable identification of servers in a *Middleware* cluster. The identifier is formatted as `host:port/context`:

[source, plain]
----
tezis.haulmont.com:80/app-core
----

[source, plain]
----
192.168.44.55:8080/app-core
----

The identifier is formed based on the configuration parameters <<cuba.webHostName,cuba.webHostName>>, <<cuba.webPort,cuba.webPort>>, <<cuba.webContextName,cuba.webContextName>>, therefore it is very important to specify these parameters for the Middleware blocks working within the cluster.

Server ID can be obtained using the `ServerInfoAPI` bean or via the <<serverInfoMBean,ServerInfoMBean>> JMX interface.

[[jmx_tools]]
=== Using JMX Tools

This section describes various aspects of using *Java Management Extensions* in CUBA-based applications.

[[jmx_console]]
==== Built-In JMX Console

The Web Client module of the *cuba* <<app_components,application component>> contains JMX objects viewing and editing tool. The entry point for this tool is `com/haulmont/cuba/web/app/ui/jmxcontrol/browse/display-mbeans.xml` screen registered under the `jmxConsole` identifier and accessible via *Administration* > *JMX Console* in the standard application menu.

Without extra configuration, the console shows all JMX objects registered in the JVM where the Web Client block of the current user is running. Therefore, in the simplest case, when all application blocks are deployed to one web container instance, the console has access to the JMX beans of all tiers as well as the JMX objects of the JVM itself and the web container.

Names of the application beans have a prefix corresponding to the name of the web-app that contains them. For example, the `app-core.cuba:type=CachingFacade` bean has been loaded by the *app-core* web-app implementing the Middleware block, while the `app.cuba:type=CachingFacade` bean has been loaded by the *app* web-app implementing the Web Client block.

JMX console can also work with the JMX objects of a remote JVM. This is useful when application blocks are deployed over several instances of a web container, for example separate Web Client and Middleware.

To connect to a remote JVM, a previously created connection should be selected in the *JMX Connection* field of the console, or a new connection can be created:

.Editing a JMX Connection
image::jmx-connection-edit.png[align="center"]

To get a connection, JMX host, port, login and password should be specified. There is also the *Host name* field, which is populated automatically, if any CUBA-application block is detected at the specified address. In this case, the value of this field is defined as the combination of <<cuba.webHostName,cuba.webHostName>> and <<cuba.webPort,cuba.webPort>> properties of this block, which enables identifying the server that contains it. If the connection is done to a 3rd party JMX interface, then the *Host name* field will have the "Unknown JMX interface" value. However it can be changed arbitrarily.

In order to allow a remote JVM connection, the JVM should be configured properly (see below).

[[jmx_remote_access]]
==== Setting up a Remote JMX Connection

This section describes *Tomcat* startup configuration required for a remote connection of JMX tools.

[[jmx_remote_access_tomcat_windows]]
===== Tomcat JMX for Windows

* Edit `bin/setenv.bat` in the following way:
+
[source, properties]
----
set CATALINA_OPTS=%CATALINA_OPTS% ^
-Dcom.sun.management.jmxremote ^
-Djava.rmi.server.hostname=192.168.10.10 ^
-Dcom.sun.management.jmxremote.ssl=false ^
-Dcom.sun.management.jmxremote.port=7777 ^
-Dcom.sun.management.jmxremote.authenticate=true ^
-Dcom.sun.management.jmxremote.password.file=../conf/jmxremote.password ^
-Dcom.sun.management.jmxremote.access.file=../conf/jmxremote.access
----
+
Here, the `java.rmi.server.hostname` parameter should contain the actual IP address or the DNS name of the computer where the server is running; `com.sun.management.jmxremote.port` sets the port for JMX tools connection.

* Edit the `conf/jmxremote.access` file. It should contain user names that will be connecting to the JMX and their access level. For example:
+
[source, plain]
----
admin readwrite
----

* Edit the `conf/jmxremote.password` file. It should contain passwords for the JMX users, for example:
+
[source, plain]
----
admin admin
----

* The password file should have reading permissions only for the user running the *Tomcat*. server. You can configure permissions the following way:

** Open the command line and go to the conf folder

** Run the command:`++cacls jmxremote.password /P "domain_name\user_name":R++`
+
where `++domain_name\user_name++` is the user's domain and name

** After this command is executed, the file will be displayed as locked (with a lock icon) in *Explorer*.

* If *Tomcat* is installed as a Windows service, than the service should be started on behalf of the user who has access permissions for jmxremote.password. It should be kept in mind that in this case the `bin/setenv.bat` file is ignored and the corresponding JVM startup properties should be specified in the application that configures the service.

[[jmx_remote_access_tomcat_linux]]
===== Tomcat JMX for Linux

* Edit `bin/setenv.sh` the following way:
+
[source, properties]
----
CATALINA_OPTS="$CATALINA_OPTS -Dcom.sun.management.jmxremote \
-Djava.rmi.server.hostname=192.168.10.10 \
-Dcom.sun.management.jmxremote.port=7777 \
-Dcom.sun.management.jmxremote.ssl=false \
-Dcom.sun.management.jmxremote.authenticate=true"

CATALINA_OPTS="$CATALINA_OPTS -Dcom.sun.management.jmxremote.password.file=../conf/jmxremote.password -Dcom.sun.management.jmxremote.access.file=../conf/jmxremote.access"
----
+
Here, the `java.rmi.server.hostname` parameter should contain the real IP address or the DNS name of the computer where the server is running; `com.sun.management.jmxremote.port` sets the port for JMX tools connection

* Edit `conf/jmxremote.access` file. It should contain user names that will be connecting to the JMX and their access level. For example:
+
[source, plain]
----
admin readwrite
----

* Edit the `conf/jmxremote.password` file. It should contain passwords for the JMX users, for example:
+
[source, plain]
----
admin admin
----

* The password file should have reading permissions only for the user running the *Tomcat* server. Permissions for the current user can be configured the following way:

** Open the command line and go to the conf folder.

** Run the command:
+
`chmod go-rwx jmxremote.password`

[[server_push_settings]]
=== Server Push Settings

CUBA applications use server push technology in the <<background_tasks,Background Tasks>> mechanism. It may require an additional setup of the application and proxy server (if any).

By default, server push uses the WebSocket protocol. The following application properties affect the platform server push functionality:

<<cuba.web.pushLongPolling,cuba.web.pushLongPolling>>

<<cuba.web.pushEnabled,cuba.web.pushEnabled>>

If users connect to the application server via a proxy that does not support WebSocket, set `cuba.web.pushLongPolling` to `true` and increase proxy request timeout to 10 minutes or more.

Below is an example of the *Nginx* web server settings for using WebSocket:

[source, plain]
----
location / {
    proxy_set_header X-Forwarded-Host $host;
    proxy_set_header X-Forwarded-Server $host;
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_read_timeout     3600;
    proxy_connect_timeout  240;
    proxy_set_header Host $host;
    proxy_set_header X-RealIP $remote_addr;

    proxy_pass http://127.0.0.1:8080/;
    proxy_set_header X-Forwarded-Proto $scheme;

    proxy_set_header Upgrade $http_upgrade;
    proxy_set_header Connection "upgrade";
}
----
