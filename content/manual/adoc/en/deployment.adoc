[[deployment]]
== Application Deployment

This chapter describes different aspects of CUBA applications deployment and operation.

image::DeploymentStructure.png[align="center"]

In the example above, the application prevents a single point of failure existence, provides load balancing and various client types connection. In the simplest case, however, the server part of an application can be installed on one computer that includes the database. Various deployment options depending on load and fault tolerance requirements are described in detail in <<scaling,>>.

[[app_dirs]]
=== Application Directories

This section describes file system directories used by various <<app_tiers,application blocks>> at runtime.

[[conf_dir]]
==== Configuration Directory

The configuration directory is intended for resources that complement and override application properties, user interface and business logic after the application is deployed. Overriding is provided by the loading mechanism of the <<resources,Resources>> infrastructure interface. Firstly it performs search in the configuration directory and then in CLASSPATH, so that resources from the configuration directory take precedence over identically named resources located in JAR files and class directories.

The configuration directory may contain the following resource types:

*  <<app_properties_files,local.app.properties>> – a file that defines deployment parameters of the server-based application blocks.

*  <<metadata.xml,metadata.xml>>, <<persistence.xml,persistence.xml>>, <<views.xml,views.xml>>, <<remoting-spring.xml,remoting-spring.xml>> configuration files.

* <<screen_xml,XML-descriptors>> of UI screens.

* <<screen_controller,Controllers>> of UI screens in the form of Java or Groovy source code.

* Groovy scripts or classes, and Java source code that is used by the application via the <<scripting,Scripting>> interface.

The location of the configuration directory is determined by the <<cuba.confDir,cuba.confDir>> application property. For the Middleware, Web Client and Web Portal blocks in <<fast_deployment,fast deployment>> configuration in Tomcat, it is a subdirectory with the web application name in the `tomcat/conf` directory, for example, `tomcat/conf/app-core` for the Middleware.

[[work_dir]]
==== Work Directory

The application uses the work directory to store data and configuration files.

For example, the `filestorage` subdirectory of the work directory is used by the <<file_storage,file storage>>. In addition, the Middleware block saves generated <<persistence.xml,persistence.xml>> and orm.xml files in the work directory on start.

Work directory location is determined by the <<cuba.dataDir,cuba.dataDir>> application property. For the Middleware, Web Client and Web Portal blocks in <<fast_deployment,fast deployment>> configuration in Tomcat, it is a subdirectory with the name of the web application in the `tomcat/work` directory.

[[log_dir]]
==== Log Directory

The content and settings of the log files are determined by the configuration of the *Logback* framework. The configuration file location is determined by <<logback.configurationFile,logback.configurationFile>> system property.

This directory can also be used to store arbitrary information about the running application. The log directory location is determined by <<cuba.logDir,cuba.logDir>> application property. For the Middleware, Web Client and Web Portal blocks in <<fast_deployment,fast deployment>> configuration in Tomcat, it is the `tomcat/logs` directory.

See also <<logging,>>.

[[temp_dir]]
==== Temporary Directory

This directory can be used for creating arbitrary temporary files at application run time. The path to the temporary directory is determined by the <<cuba.tempDir,cuba.tempDir>> application property. For the Middleware, Web Client and Web Portal blocks in <<fast_deployment,fast deployment>> configuration in Tomcat, it is a subdirectory with the web application name in the `tomcat/temp` directory.

[[db_dir]]
==== Database Scripts Directory

This directory of the deployed Middleware block stores the set of SQL scripts to create and update the DB.

The script directory structure reproduces the one described in <<db_scripts,>>, but it also has an additional top level that separates <<app_components,application components>> and the application scripts. The numbering of top level directories is performed by project build <<build.gradle,tasks>>.

The DB scripts directory location is determined by <<cuba.dbDir,cuba.dbDir>> application property. For <<fast_deployment,fast deployment>> configuration in *Tomcat*, it is the `WEB-INF/db` subdirectory of the middleware web application directory: `tomcat/webapps/app-core/WEB-INF/db`.

[[deployment_variants]]
=== Deployment Options

This section describes different ways to deploy CUBA applications:

* <<fast_deployment,Fast deployment in Tomcat>>

* <<war_deployment,WAR deployment to Jetty>>

* <<tomcat_war_deployment,WAR deployment to Tomcat Windows Service>>

* <<uberjar_deployment,UberJAR>>

* <<jelastic_deployment,Jelastic Cloud>>

* <<bluemix_deployment,Bluemix Cloud>>

* <<heroku_deployment,Heroku Cloud>>

[[fast_deployment]]
==== Fast Deployment in Tomcat

Fast deployment is used by default when developing an application, as it provides minimum time for building, installation and starting the application. This option can also be used in production.

Fast deployment is performed using the <<build.gradle_deploy,deploy>> task that is declared for *core* and *web* modules in the `build.gradle` file. Before the first execution of `deploy`, a local Tomcat server should be set up and initialized using the <<build.gradle_setupTomcat,setupTomcat>> task.

[WARNING]
====
Please make sure your environment does not contain `CATALINA_HOME`, `CATALINA_BASE` and `CLASSPATH` variables. They may cause problems starting Tomcat without any reflection in logs. Reboot your computer after removing the variables.
====

As result of fast deployment, the following structure is created in the directory that is specified by the `cuba.tomcat.dir` property of the `build.gradle` script (only important directories and files are listed below):

[source, plain]
----
bin/
    setenv.bat, setenv.sh
    startup.bat, startup.sh
    debug.bat, debug.sh
    shutdown.bat, shutdown.sh

conf/
    catalina.properties
    server.xml
    logback.xml
    logging.properties
    Catalina/
        localhost/
    app/
    app-core/

lib/
    hsqldb-2.2.9.jar

logs/
    app.log

shared/
    lib/

temp/
    app/
    app-core/

webapps/
    app/
    app-core/

work/
    app/
    app-core/
----

* `bin` – the directory that contains tools to start and stop the Tomcat server:

** `setenv.bat`, `setenv.sh` – the scripts that set environment variables. These scripts should be used for setting JVM memory parameters, specifying a configuration file for <<logging_setup_tomcat,logging>>, configuring <<jmx_remote_access,access to JMX>>, parameters to <<debug_setup,connect the debugger>>.

** `startup.bat`, `startup.sh` – the scripts that start Tomcat. The server starts in a separate console window on *Windows* and in background on **nix*.
+
To start the server in the current console window, use the following commands instead of `++startup.*++`:
+
`> catalina.bat run`
+
`$ ./catalina.sh run`

** `debug.bat`, `debug.sh` – the scripts that are similar to `++startup.*++`, but start Tomcat with an ability to connect the debugger. These scripts are launched when running the <<build.gradle_start,start>> task of the build script.

** `shutdown.bat`, `shutdown.sh` – the scripts that stop Tomcat.

* `conf` – the directory that contains configuration files of Tomcat and its deployed applications.

** `catalina.properties` – the Tomcat properties. To load shared libraries from the `shared/lib` directory (see below), this file should contain the following line:
+
[source, properties]
----
shared.loader=${catalina.home}/shared/lib/*.jar
----

** `server.xml` – Tomcat configuration descriptor.

** `logback.xml` – application <<logging_setup_tomcat,logging>> configuration descriptor.

** `logging.properties` – Tomcat server logging configuration descriptor.

** `Catalina/localhost` – in this directory, <<context.xml,context.xml>> application deployment descriptors can be placed. Descriptors located in this directory take precedence over the descriptors in the `META-INF` directories of the application. This approach is often convenient for production environment. For example, with this descriptor, it is possible to specify the database connection parameters that are different from those specified in the application itself.
+
Server-specific deployment descriptor should have the application name and the `.xml` extension. So, to create this descriptor, for example, for the `app-core` application, copy the contents of the `webapps/app-core/META-INF/context.xml` file to the `conf/Catalina/localhost/app-core.xml` file.

** `app` – web client application <<conf_dir,configuration directory>>.

** `app-core` – middleware application <<conf_dir,configuration directory>>.

* `lib` – directory of the libraries that are loaded by the server's _common classloader_. These libraries are available for both the server and all web applications deployed in it. In particular, this directory should have JDBC drivers of the utilized databases (`hsqldb-XYZ.jar`, `postgresql-XYZ.jar`, etc.)

* `logs` – application and server <<logging,logs>> directory. The main log file of the application is `app.log` (see <<logging_setup_tomcat>>).

* `shared/lib` – directory of libraries that are available to all deployed applications. These libraries classes are loaded by the server's special _shared classloader_. Its usage is configured in the `conf/catalina.properties` file as described above.
+
The <<build.gradle_deploy,deploy>> task of the build script copies all libraries not listed in the `jarNames` parameter, i.e. not specific for the given application, into this directory.

* `temp/app`, `temp/app-core` – web client and the middleware applications <<temp_dir,temporary directories>>.

* `webapps` – web application directories. Each application is located in its own subdirectory in the _exploded WAR_ format.
+
The <<build.gradle_deploy,deploy>> task of the build script create application subdirectories with the names specified in the `appName` parameters and, among other things, copy the libraries listed in the `jarNames` parameter to the `WEB-INF/lib` subdirectory for each application.

* `work/app`, `work/app-core` – web client and the middleware applications <<work_dir,work directories>>.

[[tomcat_in_prod]]
===== Using Tomcat in Production

By default, the <<fast_deployment,fast deployment>> procedure creates the `app` and `app-core` web applications running on port 8080 of the local Tomcat instance. It means that the web client is available at `++http://localhost:8080/app++`.

You can use this Tomcat instance in production just by copying the `tomcat` directory to the server. All you have to do is to set up the server host name in both `conf/app/local.app.properties` and `conf/app-core/local.app.properties` files (create the files if they do not exist):

[source, properties]
----
cuba.webHostName = myserver
cuba.webAppUrl = http://myserver:8080/app
----

Besides, set up the connection to you production database. You can do it in the <<context.xml>> file of your web application (`webapps/app-core/META-INF/context.xml`), or copy this file to `conf/Catalina/localhost/app-core.xml` as described in the previous section to separate development and production settings.

You can create the production database from a development database backup, or set up the automatic creation and further updating of the database. See <<db_update_in_prod>>.

If you want to change the Tomcat port or web context (the last part of the URL after `/`), use *Studio*:

* Open the project in Studio.

* Go to *Project Properties* > *Edit* > *Advanced*.

* To change the web context, edit the *Modules prefix* field.

* To change the Tomcat port, edit the *Tomcat ports* > *HTTP port* field.

If you want to use the root context for the web client (`++http://myserver:8080++`), rename `app` directories to `ROOT`

[source, plain]
----
tomcat/
    conf/
        ROOT/
            local.app.properties
        app-core/
            local.app.properties
    webapps/
        ROOT/
        app-core/
----

and use `/` as the web context name in `conf/ROOT/local.app.properties`:

[source, properties]
----
cuba.webContextName = /
----

[[war_deployment]]
==== WAR deployment to Jetty

You can deploy CUBA applications to WAR files using the <<build.gradle_buildWar,buildWar>> build task and run them on any Java servlet container. An example of deployment of the WAR files to the *Jetty* web server is provided below.

. Add the <<build.gradle_buildWar, buildWar>> task to the end of <<build.gradle,build.gradle>>:
+
[source, groovy]
----
include::{sourcesdir}/deployment/warDeployment_1.groovy[]
----
+
[TIP]
====
Please note that we build two separate WAR files for Middleware and Web Client blocks here. If you want to combine them into one WAR file with the `singleWar = true` parameter, provide the special `web.xml` file as described in <<build.gradle_buildWar,this section>>.
====

. Start build process:
+
[source, plain]
----
gradlew buildWar
----
+
As a result, the `app-core.war` and `app.war` files will be created in the `build\distributions\war` project subdirectory.

. Create an application home directory, for example, `c:\work\app_home`.

. Download and install Jetty to a local directory, for example `c:\work\jetty-home`. This example has been tested on `jetty-distribution-9.3.6.v20151106.zip`.

. Create the `c:\work\jetty-base` directory, open the command prompt in it and execute:
+
[source, plain]
----
java -jar c:\work\jetty-home\start.jar --add-to-start=http,jndi,deploy,plus,ext,resources
----

. Create the `c:\work\jetty-base\app-jetty.xml` file with the following contents (for a PostgreSQL database named `test`):
+
[source, xml]
----
include::{sourcesdir}/deployment/warDeployment_2.xml[]
----

. Add the following text to the beginning of `c:\work\jetty-base\start.ini` file:
+
[source, plain]
----
include::{sourcesdir}/deployment/warDeployment_3.ini[]
----

. Copy the JDBC driver for your database to the `c:\work\jetty-base\lib\ext` directory. You can take the driver file from the CUBA Studio `lib` directory or from the `build\tomcat\lib` project directory. In case of PostgreSQL database, it is `postgresql-9.1-901.jdbc4.jar`.

. Copy WAR files to the `c:\work\jetty-base\webapps` directory.

. Open the command prompt in the `c:\work\jetty-base` directory and run:
+
[source, plain]
----
java -jar c:\work\jetty-home\start.jar
----

. Open `++http://localhost:8080/app++` in your web browser.


[[tomcat_war_deployment]]
==== WAR deployment to Tomcat Windows Service

. Add the <<build.gradle_buildWar, buildWar>> task to the end of <<build.gradle,build.gradle>>:
+
--
[source, groovy]
----
include::{sourcesdir}/deployment/warDeployment_2.groovy[]
----

If the target server parameters differ from what you have on the local Tomcat used for <<fast_deployment,fast deployment>>, provide appropriate application properties. For example, if the target server runs on port 9999, the task definition should be as follows:

[source, groovy]
----
include::{sourcesdir}/deployment/warDeployment_3.groovy[]
----

You can also specify a different `context.xml` file to setup the connection to the production database, for example:

[source, groovy]
----
include::{sourcesdir}/deployment/warDeployment_4.groovy[]
----
--

. Run the `buildWar` gradle task. As a result, `app.war` and `app-core.war` files will be generated in the `build/distibutions` directory of your project.
+
[source, plain]
----
gradlew buildWar
----

. Download and run Tomcat 8 Windows Service Installer.

. Go to the `bin` directory of the installed server and run `tomcat8w.exe` with the administrative rights.
Set *Maximum memory pool* to 1024MB on the *Java* tab. Then go to the *General* tab and restart the service.
+
image::tomcatPropeties.jpg[align="center"]

. Add `-Dfile.encoding=UTF-8` to the _Java Options_ field.

. Copy the generated `app.war` and `app-core.war` files to the `webapps` directory of the server.

. Start the Tomcat service.

. Open `++http://localhost:8080/app++` in your web browser.

[[uberjar_deployment]]
==== UberJAR Deployment

This is one of the simplest ways to run your CUBA application in a production environment. You need to build all-in-one JAR files for your application blocks using the <<build.gradle_buildUberJar>> Gradle task (see also the *Deployment settings > Uber JAR* page in Studio) and then you can run the application from the command line using the `java` executable:

----
java -jar app-core.jar

java -jar app.jar
----

All parameters of the application are defined at the build time. The default port of the web client is `8080` and it will try to connect to the middleware running on `localhost:8079`. So after running the above commands in two separate terminal windows you will be able to connect to the web client at `++http://localhost:8080/app++`. If your project has Polymer UI, it will be available at `++http://localhost:8080/app-front++`.

You can change the parameters defined at the build time by providing application properties via Java system properties. Besides, ports and context names can be provided as command line arguments.

Command line arguments:

* `port` - defines the port on which the embedded HTTP server will run. For example:
+
----
java -jar app.jar -port 9090
----
+
Please note that if you specify a port for the *core* block, you need to provide the <<cuba.connectionUrlList,cuba.connectionUrlList>> application property with the corresponding address to the client blocks, for example:
+
----
java -jar app-core.jar -port 7070

java -jar -Dcuba.connectionUrlList=http://localhost:7070/app-core app.jar
----

* `contextName` - a web context name for this application block. For example, in order to access your web client at `++http://localhost:8080/sales++`, run the following command:
+
----
java -jar app.jar -contextName sales
----

* `frontContextName` - a web context name for the Polymer UI running in the web or portal client block.
+
[WARNING]
====
Please note that current implementation of Polymer UI requires manual setting of `<base>` tag to the actual web context available at run time. So changing the context by using `-frontContextName` command line argument can make the Polymer UI inaccessible.
====

* `jettyEnvPath` - a path to the Jetty environment file. It can be an absolute path or a path relative to the working directory.

[[jelastic_deployment]]
==== Deployment to Jelastic Cloud

CUBA Studio allows you to deploy your application to the link:$$https://jelastic.com/$$[Jelastic] cloud in a few easy steps.

[TIP]
====
Please note that only projects using PostgreSQL or HSQL databases are currently supported.
====

. Click the *Deployment settings* link on the *Project properties* section and switch to the *CLOUD* tab.

. If the project is not yet set up for cloud deployment, you can use the field on top to create a free trial Jelastic account.

. After completing your registration, enter the email, password and selected provider.
+
image::jelastic_1.png[align="center"]

. *Environment* field defines the environment in which the application WAR will be deployed. Click on the ellipsis button and select an existing environment or create a new one. You can check the selected environment for compatibility with your project. A compatible environment should have Java 8, Tomcat 8 and PostgreSQL 9.1+ (if the project uses PostgreSQL database). If your project uses PostgreSQL, you will receive an email with the database connection details. Please use them when generating custom `context.xml` file, see *Custom context.xml path* field below. Besides, you should create an empty PostgreSQL database using the provider's web interface link containing in the email. The database name should be specified later in custom context.xml (see below).
+
image::jelastic_6.png[align="center"]

. Press *Generate* button next to the *Custom web.xml path* field. Studio will generate a special `web.xml` of the <<build.gradle_buildWar,single WAR>> comprising the Middleware and Web Client application blocks.
+
image::jelastic_2.png[align="center"]

. If your project uses HSQLDB, that is all - you can press *OK* and start deployment by clicking *Run > Deploy to cloud* main menu item. The deployment parameters can be later adjusted in <<build.gradle_deployWar,build.gradle>>.

. If your project uses PostgreSQL, go to the database administration web interface by the link in the email recieved after creation of the environment and create a database.

. Press *Generate* button next to the *Custom context.xml path* field and specify the database user, password, host and name.
+
image::jelastic_3.png[align="center"]

. Leave the *Include JDBC driver* and *Include context.xml* checkboxes selected.
+
image::jelastic_4.png[align="center"]

. Now you can press *OK* and start deployment by clicking *Run > Deploy to cloud* main menu item.

. After completing the deployment, use the link at the bottom left corner to open the application web interface.
+
image::jelastic_5.png[align="center"]

[[bluemix_deployment]]
==== Deployment to Bluemix Cloud

CUBA Studio provides support of IBM® Bluemix® cloud deployment in a few easy steps.

[TIP]
====
Bluemix cloud deployment is currently applicable only for projects using PostgreSQL database. HSQLDB is available with _in-process_ option only, that means the database will be recreated on every application restart, and the user data will be lost.
====

. Create an account in the Bluemix. Download and install:
.. Bluemix CLI: http://clis.ng.bluemix.net/ui/home.html
.. Cloud Foundry CLI: https://github.com/cloudfoundry/cli/releases
.. Make sure the commands `bluemix` and `cf` work in the command line. If not, add your `\IBM\Bluemix\bin` path to the `PATH` environment variable.

. Create a Space in the Bluemix with any space name. You can group several applications within one space, if needed.

. In the Space create an application server: *Create App* -> *CloudFoundry Apps* -> *Tomcat*.

. Specify the name of the application. The name should be unique as it will be used as part of the URL of your application.

. To create a Database service, click *Create service* in the Space dashboard and choose *ElephantSQL*.

. Open the application manager and connect the created DB Service to the application. Click *Connect Existing*. For the changes to take effect, the system requires to restage (update) the application. In our case it is not necessary, as the application will be redeployed.

. After the DB Service is connected, DB credentials become available by the *View Credentials* button. The DB properties are also stored in the `VCAP_SERVICES` environment variable of the application runtime and could be viewed by calling the `cf&#160;env` command. The created database is also accessible from outside of the Space, so you can work with it from your development environment.

. Setup your CUBA project to run with the PostgreSQL (the DBMS similar to one you have in the Bluemix).

. Generate DB scripts and start the local Tomcat server. Make sure the application works.

. Generate WAR-file to deploy the application to Tomcat.
.. Click *Deployment Settings* in the *Project Properties* section of Studio navigation panel.
.. Switch to the *WAR* tab.
.. Enable all the options using checkboxes, as for correct deployment it should be the *Single WAR* with JDBC driver and `context.xml` inside.
+
image::bluemix_war_settings.png[align="center"]

.. Click *Generate* button near the *Custom context.XML field*. In the opened dialog fill the credentials of the Database you have created in Bluemix.
+
Use the credentials from `uri` of your DB service following the example below:
+
[source, json]
----
include::{sourcesdir}/deployment/bluemix_credentials.json[]
----
+
*Database user*: `ldwpelpl`
+
*Database password*: `eFwXx6lNFLheO5maP9iRbS77Sk1VGO_T`
+
*Database URL*: `echo-01.db.elephantsql.com:5432`
+
*Database name*: `ldwpelpl`

.. Click *Generate* button to generate the custom `web.xml` file required for the single WAR.

.. Save the settings. Generate the WAR-file using the `buildWar` Gradle task in Studio or command line.
+
image::bluemix_buildWar.png[align="center"]
+
As a result, the  `app.war` appears in the `build/distributions/war/` sub-directory of the project.

. In the root directory of the project create manually the `manifest.yml` file. The contents of the file should be like follows:
+
[source, yml]
----
include::{sourcesdir}/deployment/bluemix_manifest.yml[]
----
+
where
+
* `path` is the relative path to WAR-file.
* `memory`: the default memory limit is 1G. You may want to allocate less or more memory to your application, this can also be done via Bluemix WEB interface. Note that the allocated memory affects the Runtime Cost.
* `name` is the name of the Tomcat application you have created in the Cloud above.
* `host`: the same as name.
* `env`: the environment variables used to set the Tomcat and Java versions.

. In the command line switch to the root directory of your CUBA project.
+
[source, yml]
----
cd your_project_directory
----

. Connect to Bluemix.
+
[source, yml]
----
bluemix api https://api.ng.bluemix.net
----

. Log in to your Bluemix account.
+
[source, yml]
----
cf login
----

. Deploy your WAR to your Tomcat.
+
[source, yml]
----
cf push
----
+
The `push` command gets all the required parameters from the `manifest.yml` file.

. You can find Tomcat server logs via Bluemix WEB-interface in the *Logs* tab on the application dashboard, as well as in command line using the command
+
[source, yml]
----
cf logs cuba-app --recent
----

. After the deployment process is completed, your application will become accessible in browser using the URL `host.domain`. This URL will be displayed in the *ROUTE* field in the table of your *Cloud Foundry Apps*.


[[heroku_deployment]]
==== Deployment to Heroku Cloud

The section describes how to deploy CUBA applications to the https://www.heroku.com/[Heroku®] cloud platform.

[TIP]
=====
This tutorial covers deployment of a project using PostgreSQL database.
=====

[[heroku_war_deployment]]
===== WAR Deployment to Heroku

Heroku account::
+
--
First, create an account on Heroku using the web browser, free account `hobby-dev` is enough. Then login to the account and create new application using *New* button at the top of the page.

Select unique name (or left the field blank to assign automatically) and choose a server location. Now you have an application, for example `morning-beach-4895`, this is the Heroku application name.

First time you will be redirected to the *Deploy* tab. Use Heroku Git deployment method.
--

Heroku CLI::
+
--
* Install https://devcenter.heroku.com/articles/heroku-command-line[Heroku CLI] on your computer.

* Navigate to the folder containing your CUBA project. Further on we will use `$PROJECT_FOLDER` for it.

* Open command prompt in `$PROJECT_FOLDER` and type:
+
[source]
----
heroku login
----

* Enter your credentials when prompted. From now on you don't need to enter credentials for this project anymore.

* Install Heroku CLI deployment plugin:
+
[source]
----
heroku plugins:install heroku-cli-deploy
----
--

PostgreSQL database::
+
--
Using the web browser go to Heroku data https://data.heroku.com/[page]

You can choose existent Postgres database or create one. Next steps describe how to create a new database.

* Find *Heroku Postgres* block and click *Create one*
* On the next screen click *Install Heroku Postgr...*
* Connect the database to Heroku application selected from a dropdown list
* Select your Plan (for example: `hobby-dev`)

Alternatively, you can install PostgreSQL using Heroku CLI:

[source]
----
heroku addons:create heroku-postgresql:hobby-dev --app morning-beach-4895
----

Here `morning-beach-4895` is your Heroku application name.

Now you can find the new database on the *Resources* tab. The database is connected to the Heroku application. To obtain database credentials go to the *Datasource* page of your Heroku database, scroll down to *Administration* section and click *View credentials* button.

----
Host compute.amazonaws.com
Database d2tk
User nmmd
Port 5432
Password 9c05
URI postgres://nmmd:9c05@compute.amazonaws.com:5432/d2tk
----
--

Project deployment settings::
+
--
* We assume that you use PostgreSQL with your CUBA project.

* Open your CUBA project in Studio, navigate to *Deployment settings*, go to *WAR* tab and then configure options as described below.
+
** Select *Build WAR*
** Set application home directory to '.' (dot)
** Select *Include JDBC driver*
** Select *Include Tomcat's context.xml*
** Click *Generate* button next to the *Custom context.xml path* field. Fill your database connection details in modal window.
** Open the file generated `modules/core/web/META-INF/war-context.xml` and check connection params and credentials:
+
[source, xml]
----
<Context>
    <!-- Database connection -->
    <Resource
      name="jdbc/CubaDS"
      type="javax.sql.DataSource"
      maxTotal="20"
      maxIdle="2"
      maxWaitMillis="5000"
      driverClassName="org.postgresql.Driver"
      url="jdbc:postgresql://compute.amazonaws.com/d2tk"
      username="nmmd"
      password="9c05"/>

      <!-- ... -->
</Context>
----
** Select *Single WAR for Middleware and Web Client*
** Click *Generate* button next to the *Custom web.xml path* field
** Copy the code shown below and paste it into the *App properties* field:
+
[source, groovy]
----
[
  'cuba.automaticDatabaseUpdate' : true
]
----
+
** Save deployment settings.
--

Build WAR file::
+
--
Build WAR file by executing the `buildWar` Gradle task. You can do it right from the Studio *Search* dialog or from the command line:

[source]
----
gradlew buildWar
----

In order to use `gradlew` command in the command line, create Gradle wrapper using Studio *Build* menu command beforehand.
--

Application setup::
+
--

* Download Tomcat Webapp Runner from https://mvnrepository.com/artifact/com.github.jsimone/webapp-runner. The version of Webapp Runner must conform to the Tomcat version in use. For example, version 8.5.11.3 of Webapp Runner is suitable for Tomcat version 8.5.11. Rename JAR to `webapp-runner.jar` and place it into `$PROJECT_FOLDER`.

* Download Tomcat DBCP from https://mvnrepository.com/artifact/org.apache.tomcat/tomcat-dbcp.
  Use the version corresponding to your Tomcat version, for example 8.5.11. Create `$PROJECT_FOLDER/libs`, rename JAR to `tomcat-dbcp.jar` and place it into the `$PROJECT_FOLDER/libs` folder.

* Create a file named `Procfile` in `$PROJECT_FOLDER`. The file should contain the following text:
+
[source, bash]
----
web: java $JAVA_OPTS -cp webapp-runner.jar:libs/* webapp.runner.launch.Main --enable-naming --port $PORT build/distributions/war/app.war
----
--

Git setup::
+
--
Open the command prompt in `$PROJECT_FOLDER` and run the commands listed below:

[source]
----
git init
heroku git:remote -a morning-beach-4895
git add .
git commit -am "Initial commit"
----
--

Application deployment::
+
--
Open the command prompt and run the following command:

On *nix:

[source]
----
heroku jar:deploy webapp-runner.jar --includes libs/tomcat-dbcp.jar:build/distributions/war/app.war --app morning-beach-4895
----

On Windows:

[source]
----
heroku jar:deploy webapp-runner.jar --includes libs\tomcat-dbcp.jar;build\distributions\war\app.war --app morning-beach-4895
----

Open the *Resources* tab in Heroku dashboard. A new Dyno should appear with a command from your `Procfile`:

image::heroku_dyno.png[align="center"]

The application is deploying now. You can monitor logs to track the process.
--

Logs monitoring::
+
--
Wait for a message `++https://morning-beach-4895.herokuapp.com/  deployed to Heroku++` in command window.

In order to track application logs, run the following command in the command line:

[source]
----
heroku logs --tail --app morning-beach-4895
----
--

After the deployment process is completed your application will be accessible in web browser by an URL like `++https://morning-beach-4895.herokuapp.com++`.

You can also open the application from the Heroku dashboard using the *Open app* button.


[[heroku_github_deployment]]
===== Deployment from GitHub to Heroku

This guide is intended for developers who have a CUBA project located on GitHub.

Heroku account::
+
--
Create an account on Heroku using the web browser, free account `hobby-dev` is enough. Then login to the account and create new application using *New* button at the top of the page.

Select unique name (or left the field blank to assign automatically) and choose a server location. Now you have an application, for example `space-sheep-02453`, this is a Heroku application name.

First time you will be redirected to *Deploy* tab. Use *GitHub* deployment method. Follow the screen instructions how to authorize your GitHub account.
Click *Search* button to list all available Git repositories then connect to desired repo. When your Heroku application is connected to GitHub you are able to activate *Automatic Deploys*. This allows to redeploy Heroku application automatically on each Git push event. In this tutorial the option is enabled.
--

Heroku CLI::
+
--
* Install https://devcenter.heroku.com/articles/heroku-command-line[Heroku CLI]
* Open command prompt in any folder of your computer and type:
+
[source]
----
heroku login
----
+
* Enter your credentials when prompted. From now on you don't need to enter credentials for this project.
--

PostgreSQL database::
+
--
* Return to web browser with Heroku https://dashboard.heroku.com[dashboard]
* Go to *Resources* tab
* Click *Find more add-ons* button to find the database add-on
* Find *Heroku Postgres* block and click it. Follow the instruction on the screen, click *Login to install* / *Install Heroku Postgres*.

Alternatively, you can install PostgreSQL using Heroku CLI:

[source]
----
heroku addons:create heroku-postgresql:hobby-dev --app space-sheep-02453
----
where `space-sheep-02453` is your Heroku application name.

Now you can find the new database on the *Resources* tab. The database is connected to the Heroku application. To obtain database credentials go to the *Datasource* page of your Heroku database, scroll down to *Administration* section and click *View credentials* button.

----
Host compute.amazonaws.com
Database zodt
User artd
Port 5432
Password 367f
URI postgres://artd:367f@compute.amazonaws.com:5432/zodt
----
--

Project deployment settings::
+
--
* Navigate to your local CUBA project folder (`$PROJECT_FOLDER`)
* Copy the content of `modules/core/web/META-INF/context.xml` to `modules/core/web/META-INF/heroku-context.xml`
* Fill `heroku-context.xml` with your actual database connection details (see example below):
+
[source, xml]
----
<Context>
    <Resource driverClassName="org.postgresql.Driver"
              maxIdle="2"
              maxTotal="20"
              maxWaitMillis="5000"
              name="jdbc/CubaDS"
              password="367f"
              type="javax.sql.DataSource"
              url="jdbc:postgresql://compute.amazonaws.com/zodt"
              username="artd"/>

    <Manager pathname=""/>
</Context>
----
--

Build configuration::
+
--
Add the following Gradle task to your `$PROJECT_FOLDER/build.gradle`

[source, groovy]
----
include::{sourcesdir}/deployment/heroku_buildGradle.groovy[]
----
--

Procfile::
+
--
A command that launches the application on Heroku side is passed by special file `Procfile`. Create a file named `Procfile` in `$PROJECT_FOLDER` with following text:

[source]
----
web: cd ./deploy/tomcat/bin && export 'JAVA_OPTS=-Dport.http=$PORT' && ./catalina.sh run
----

This provides JAVA_OPTS environment setting to Tomcat which starts by catalina script.
--

Premium addons::
+
--
If your project uses CUBA Premium Add-ons, set additional variables for the Heroku application.

* Open the Heroku dashboard.
* Go to the *Settings* tab.
* Expand the *Config Variables* section clicking the *Reveal Config Vars* button.
* Add new *Config Vars* using your license key parts (separated by dash) as *username* and *password*:
[source]
----
CUBA_PREMIUIM_USER    | username
CUBA_PREMIUM_PASSWORD | password
----
--

Gradle wrapper::
+
--
Your project requires Gradle wrapper. You can use CUBA Studio to add it: see the *Build > Create or update Gradle wrapper* main menu command.

* Create the `system.properties` file in `$PROJECT_FOLDER` with the following content (example corresponds to local JDK 1.8.0_121 installed):
+
[source]
----
java.runtime.version=1.8.0_121
----
+
* Check that files `Procfile`, `system.properties`, `gradlew`, `gradlew.bat` and `gradle` are not in `.gitignore`
* Add these files to repository and commit it

[source]
----
git add gradlew gradlew.bat gradle/* system.properties Procfile
git commit -am "Added Gradle wrapper and Procfile"
----
--

Application deployment::
+
--
Once you commit and push all changes to GitHub, Heroku starts redeploying the application.

[source]
----
git push
----

Building process is available in the dashboard on the *Activity* tab. Click *View build log* link to track the build log.

After building process is completed, your application will become accessible in browser using the `++https://space-sheep-02453.herokuapp.com/++`. You can open the application from Heroku dashboard using the *Open app* button.
--

Logs monitoring::
+
--
Heroku application log is shown by console command:

[source]
----
heroku logs --tail --app space-sheep-02453
----

Tomcat logs are also available in web application: *Menu > Administration > Server Log*
--

[[scaling]]
=== Application Scaling

This section describes ways to scale a CUBA application that consists of the *Middleware* and the *Web Client* for increased load and stronger fault tolerance requirements.

[cols="3,2", frame="all", width="100%"]
|===

a| *Stage 1. Both blocks are deployed on the same application server.*

This is the simplest option implemented by the standard <<fast_deployment,fast deployment>> procedure.

In this case, maximum data transfer performance between the *Web Client* and the *Middleware* is provided, because when the <<cuba.useLocalServiceInvocation,cuba.useLocalServiceInvocation>> application property is enabled, the Middleware services are invoked bypassing the network stack.
^| image:scaling_1.png[align="center"]

a| *Stage 2. The Middleware and the Web Client blocks are deployed on separate application servers.*

This option allows you to distribute load between two application servers and use server resources better. Furthermore, in this case the load coming from web users has smaller effect on the other processes execution. Here, the other processes mean handling other client types (for example, Desktop), running <<scheduled_tasks,scheduled tasks>> and, potentially, integration tasks which are performed by the middle layer.

Requirements for server resources:

* Tomcat 1 (Web Client):
** Memory size – proportional to the number of simultaneous users
** CPU power – depends on the usage intensity
* Tomcat 2 (Middleware):
** Memory size – fixed and relatively small
** CPU power – depends on the intensity of web client usage and of other processes

In this case and when more complex deployment options are used, the Web Client's <<cuba.useLocalServiceInvocation,cuba.useLocalServiceInvocation>> application property should be set to `false`, and <<cuba.connectionUrlList,cuba.connectionUrlList>> property should contain the URL of the Middleware block.
^| image:scaling_2.png[align="center"]

| *Stage 3. A cluster of Web Client servers works with one Middleware server.*

This option is used when memory requirements for the Web Client exceed the capabilities of a single JVM due to a large number of concurrent users. In this case, a cluster of Web Client servers (two or more) is started and user connection is performed through a Load Balancer. All Web Client servers work with one Middleware server.

Duplication of Web Client servers automatically provides fault tolerance at this level. However, the replication of HTTP sessions is not supported, in case of unscheduled outage of one of the Web Client servers, all users connected to it will have to login into the application again.

Configuration of this option is described in <<cluster_webclient,>>.
^| image:scaling_3.png[align="center"]

| *Stage 4. A cluster of Web Client servers working with a cluster of Middleware servers.*

This is the most powerful deployment option providing fault tolerance and load balancing for the Middleware and the Web Client.

Connection of users to the Web Client servers is performed through a load balancer. The Web Client servers work with a cluster of Middleware servers. They do not need an additional load balancer – it is sufficient to determine the list of URLs for the Middleware servers in the <<cuba.connectionUrlList,cuba.connectionUrlList>> application property. Another option is to use <<cluster_mw_zk,Apache ZooKeeper Integration Add-on>> for dynamic discovery of middleware servers.

Middleware servers exchange the information about user sessions, locks, etc. In this case, full fault tolerance of the Middleware is provided – in case of an outage of one of the servers, execution of requests from client blocks will continue on an available server without affecting users.

Configuration of this option is described in <<cluster_mw,>>.
^| image:scaling_4.png[align="center"]

|===

[[cluster_webclient]]
==== Setting up a Web Client Cluster

This section describes the following deployment configuration:

image::cluster_webclient.png[align="center"]

Servers `host1` and `host2` host Tomcat instances with the `app` web-app implementing the Web Client block. Users access the load balancer at `++http://host0/app++`, which redirects their requests to the servers. Server `host3` hosts a Tomcat instance with the `app-core` web-app that implements the Middleware block.

[[cluster_webclient_lb]]
===== Installing and Setting up a Load Balancer

Let us consider the installation of a load balancer based on *Apache HTTP Server* for *Ubuntu 14.04*.

. Install *Apache HTTP Server* and its *mod_jk* module:
+
`$ sudo apt-get install apache2 libapache2-mod-jk`

. Replace the contents of the `/etc/libapache2-mod-jk/workers.properties` file with the following:
+
[source, properties]
----
workers.tomcat_home=
workers.java_home=
ps=/

worker.list=tomcat1,tomcat2,loadbalancer,jkstatus

worker.tomcat1.port=8009
worker.tomcat1.host=host1
worker.tomcat1.type=ajp13
worker.tomcat1.connection_pool_timeout=600
worker.tomcat1.lbfactor=1

worker.tomcat2.port=8009
worker.tomcat2.host=host2
worker.tomcat2.type=ajp13
worker.tomcat2.connection_pool_timeout=600
worker.tomcat2.lbfactor=1

worker.loadbalancer.type=lb
worker.loadbalancer.balance_workers=tomcat1,tomcat2

worker.jkstatus.type=status
----

. Add the lines listed below to `/etc/apache2/sites-available/000-default.conf`:
+
[source, xml]
----
<VirtualHost *:80>
...
    <Location /jkmanager>
        JkMount jkstatus
        Order deny,allow
        Allow from all
    </Location>

    JkMount /jkmanager/* jkstatus
    JkMount /app loadbalancer
    JkMount /app/* loadbalancer

</VirtualHost>
----

. Restart the Apache HTTP service:
+
`$ sudo service apache2 restart`

[[cluster_webclient_tomcat]]
===== Setting up Web Client Servers

[TIP]
====
In the examples below, we provide paths to configuration files as if <<fast_deployment>> is used.
====

On the Tomcat 1 and Tomcat 2 servers, the following settings should be applied:

. In `tomcat/conf/server.xml`, add the `jvmRoute` parameter equivalent to the name of the worker specified in the load balancer settings for `tomcat1` and `tomcat2`:
+
[source, xml]
----
<Server port="8005" shutdown="SHUTDOWN">
  ...
  <Service name="Catalina">
    ...
    <Engine name="Catalina" defaultHost="localhost" jvmRoute="tomcat1">
      ...
    </Engine>
  </Service>
</Server>
----

. Set the following application properties in `tomcat/conf/app/local.app.properties`:
+
[source, properties]
----
cuba.useLocalServiceInvocation = false
cuba.connectionUrlList = http://host3:8080/app-core

cuba.webHostName = host1
cuba.webPort = 8080
cuba.webContextName = app
----
+
<<cuba.webHostName,cuba.webHostName>>, <<cuba.webPort,cuba.webPort>> and <<cuba.webContextName,cuba.webContextName>> parameters are not mandatory for WebClient cluster, but they allow easier identification of a server in other platform mechanisms, such as the <<jmx_console, JMX console>>. Additionally, *Client Info* attribute of the *User Sessions* screen shows an identifier of the Web Client that the current user is working with.

[[cluster_mw]]
==== Setting up a Middleware Cluster

This section describes the following deployment configuration:

image::cluster_mw.png[align="center"]

Servers `host1` and `host2` host Tomcat instances with the `app` web application implementing the Web Client block. Cluster configuration for these servers is described in the <<cluster_webclient,previous section>>. Servers `host3` and `host4` host Tomcat instances with the `app-core` web application implementing the Middleware block. They are configured to interact and share information about user sessions, locks, cash flushes, etc.

[TIP]
====
In the examples below, we provide paths to configuration files as if <<fast_deployment>> is used.
====

[[cluster_mw_client]]
===== Setting up Connection to the Middleware Cluster

In order for the client blocks to be able to work with multiple Middleware servers, the list of server URLs should be specified in the <<cuba.connectionUrlList,cuba.connectionUrl>> application property. For the Web Client, this can be done in `tomcat/conf/app/local.app.properties`:

[source, properties]
----
cuba.useLocalServiceInvocation = false
cuba.connectionUrlList = http://host3:8080/app-core,http://host4:8080/app-core

cuba.webHostName = host1
cuba.webPort = 8080
cuba.webContextName = app
----

A middleware server is randomly determined on the first remote connection for a <<userSession,user session>>, and it is fixed for the whole session life time ("sticky session"). Requests from anonymous session and without session do not stick to a server and go to random servers.

The algorithm of selecting a middleware server is provided by the `cuba_ServerSorter` bean which is by default implemented by the `RandomServerSorter` class. You can provide your own implementation in your project.

[[cluster_mw_server]]
===== Configuring Interaction between Middleware Servers

Middleware servers can maintain shared lists of <<userSession,user sessions>> and other objects and coordinate invalidation of caches. <<cuba.cluster.enabled,cuba.cluster.enabled>> property should be enabled on each server to achieve this. Example of the `tomcat/conf/app-core/local.app.properties` file is shown below:

[source, properties]
----
cuba.cluster.enabled = true

cuba.webHostName = host3
cuba.webPort = 8080
cuba.webContextName = app-core
----

For the Middleware servers, correct values of the <<cuba.webHostName,cuba.webHostName>>, <<cuba.webPort,cuba.webPort>> and <<cuba.webContextName,cuba.webContextName>> properties should be specified to form a unique <<serverId,Server ID>>.

Interaction mechanism is based on link:http://www.jgroups.org[JGroups]. The platform provides two configuration files for JGroups:

* `jgroups.xml` - UDP-based stack of protocols which is suitable for local network with enabled broadcast communication. This configuration is used by default when the cluster is turned on.

* `jgroups_tcp.xml` - TCP-based stack of protocols which is suitable for any network. It requires explicit setting of cluster members addresses in `TCP.bind_addr` and `TCPPING.initial_hosts` parameters. In order to use this configuration, set <<cuba.cluster.jgroupsConfig,cuba.cluster.jgroupsConfig>> application property.

In order to set up JGroups parameters for your environment, copy the appropriate `jgroups.xml` file from the root of `cuba-core-<version>.jar` to your project *core* module or to `tomcat/conf/app-core` and modify it.

`ClusterManagerAPI` bean provides the program interface for interaction between servers in the Middleware cluster. It can be used in the application – see JavaDocs and usages in the platform code.

[[cluster_mw_zk]]
===== Using ZooKeeper for Cluster Coordination

There is an <<app_components,application component>> that enables dynamic discovery of middleware servers for communication between middleware blocks and for requesting middleware from client blocks. It is based on integration with https://zookeeper.apache.org[Apache ZooKeeper] - a centralized service for maintaining configuration information. When this component is included in your project, you need to specify only one static address when running your application blocks - the address of ZooKeeper. Middleware servers will advertise themselves by publishing their addresses on the ZooKeeper directory and discovery mechanisms will request ZooKeeper for addresses of available servers. If a middleware server goes down, it will be automatically removed from the directory immediately or after a timeout.

The source code of application component is available on https://github.com/cuba-platform/cuba-zk[GitHub], the binary artifacts are published in the standard CUBA repositories. See https://github.com/cuba-platform/cuba-zk[README] for information about including and configuring the component.

[[serverId]]
==== Server ID

_Server ID_ is used for reliable identification of servers in a *Middleware* cluster. The identifier is formatted as `host:port/context`:

[source, plain]
----
tezis.haulmont.com:80/app-core
----

[source, plain]
----
192.168.44.55:8080/app-core
----

The identifier is formed based on the configuration parameters <<cuba.webHostName,cuba.webHostName>>, <<cuba.webPort,cuba.webPort>>, <<cuba.webContextName,cuba.webContextName>>, therefore it is very important to specify these parameters for the Middleware blocks working within the cluster.

Server ID can be obtained using the `ServerInfoAPI` bean or via the <<serverInfoMBean,ServerInfoMBean>> JMX interface.

[[jmx_tools]]
=== Using JMX Tools

This section describes various aspects of using *Java Management Extensions* in CUBA-based applications.

[[jmx_console]]
==== Built-In JMX Console

The Web Client module of the *cuba* <<app_components,application component>> contains JMX objects viewing and editing tool. The entry point for this tool is `com/haulmont/cuba/web/app/ui/jmxcontrol/browse/display-mbeans.xml` screen registered under the `jmxConsole` identifier and accessible via *Administration* > *JMX Console* in the standard application menu.

Without extra configuration, the console shows all JMX objects registered in the JVM where the Web Client block of the current user is running. Therefore, in the simplest case, when all application blocks are deployed to one web container instance, the console has access to the JMX beans of all tiers as well as the JMX objects of the JVM itself and the web container.

Names of the application beans have a prefix corresponding to the name of the web-app that contains them. For example, the `app-core.cuba:type=CachingFacade` bean has been loaded by the *app-core* web-app implementing the Middleware block, while the `app.cuba:type=CachingFacade` bean has been loaded by the *app* web-app implementing the Web Client block.

.JMX Console
image::jmx-console.png[align="center"]

JMX console can also work with the JMX objects of a remote JVM. This is useful when application blocks are deployed over several instances of a web container, for example separate Web Client and Middleware.

To connect to a remote JVM, a previously created connection should be selected in the *JMX Connection* field of the console, or a new connection can be created:

.Editing a JMX Connection
image::jmx-connection-edit.png[align="center"]

To get a connection, JMX host, port, login and password should be specified. There is also the *Host name* field, which is populated automatically, if any CUBA-application block is detected at the specified address. In this case, the value of this field is defined as the combination of <<cuba.webHostName,cuba.webHostName>> and <<cuba.webPort,cuba.webPort>> properties of this block, which enables identifying the server that contains it. If the connection is done to a 3rd party JMX interface, then the *Host name* field will have the "Unknown JMX interface" value. However it can be changed arbitrarily.

In order to allow a remote JVM connection, the JVM should be configured properly (see below).

[[jmx_remote_access]]
==== Setting up a Remote JMX Connection

This section describes *Tomcat* startup configuration required for a remote connection of JMX tools.

[[jmx_remote_access_tomcat_windows]]
===== Tomcat JMX for Windows

* Edit `bin/setenv.bat` in the following way:
+
[source, properties]
----
set CATALINA_OPTS=%CATALINA_OPTS% ^
-Dcom.sun.management.jmxremote ^
-Djava.rmi.server.hostname=192.168.10.10 ^
-Dcom.sun.management.jmxremote.ssl=false ^
-Dcom.sun.management.jmxremote.port=7777 ^
-Dcom.sun.management.jmxremote.authenticate=true ^
-Dcom.sun.management.jmxremote.password.file=../conf/jmxremote.password ^
-Dcom.sun.management.jmxremote.access.file=../conf/jmxremote.access
----
+
Here, the `java.rmi.server.hostname` parameter should contain the actual IP address or the DNS name of the computer where the server is running; `com.sun.management.jmxremote.port` sets the port for JMX tools connection.

* Edit the `conf/jmxremote.access` file. It should contain user names that will be connecting to the JMX and their access level. For example:
+
[source, plain]
----
admin readwrite
----

* Edit the `conf/jmxremote.password` file. It should contain passwords for the JMX users, for example:
+
[source, plain]
----
admin admin
----

* The password file should have reading permissions only for the user running the *Tomcat*. server. You can configure permissions the following way:

** Open the command line and go to the conf folder

** Run the command:`++cacls jmxremote.password /P "domain_name\user_name":R++`
+
where `++domain_name\user_name++` is the user's domain and name

** After this command is executed, the file will be displayed as locked (with a lock icon) in *Explorer*.

* If *Tomcat* is installed as a Windows service, than the service should be started on behalf of the user who has access permissions for jmxremote.password. It should be kept in mind that in this case the `bin/setenv.bat` file is ignored and the corresponding JVM startup properties should be specified in the application that configures the service.

[[jmx_remote_access_tomcat_linux]]
===== Tomcat JMX for Linux

* Edit `bin/setenv.sh` the following way:
+
[source, properties]
----
CATALINA_OPTS="$CATALINA_OPTS -Dcom.sun.management.jmxremote \
-Djava.rmi.server.hostname=192.168.10.10 \
-Dcom.sun.management.jmxremote.port=7777 \
-Dcom.sun.management.jmxremote.ssl=false \
-Dcom.sun.management.jmxremote.authenticate=true"

CATALINA_OPTS="$CATALINA_OPTS -Dcom.sun.management.jmxremote.password.file=../conf/jmxremote.password -Dcom.sun.management.jmxremote.access.file=../conf/jmxremote.access"
----
+
Here, the `java.rmi.server.hostname` parameter should contain the real IP address or the DNS name of the computer where the server is running; `com.sun.management.jmxremote.port` sets the port for JMX tools connection

* Edit `conf/jmxremote.access` file. It should contain user names that will be connecting to the JMX and their access level. For example:
+
[source, plain]
----
admin readwrite
----

* Edit the `conf/jmxremote.password` file. It should contain passwords for the JMX users, for example:
+
[source, plain]
----
admin admin
----

* The password file should have reading permissions only for the user running the *Tomcat* server. Permissions for the current user can be configured the following way:

** Open the command line and go to the conf folder.

** Run the command:
+
`chmod go-rwx jmxremote.password`

[[server_push_settings]]
=== Server Push Settings

CUBA applications use server push technology in the <<background_tasks,Background Tasks>> mechanism. It may require an additional setup of the application and proxy server (if any).

By default, server push uses the WebSocket protocol. The following application properties affect the platform server push functionality:

<<cuba.web.pushLongPolling,cuba.web.pushLongPolling>>

<<cuba.web.pushEnabled,cuba.web.pushEnabled>>

If users connect to the application server via a proxy that does not support WebSocket, set `cuba.web.pushLongPolling` to `true` and increase proxy request timeout to 10 minutes or more.

Below is an example of the *Nginx* web server settings for using WebSocket:

[source, plain]
----
location / {
    proxy_set_header X-Forwarded-Host $host;
    proxy_set_header X-Forwarded-Server $host;
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_read_timeout     3600;
    proxy_connect_timeout  240;
    proxy_set_header Host $host;
    proxy_set_header X-RealIP $remote_addr;

    proxy_pass http://127.0.0.1:8080/;
    proxy_set_header X-Forwarded-Proto $scheme;

    proxy_set_header Upgrade $http_upgrade;
    proxy_set_header Connection "upgrade";
}
----
